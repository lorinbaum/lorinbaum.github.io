<!DOCTYPE html>
<html lang=en>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Towards Computation</title>
    <link rel="stylesheet" href="main.css">
    <link rel="shortcut icon" href="favicon.ico">
</head>

<body>
    <nav><a href='index.html'>Entrance</a><a href="#top">Top</a></nav>
    <div id="top"></div>

    <p>Notes on the computer technology stack.</p>
    
    <h1>Notes on computers</h1>

    <nav class="toc"></nav>

    <h2>More refined</h2>

    <p>
        Mechanisms that can control propagation of a signal depending on their state and that can change their state depending on another such signal can be used to build computers.<br>
        A NOT-Gate inverts the input signal. An AND-Gate outputs a high signal if both its inputs are high. Any state transition that can be expressed in a series of signals (as a binary number) can be defined using NOT and AND gates. OR gates are implicit when running multiple such functions in parallel.<br>
        Everything else is performance optimization. Intelligence can be seen as the ability to compress complex behavior into efficient, useful functions.<br>
        Spirits themselves resemble state transition functions. Bacteria, plants, brains, humans, computers, companies, countries, ecosystems.<br>
        They can form larger, more complex functions - organisms to help them survive.<br>
    </p>
    <p>
        Computing substrates are unified under hardware description languages, which define connections between given substrate-dependent logical units to form other desirable functions.
    </p>
    <p>
        Because hardware is expensive, most computers don't execute fixed functions, but offer a set of generic functions like arithmetic and an instruction input, which defines which functions should be applied to which data and where the result should be stored.<br>
        Software is series of instructions. It can be abstracted away from the specific hardware implementation, and so is most flexible, can adapt to other substrates.
    </p>
    <p>
        Operating systems provide a platform to make it easier for other software to execute. They often manage device drivers, like for storage devices, handle multiple, simultaneous processes, manage memory and offer a way to write and execute programs.<br>
        Programming languages help humans ergonomically state their wishes and must eventually be compiled to hardware-specific instructions.
    </p>

    <h2>Less refined</h2>

    <h3>Hardware implementation</h3>
    <p>Signal propagation speeds: Flowing water ~ tens of m/s. Soundwaves in solids ~ thousands of m/s. Electromagnetic signals approach the speed of light ~300,000,000 m/s.</p>

    <h4>Electronics</h4>
    <p>
        They went outside and found that two groups of stuff attract each other but repel members of the same group and still nobody knows why. <i>charges</i>.<br>
        If I wiggle a charge, it takes some time for charges nearby to react. As if a signal travelling near 300,000,000 m/s lets them know.<br>
        Some materials only display charge behavior after being rubbed together.
        <br>
        When some materials touch some charged materials, the charge appears to spread into the new material as observed by a weaker attraction/repulsion to other charges.<br>
        Some materials permit such flow of charge more easily than others, are roughly divided into insulators and conductors.<br>
        <!-- Magnets? Induce flow? -->
        <!-- <br>
        Only electrons can move?</p> -->

    <p>How are these ideas embedded today?
        <br>
        In some substrates, there are electrons loosely bound to their atom cores. They are not part of a bond and are presumably far away from the core.<br>
        If there is an electron surplus on one side and an electron deficit on the other, they will drift in the direction of the deficit.<br>
        More loose electrons = better conductor?
        <br>
        Resistors, sometimes made from both conductors and insulators impede flow by forcing barriers into the path. Energy is converted to heat.</p>

    <p>
        In a silicon crystal, electrons are can leave or join an atom core with similar energy. This means the crystal can be bombarded with two types of atoms in different places, one will provide an extra electron when it enters the crystal (n-type) and the other will want an electron from the silicon (p-type). Both <i>doped</i> regions will gain conductance while remaining electrically neutral. Pure silicon crystal can be used as a weak insulator/resistor.<br>
        Bombarded silicon crystals are damaged, but will repair when heated. About 1 in 1000 atoms in the lattice is swapped, lest the crystal fragments (?).
    </p>
    
    <p>If more electrons are added to the side of free floating electrons (n) and a deficit connected to the other side (p). The side which lacks electrons will lack even more and the side that has them will have even more and quickly the pressure becomes large enough for electrons to flow through the gap.
        <br>
        In the other direction, electrons will first fill the electron holes and on the other side they will be sucked out, effectively eliminating charge carries and widening the non-conductive gap (depletion zone). Only if a relatively large voltage is applied, do electrons flow again.
        <br>
        Transistors can be built from an n-p-n arrangement (or p-n-p) with an insulator and a capacitor above the p region. When the capacitor is charged, it sucks electrons into the region, first filling electronholes, then adding free floating electrons allowing free flow to the n-regions: Gate is open. Higher current means the gate capacitor charges faster or higher, opening the channel sooner. Enables faster switching speeds at the cost of power, which rises at current squared.<br>
        (FinFETs solve problems from small scale somehow to get smaller)
    </p>

    <p>Sand is filtered by size, weight (and?) to extract most pure silicon, which is molten and a crystal is drawn from it.<br>
        Slices of pure silicon crystal (wafers, 15-30cm diameter, 0.2mm thick) are partially masked (with what), bombarded with Bor/Phosphor to create n-p-n regions. Repeated for the other element. Heated to repair the crystal.<br>
        Coated with a thin film, which is weakened by UV light shining through a mask. Weakened film washed away. Etched. Conductor (other elements, insulation?) poured into the etched space. Sanded flat. Build up more layers? Cleaned repeatedly.<br>
        Practically 3D printing. Many cycles. 3 months for a any chip to complete (with equipment already in place and running!).</p>

    <h3>Hardware description language</h3>
    <p>Depending on the implementation, various basic building blocks may be available. Mechanical adders or square root calculators, transistors.<br>
        HDL expresses connections between these. No speak of electrons, but bits, gates, clocks and busses.<br>
        Bits is information spread into multiple signals like (1101 = 13), instead of single 13 signal. For flexibility?<br>
        Gates are commonly used arrangements. Truth table of all possible gates with two binary inputs:</p>
    <table>
        <thead><tr>
                <th>x:<br>y:</th>
                <th>0<br>0</th>
                <th>0<br>1</th>
                <th>1<br>0</th>
                <th>1<br>1</th>
        </tr></thead>
        <tbody>
            <tr><td>constant 0</td> <td>0</td><td>0</td><td>0</td><td>0</td></tr>
            <tr><td>And</td>        <td>0</td><td>0</td><td>0</td><td>1</td></tr>
            <tr><td>x And Not y</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
            <tr><td>x</td>          <td>0</td><td>0</td><td>1</td><td>1</td></tr>
            <tr><td>Not x And y</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
            <tr><td>y</td>          <td>0</td><td>1</td><td>0</td><td>1</td></tr>
            <tr><td>Xor</td>        <td>0</td><td>1</td><td>1</td><td>0</td></tr>
            <tr><td>Or</td>         <td>0</td><td>1</td><td>1</td><td>1</td></tr>
            <tr><td>Nor</td>        <td>1</td><td>0</td><td>0</td><td>0</td></tr>
            <tr><td>Equivalence</td><td>1</td><td>0</td><td>0</td><td>1</td></tr>
            <tr><td>Not y</td>      <td>1</td><td>0</td><td>1</td><td>0</td></tr>
            <tr><td>If y then x</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
            <tr><td>Not x</td>      <td>1</td><td>1</td><td>0</td><td>0</td></tr>
            <tr><td>If x then y</td><td>1</td><td>1</td><td>0</td><td>1</td></tr>
            <tr><td>Nand</td>       <td>1</td><td>1</td><td>1</td><td>0</td></tr>
            <tr><td>Constant 1</td> <td>1</td><td>1</td><td>1</td><td>1</td></tr>
        </tbody>
    </table>
    <p>FPGAs?</p>
    <p>
        Signal propgation takes some time until the output stabilizies (adding 15 and 17 might initally output 22 before carrying over 1 and stabilizing at 32). Clocks exist to store data only when it is expected to have stabilized. They periodically change their output between 0 and 1. Storage will only accept input if the clock is on a 1 cycle. The clock is timed such that any calculation can stabilize during the 0 cycle. They also help synchronized processes. Desktop consumer processors today reach 5 GHz clockrates.
    </p>
    <p>
        <img alt="" src="20241122_Towards_computation_CPU-GPU_2.svg" /><br>
        Latency-oriented vs throughput-orientied processors.<br>
        Latency is reduced through:
    </p>
    <ul>
        <li>large on-chip caches (L1, L2, L3 = yellow) for faster data access</li>
        <li>large control units (blue) for better utilization</li>
        <li>higher clockrates</li>
    </ul>
    <p>each with diminishing returns.</p>
    <p>Throughput-oriented processors use the chip area for more processing cores (magenta) at lower clockrates, saving power. The cores require fast, parallel memory access to stay fed.</p>
    
    <h3>The Elements of Computing Systems: Building a Modern Computer from First Principles</h3>
    <p>(second edition - Noam Nisan, Shimon Schocken)</p>
    
    <blockquote>
        <p>What I hear, I forget; What I see, I remember; What I do, I understand.<br>
            —Confucius (551–479 B.C.)</p>
    </blockquote>
    <h4>Hardware</h4>
    <ul>
        <li>Church-Turing conjecture that all computers are essentially the same. It does not matter which computer is implemented here.</li>
        <li>good modular design -> modules are truly independent and can be treated as black boxes by users</li>
        <li>NAND or NOR Gates can implement any computer</li>
    </ul>
    <h4>1. Boolean Logic</h4>
    <p>Possible boolean functions for n binary inputs is ${2}^{2^{n}}$. and some have names, like here with two inputs:</p>
    <p>Testing complex chip implementation completely is infeasible, so they test on a subset.</p>

    <h3>CUDA</h3>
    
    <p>Data parallelism<br>
        data parallelism is a simpler special case of task parallelism, where a task is split up into parallelizable sections. data parallelism already provides data that can be treated independently.<br>
        code is being reorganized to be executed around the new data structure</p>
    <p>2.2 CUDA C program structure</p>
    <ul>
        <li>framework vs programming model<ul>
                <li>framwork provides specific functions, programming model more like a way to think about program and data structure (warps, block, threads)</li>
            </ul>
        </li>
        <li>kernel (seed) vs function. its a function that is "launched" onto the GPU and executed for each thread</li>
        <li>cuda c extends normal c (ANSI C) with keywords, some new syntax and functions to run stuff on the GPU. all normal c code runs on CPU</li>
        <li>host and device code somehow cooperate. the host launches (kernel) a grid of threads</li>
        <li>cuda threads are sequential programs with a program counter and its variables. (each thread runs the same program but effectively has an id as defined by predefined variables that differ for each thread)</li>
        <li>Generating and scheduling threads on GPU is very fast. Not on CPU. kernels tend to be simple and can just be copied into the threads. context switching is also extremely fast on GPU for latency hiding.</li>
        <li>On CPU what if I open more threads than are available on the CPU? they are switched into the physical threads</li>
    </ul>
    <p>2.3 vector addition kernel CPU</p>
    <ul>
        <li>conventional c host code</li>
        <li>declaring pointers is with <code>float *P;</code>, accessing address of a variable with <code>int *addr = &amp;V</code> and getting the item at the pointer with <code>float V = *P</code></li>
        <li>subsequent statements in the main function can use the output <code>C</code></li>
        <li>getting ~20 MFLOPS on CPU</li>
        <li>the following code for on-device computation will practically outsource this part, but its inefficient because it will be moving a lot of dta around which is slow.</li>
        <li>it will allocate memory on device and move the arrays there, then run the kernel, then free device vectors</li>
    </ul>
    <div class="codeblock">#include &lt;stdio.h>
#include &lt;time.h>

void vecAdd(float* A, float* B, float* C, int n) {
    for (int i = 0; i &lt; n; i++) C[i] = A[i] + B[i];
}

int main() {
    // initialize vectors
    int n = 5;
    float A[n] = {1.2, 3.1, 0.7, 1.6, 2.5};
    float B[n] = {3.0, 2.7, 0.3, 1.3, 2.2};
    float C[n];

    // kernel
    struct timespec start, end;
    clock_gettime(CLOCK_MONOTONIC_RAW, &start);
    vecAdd(A, B, C, n);
    clock_gettime(CLOCK_MONOTONIC_RAW, &end);

    // results
    for (int i = 0; i &lt; n; i++) printf("%.2f + %.2f = %.2f\n", A[i], B[i], C[i]);
    printf("Wall clock time: %.9fs\n", ((end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / 1000000000.0));

    return 0;
}</div>

    <p>2.4 Device global memory and data transfer</p>
    <ul>
        <li>cudaMalloc and cudaFree</li>
        <li>cudaMalloc assigns to the argument pointer (<code>void **</code>) and returns possible errors. so there is need for the cudaCheck function.</li>
        <li>A_h and A_d for host and device</li>
        <li>the vecAdd function that allocates, copies and copies back and frees is called <i>stub</i> for calling a kernel.</li>
        <li>error checking macro</li>
    </ul>
    <p>2.5 Kernel functions and threading</p>
    <ul>
        <li>the kernel functions will be written in SPMD style (single program multiple data).</li>
        <li>grid -&gt; block -&gt; thread</li>
        <li>1 block = 1024 threads max</li>
        <li>threadIdx and blockidx for thread id</li>
        <li>what is ANSI C? CUDA C is an extension of ANSI C</li>
        <li>globa, host and device keywords for kernel functions.</li>
        <li>global kernel function = new grid. </li>
        <li>grid of threads = loop (loop parallelism)</li>
        <li>boundary checking</li>
    </ul>
    <p>2.6 Calling kernel functions</p>
    <ul>
        <li>execution configuration parameters</li>
        <li>cannot make assumptions about execution order</li>
        <li>some gpus will work through it in smaller pieces than others</li>
        <li>language needs a compiler. NVCC produces host code (gcc?) and device code (PTX -&gt; binary)</li>
        <li>what is the purpose of just in time compilation?</li>
    </ul>
    <p>2.7 Compilation</p>
    <ul>
        <li>needs a different compiler</li>
        <li>to virtual binary files (PTX)</li>
        <li>runtime component of nvcc translates to "real object files" to be executed on GPU. but in the illustration its called "device just-in-time compiler"</li>
    </ul>

    <h3>fastai diffusion</h3>

    <p><a href="https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP">PART 2: deep learning foundations to stable diffusion 2022</a></p>
    <ol>
        <li>
            <p>have a classification that says how much something corresponds to the target</p>
            <ul>
                <li>add noise to targets and train a neural net to predict what noise was added</li>
            </ul>
        </li>
        <li>
            <p>get gradient for every pixel of the input</p>
        </li>
        <li>update pixel</li>
    </ol>
    <p><i>Unet</i>: input: some noisy image. output: the noise</p>
    <p>
        Use an <i>autoencoder</i> to reduce image size before training the unet. unet now predicts the noise in the <i>latents</i> (encoded images). use autoencoder's decoder to get high res image again.<br>
        <a href="https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2">AE vs VAE</a>
    </p>
    <p>LABELS</p>
    <p>add image label to the input for unet training. Makes it easier for unet to predict noise. Now, I can input label + noise and it starts to find noise that leaves an image equal to my label.</p>
    <p>label needs encoding to be non-specific. "beautiful swan", "nice swan", "graceful swan" should return similar images. Training the network on every wording leads to combinatorial explosion.</p>
    <p>Instead: train a network to encode images and their labels with a similar vector. Then, since, slight differences in wordings lead to the similar images, the network understands their similarity and can interpolate usefully.</p>
    <p>the image vector and its label's vector should be similar. Their vector should be dissimilar to other image or text embedding vectors.<br>
        Calculate similarity of two vectors: dot product (= higher if more similar)</p>
    <p>loss function (in this case higher = better) = dot product of matching image+label - dot product of non-matching image+label<br>
        (= <i>contrastive loss</i>)<br>
        models used in this case for image and text encoding : CLIP (contrastive loss IP(?))</p>
    <p>network being <i>multimodal</i>: similar embeddings in different modes</p>
    <p>model does not know how to improve on a finished image if it turned out wrong. needs to add noise, then redo.</p>
    
    <h3>Inbox</h3>

    <h4>Electronics</h4>
    <p>
        Learning the Art of Electronics<br>
        The Art of Electronics<br>
        https://www.tinkercad.com/circuits</p>
    <p>Paul Drude model of electricty pretends that electrons are discrete mechanical objects. This works, but really they are quantum particles.</p>
    
    <h4>Chips</h4>

    <p>GPU PCBs are huge but mostly data storage and delivery, power transformation and delivery and other I/O in support of the core. The Voltage Regulator Modules (VRMs) emit notable heat.<br>
        Non Founders Edition cards offer more powerful cooling and sometimes electrical robustness and smallness.</p>
    <p>Trying to verify ALU percentage on chip area, but ALUs are too small to differentiate easily?<br>
        <a href="https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake">Intel Raptor Lake microarchitecture</a><br>
        <a href="https://hothardware.com/news/intel-raptor-lake-huge-cache-upgrade-for-gaming">Intel Alder lake-S good annotation</a><br>
        <img alt="" src="H100-chip.jpg" /><br>
        <i>H100 die. "squares" are streaming multiprocessors (144). Darker areas between are mostly L3 Cache.</i><br>
        <a href="https://resources.nvidia.com/en-us-tensor-core">H100 Tensor Core GPU Architecture</a>
    </p>
    <p><a href="https://blog.ovhcloud.com/understanding-the-anatomy-of-gpus-using-pokemon/">Understanding the anatomy of GPUs using Pokémon</a><br>
        <a href="https://www.reddit.com/r/GraphicsProgramming/comments/1871frx/books_for_gpu_arch/">Reddit Books for GPU arch</a>
    </p>
    <h4>AI</h4>
    <p>Backpropagation described here:<br>
        <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Andrej Karpathy: Neural networks: Zero to Hero</a><br>
        <a href="https://www.wolframalpha.com/">Wolfram Alpha to look up functions for derivatives</a>.<br>
        <a href="https://ml4a.github.io/ml4a/">Linear layers, convolutional neural networks and optimizers</a>
    </p>
    <ul>
        <li><a href="https://course.fast.ai/">https://course.fast.ai/</a>
            <ul>
                <li>The book: <a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb">https://github.com/fastai/fastbook/blob/master/01_intro.ipynb</a></li>
                <li><a href="https://course.fast.ai">course</a></li>
                <li><a href="https://forums.fast.ai">forums</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU">youtube part 1</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP">youtube part 2</a></li>
            </ul>
        </li>
        <li><a href="https://wesmckinney.com/book">essential libraries: numpy, matplotlib, pandas, pytorch</a></li>
        <li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">https://huggingface.co/learn/nlp-course/chapter1/1</a></li>
        <li>sympy: symbolic processing?</li>
        <li>wolfram alpha</li>
        <li>higher level papers by Joscha Bach</li>
        <li><a href="https://www.youtube.com/watch?v=wjZofJX0v4M&amp;vl=en">Transformers</a></li>
        <li><a href="https://www.nayuki.io/page/a-fundamental-introduction-to-x86-assembly-programming">Assembly</a></li>
        <li>How does long term memory emerge? How is information stored in the brain? LSTMs</li>
    </ul>
    <p>A Path Towards Autonomous Machine Intelligence (Yann Lecun)<br>
        Model Predictive Control MPC<br>
        hierarchical planning - no AI system does this so far except implementing by hand<br>
        generative adversarial network GAN</p>
    <p>LLM Security threats Promt insertion, jailbreak, data poisoning</p>
    <p>Robot:<br>
        step motor, brushless motor -&gt; more complicated control (servos?), brushed motor<br>
        harmonic reducers, planetary gearboxes<br>
        <a href="https://www.youtube.com/watch?v=F29vrvUwqS4">building a robot arm</a><br>
        I should be able to fist bump the robot hard, so it flies back but catches itself.
    </p>
    <p>perceptual loss?</p>

    <h3>OpenGL</h3>

    <p>It forces a framework onto graphics. Rendering each frame in a scene, it goes through a given graphics pipeline, determining the shape, rasterizing it and calculating color. Programs for each step are called shaders, though they look equivalent to compute kernels operating on different data. Minimum two shaders must be user specified: 1. Vertex shader: operates on vertex data for each frame without altering the original geometry. Can also add or remove vertices. 2. Fragment shader: operates on pixels after all invisible vertices have been culled. Handles texture sampling and color mixing.</p>

    <h4><a href="https://learnopengl.com/Getting-started/Hello-Triangle">Vertex Input</a></h4>
    <p>vertices = array of numbers -1.0 to 1.0 (outside those bounds is discarded) later to be interpreted as series of x,y,z coordinates of points, usually points of a triangle.</p>
    <p>Vertex Buffer Objects (VBOs) help with sending the data to the GPU. Bufferse are generated using a function (which assings an ID too). One Buffer can be bound at any time to any buffer type so all functions manipulating that buffer type will apply to the bound buffer. eg: <code>glBufferData(buffer type, size, vertices, memory optimization)</code> applies to the buffer current bound to [buffer type]</p>
    <p>Shaders are compile at runtime.</p>
    <p>Shader program will make the still separate vertex- and fragment shaders work, linking their in- and outputs accordingly.</p>
    <p>After program creation, the separate shaders are not needed anymore. <code>glUseProgram([shaderProgram])</code> will make any following shader/render call use it.</p>

    <h4>Linking vertex attriutes</h4>
    <p>how to interpret the vertex input? for which location (specified in vertex shader) is the data? how many values? what type? what stride? start reading the buffer with an offset?</p>

    <h3>OS</h3>
    <details>
        <summary><a href="https://wiki.osdev.org/BareBones">OSDev</a> Barebones and preliminary notes</summary>
        <p><a href="https://wiki.osdev.org/Expanded_Main_Page">OSDev.org</a>. The OS is first developed for only one target machine. Relevant components:</p>
        <ul>
            <li>Asus Z270E mainboard</li>
            <li>Intel 7700k processor</li>
            <!-- <li>Nvidia GTX 1080ti</li> -->
            <li>SATA HDDs and SSD, M.2 SSD</li>
            <li>3860x2160 monitor via display port</li>
            <li>USB mouse and keyboard</li>
            <li>Headphones via line out</li>
        </ul>
        <ul>
            <li><a href="#bare-bones">Bare Bones</a> reinterprets the <a href="https://wiki.osdev.org/Bare_Bones">Bare Bones tutorial</a> on OSDev.org for my usecase, shortening some and expanding on other concepts that seem fundamentally important.</li>
        </ul>
        <!--
        GRUB magic software but needs glue code and set up stack for kernel (C)
        explain stack
        GRUB loads me into 32 bit protected mode, sets up a VGA buffer.
        The kernel uses the buffer to write text. summarized

        linker manages the situation where multiple files are needed for a program to run.

        creating an iso image with xorriso.
        loading it with qemu

        paging is coping with limited 32 bit address space. long mode solves this
        instruction set is HUGE. 

        getting graphics working may be hard. nvidia driver is proprietary, can only try and port the linux version of their driver. intel integrated graphics driver is open source.
        otherwise use VGA / VESA but has limited resolution/capability/efficiency. Always runs on CPU, but can use integrated graphics. intel integrated graphics has a completely open driver.
        
        my own bootloader: https://wiki.osdev.org/Rolling_Your_Own_Bootloader
        bootloader UEFI app: https://wiki.osdev.org/UEFI_App_Bare_Bones
        making a bootable disk: https://wiki.osdev.org/Bootable_Disk
        -->
        <h4><a href="https://wiki.osdev.org/Bare_Bones">Bare Bones</a></h4>

        <h5>How to launch the OS? ESP with bootloader+kernel</h5>
        <p>
            When the computer starts (power button shorts two mainboard pins), the mainboard firmware, following the widespread <abbr title="Unified Extensible Firmware Interface">UEFI</abbr> standard initializes connected hardware and, for each storage device, looks for partitions (separately managed storage parts) that are <abbr title="Extensible Firmware Interface">EFI</abbr> System Partitions (ESP).<br> UEFI supports some MBR and GPT partition table schemes <sup><a href="https://en.wikipedia.org/wiki/UEFI#EFI_system_partition">[1]</a></sup>.
            It contains bootloaders: .efi files made to hand control from the firmware to the OS kernel (central program in the OS, responsible for ressource allocation and system calls etc.). In some configurable order, UEFI tries launching bootloaders until one works. The bootloader
            <ul>
                <li>determines which partition to boot from</li>
                <li>finds the kernel in the partition and loads it into memory</li>
                <li>sets up the environment: stack allocation, protected mode</li>
                <li>hands control to the kernel, passing any necessary parameters to it</li>
            </ul>
        </p>

        <h5>Bare Bones bootloader (GRUB), gluecode and kernel</h5>
        <p><abbr title="GRand Unified Bootloader">GRUB</abbr> is a magic bootloader that does everything for me and follows the Multiboot Standard when finding and handing control to the kernel. This means the kernel 1. needs a multiboot header and 2. is left to set up the stack itself. This first kernel part is written in assembly, because compiled C already requires a stack to run.</p>
        <div class="codeblock">/*
boot.s: minimal assembly to launch the kernel,
compiled using <abbr title="GNU Assembler">GAS</abbr> to boot.o
<a href="https://wiki.osdev.org/Bare_Bones#Bootstrap_Assembly">Bare Bones Boostrap Assembly</a> includes detailed comments
*/

.set ALIGN,    1&lt;&lt;0      
.set MEMINFO,  1&lt;&lt;1      
.set FLAGS,    ALIGN | MEMINFO 
.set MAGIC,    0x1BADB002      
.set CHECKSUM, -(MAGIC + FLAGS)

.section .multiboot
.align 4
.long MAGIC
.long FLAGS
.long CHECKSUM

.section .bss
.align 16
stack_bottom:
.skip 16384 # 16 KiB
stack_top:

.section .text
.global _start
.type _start, @function
_start:
mov $stack_top, %esp
call kernel_main
cli
1:	hlt
jmp 1b
.size _start, . - _start</div>
        <p>The kernel in Bare Bones, written in C, only uses the VGA text mode buffer presumtively set up by GRUB to display text on the screen.<br>
        It does this without the C standard library, which is unavailable because it would require 1. system calls and dynamic linker support already implemented in the kernel, and 2. the linker and library available at runtime. This is called being in a Freestanding Environment as opposed to a Hosted Environment. Some header files, though, are part of the compiler, not the C standard library, so some data types, constants and macros are still available.</p>
        <p>To compile the kernel, a <abbr title="GNU Compiler Collection">GCC</abbr> cross compiler is used.</p>
        <p>
            Intel integrated graphics driver: <a href="https://www.intel.com/content/www/us/en/docs/graphics-for-linux/developer-reference/1-0/kaby-lake.html">Intel Docs</a>
        </p>
    </details>

    <h3>Internet</h3>
    <details>
        <summary>Notes on internet layers</summary>
        <p>https://explained-from-first-principles.com/internet/#out-of-order-delivery</p>
        <p>
            messages understood through the protocol<br>
            handle unexpected response, ask for retransmission, time out if silent, maintain order of messages, account for latency.
        </p>
        <p>5 layers</p>
        <ol>
            <li>
                <p>link:<br>
                    addressing using MAC address within a network through ethernet, wifi, bluetooth protocols.</p>
            </li>
            <li>
                <p>network layer<br>
                    uses ip address to connect between different networks through the Internet Protocol (IP). routers send error messages and other information about itself using ICMP (internet control message p)</p>
            </li>
            <li>
                <p>transport layer<br>
                    os sending/receiving stuff to/from to processes running in the OS. uses port numbers to differentiate.<br>
                    can use transmission control p (TCP). pretends there is a direct connection (not packets). enumerates, buffers and confirms received messages to put into right order, retransmit or request retransmission if no confirmation was received. uses checksum for ensuring reliability. sender slows down if it gets no feedback. tcp handshake before actual payload transfer sets a random starting point for the enumeration, making it hard for randos to impersonate.<br>
                    user datagram p (UDP) fast, unreliable connectionless transport. only source, destination ports, length and checksum in header</p>
            </li>
            <li>
                <p>Security layer</p>
            </li>
        </ol>
    </details>

    <p>From top down:</p>
    <ul>
        <li>How does AI work? <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy</a></li>
        <li><a href="https://tinygrad.org/">tinygrad</a>, simple language to describe and execute AI.</li>
        <li>How do computers work on a high level? <a href="https://www.nand2tetris.org/">Nand to Tetris</a></li>
        <li>How would I use CPUs and GPUs for AI without tinygrad? Programming massviely parallel computers (4th edition, Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj)</li>
        <li>How do electronics work? <a href="https://artofelectronics.net/">The Art of Electronics</a> and <a href="https://learningtheartofelectronics.com/">Learning the Art of Electronics</a></li>
        <li>How is silicon refined, crystallized, doped and etched?</li>
        <li>How does this compare to other computing substrates? Brain, optical and quantum computing?</li>
    </ul>
    <script>MathJax = { tex: { inlineMath: [['$', '$']], displayMath: [['$$', '$$']] } };</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>

</html>