<!DOCTYPE html>
<html lang="en" data-theme="dark-poole"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    tinygrad dev exploration
  </title>

  <link rel="stylesheet" href="/styles.css">
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/apple-touch-icon-precomposed.png"> -->
  <link rel="shortcut icon" href="/assets/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="" href="/atom.xml">


   <!-- for mathjax support --></head>
<body>
    <div class="container content">
      <main id="content">
        <nav>
  <a href='https://lorinbaumgarten.com//'>Entrance</a>
</nav>

<article class="post">
  <p class="post-date">Created <time datetime="2024-06-22T11:27:48+02:00">2024 06 22</time>, last changed <time datetime="2024-06-30T18:03:35+02:00">2024 06 30 </time></p>
  <div id="content">
    <h1 id='tinygrad dev exploration'>tinygrad dev exploration</h1><br />
<br />
<ul><br />
  <li><a href="#Direction">Direction</a></li><br />
  <li><a href="#More%20refined">More refined</a></li><br />
  <li><a href="#Less%20refined">Less refined</a><br />
    <ul><br />
      <li><a href="#tinycorp%20mission">tinycorp mission</a></li><br />
      <li><a href="#encountered%20python">encountered python</a></li><br />
      <li><a href="#creating%20a%20Tensor">creating a Tensor</a><br />
        <ul><br />
          <li><a href="#creating%20tensors%20with%20constructors">creating tensors with constructors</a></li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<h2 id='Direction'>Direction</h2><br />
<br />
<p>read tensor.py<br />
explore anything unfamiliar<br />
condense any writing<br />
create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</p><br />
<br />
<h2 id='More refined'>More refined</h2><br />
<br />
<h2 id='Less refined'>Less refined</h2><br />
<br />
<h3 id='tinycorp mission'>tinycorp mission</h3><br />
<br />
<p>accelerate, commoditize the petaflop<br />
improve soft-hardware interface for AI computesoftware first<br />
funded by love and tinyboxes</p><br />
<br />
<p>factory -&gt; soft (tinygrad), hard (tinybox, tinychip)<br />
product -&gt; compiled models</p><br />
<br />
<p><em>tinygrad model –&gt; friendly C –&gt; standalone would be (is?) nice</em></p><br />
<br />
<p>AI compute = tensors = multidimensional lists of floats</p><br />
<br />
<h3 id='encountered python'>encountered python</h3><br />
<br />
<p><code class="language-plaintext highlighter-rouge">__slots__</code> lists the expected class attributes for fast access and memory savings <a href="https://stackoverflow.com/questions/472000/usage-of-slots">more</a><br />
<code class="language-plaintext highlighter-rouge">all()</code> is True of all arguments evaluate to True<br />
<code class="language-plaintext highlighter-rouge">WeakValueDictionary</code> for accessing values that can be garbage collected like the reference isn’t there<br />
if there is an argument in a function definition like <code class="language-plaintext highlighter-rouge">*</code></p><br />
<br />
<h3 id='creating a Tensor'>creating a Tensor</h3><br />
<br />
<p>in <code class="language-plaintext highlighter-rouge">tensor.py</code></p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor(data, device=None, dtype=None, requires_grad=None)</code></p><br />
<br />
<p>determine device for the Tensor using <code class="language-plaintext highlighter-rouge">Device.canonicalize()</code></p><br />
<ul><br />
  <li>eligible devices are those for which exists a <code class="language-plaintext highlighter-rouge">runtime/ops_{device}.py</code></li><br />
  <li>if <code class="language-plaintext highlighter-rouge">device</code> is <code class="language-plaintext highlighter-rouge">None</code> and so cannot be canonicalized, it is set to the returned string from <code class="language-plaintext highlighter-rouge">Device.DEFAULT</code><br />
    <ul><br />
      <li>returns the device that is set to 1 as an environment variable</li><br />
      <li>if it finds none <code class="language-plaintext highlighter-rouge">{device}Device.__init__({device})</code> is tried for <code class="language-plaintext highlighter-rouge">METAL</code>,<code class="language-plaintext highlighter-rouge">AMD</code>,<code class="language-plaintext highlighter-rouge">CUDA</code>, <code class="language-plaintext highlighter-rouge">GPU</code>, <code class="language-plaintext highlighter-rouge">CLANG</code>, <code class="language-plaintext highlighter-rouge">LLVM</code> in their respective <code class="language-plaintext highlighter-rouge">runtime/ops_{device}.py</code><br />
        <ul><br />
          <li>for each device: <code class="language-plaintext highlighter-rouge">Compiled.__init__(device, MallocAllocator or {device allocator}, {Device renderer}, {Device compiler}, {device runtime}, {device graph})</code></li><br />
        </ul><br />
      </li><br />
      <li>the name of the first device this causes no problems with, is returned from <code class="language-plaintext highlighter-rouge">Device.DEFAULT</code> and set to 1 as an environment variable.</li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<p><em>TODO: if <code class="language-plaintext highlighter-rouge">DEBUG</code> &gt; 1, a message is printed informing which device was initialized</em></p><br />
<br />
<p>depending on type of data, sets data to return value of helper functions <code class="language-plaintext highlighter-rouge">_loadop()</code> or <code class="language-plaintext highlighter-rouge">_frompy</code><br />
Both return a <code class="language-plaintext highlighter-rouge">LazyBuffer</code> from calling:<br />
<code class="language-plaintext highlighter-rouge">LazyBuffer.loadop(op, shape, dtype, device, arg=None, src=(), enable_cache=False)</code></p><br />
<ul><br />
  <li><code class="language-plaintext highlighter-rouge">op</code> is either <code class="language-plaintext highlighter-rouge">LoadOps.EMPTY</code> or <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code>, which are just numbers (0 and 1 respectively) from the LoadOps Enumerator in <code class="language-plaintext highlighter-rouge">ops.py</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">shape</code> is <code class="language-plaintext highlighter-rouge">tuple()</code>or <code class="language-plaintext highlighter-rouge">(0,)</code> if <code class="language-plaintext highlighter-rouge">LoadOps.EMPTY</code> or the actual shape if the input was <code class="language-plaintext highlighter-rouge">tuple</code> or <code class="language-plaintext highlighter-rouge">list</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">dtype</code>is always tinygrad.dtype, either given or determined from data</li><br />
  <li><code class="language-plaintext highlighter-rouge">device</code> is what was determined above or <code class="language-plaintext highlighter-rouge">NPY</code> if the input is a list, tuple or ndarray and so is numpy. Since numpy is removed soon from tinygrad, I ingore this detail.</li><br />
  <li><code class="language-plaintext highlighter-rouge">arg</code> is <code class="language-plaintext highlighter-rouge">data</code> passed to the Tensor<br />
<code class="language-plaintext highlighter-rouge">LazyBuffer.loadop()</code> checks that src is a tuple and gets <code class="language-plaintext highlighter-rouge">ShapeTracker.from_shape(shape)</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">ShapeTracker((View.create(shape),))</code> to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function <code class="language-plaintext highlighter-rouge">strides_for_shape(shape)</code>, then canonicalized. Then <code class="language-plaintext highlighter-rouge">View(shape, stride, offset=0, mask=None, contiguous=True)</code> with these default arguments<br />
ShapeTracker is passed as an argument to the helper function <code class="language-plaintext highlighter-rouge">create_lazybuffer(device, ShapeTracker, dtype, op, arg, src, enable_cache)</code></li><br />
  <li>if <code class="language-plaintext highlighter-rouge">op==LoadOps.EMPTY</code>, the <code class="language-plaintext highlighter-rouge">size</code> of the ShapeTracker will be 0 and <code class="language-plaintext highlighter-rouge">op</code>will turn to <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code> and unless Tensor data was <code class="language-plaintext highlighter-rouge">Variable</code>.</li><br />
  <li>For reasons yet unknown, if <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code>(guaranteed at this point, unless data was <code class="language-plaintext highlighter-rouge">Variable</code>), then the data (now in <code class="language-plaintext highlighter-rouge">arg</code>) runs through pythons native <code class="language-plaintext highlighter-rouge">int()</code>, <code class="language-plaintext highlighter-rouge">float()</code> or <code class="language-plaintext highlighter-rouge">bool()</code> functions depending on its dtype.</li><br />
  <li>if the <code class="language-plaintext highlighter-rouge">LazyBuffer</code> is already <code class="language-plaintext highlighter-rouge">lazycache</code> and <code class="language-plaintext highlighter-rouge">enable_cache</code> is True, use that one.</li><br />
  <li>else create one: <code class="language-plaintext highlighter-rouge">LazyBuffer(device, st, dtype, op, arg, srcs, base=base)</code> (<code class="language-plaintext highlighter-rouge">base</code> is <code class="language-plaintext highlighter-rouge">None</code>)<br />
    <ul><br />
      <li>Creates a <code class="language-plaintext highlighter-rouge">Buffer(device, st.size, dtype)</code> with additional properties:<br />
        <ul><br />
          <li><code class="language-plaintext highlighter-rouge">self.options = None</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self.offset = 0</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self._base = None</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self.lb_refcount = 1</code><br />
The created <code class="language-plaintext highlighter-rouge">LazyBuffer</code> is stored in <code class="language-plaintext highlighter-rouge">Tensor.lazydata</code> after making sure it is on the right device</li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<h4 id='creating tensors through methods'>creating tensors through methods</h4><br />
<br />
<ul><br />
  <li>Tensor.empty - no new ops</li><br />
  <li>Tensor.zeros - <code class="language-plaintext highlighter-rouge">full(shape, 0, ...)</code></li><br />
  <li>Tensor.ones - <code class="language-plaintext highlighter-rouge">full(shape, 1, ...)</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">full(shape, fill_value)</code>:<br />
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Tensor</span><span class="p">(</span><span class="n">fill_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">).</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">new_shape</span> <span class="p">:</span><span class="o">=</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">))).</span><span class="n">expand</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span><br />
</code></pre></div>    </div><br />
  </li><br />
  <li>Tensor.arange - <code class="language-plaintext highlighter-rouge">full(shape, step, dtype, **kwargs)._cumsum() + (start - step)</code> -&gt; <code class="language-plaintext highlighter-rouge">.cast(dtype)</code></li><br />
  <li>Tensor.eye - <code class="language-plaintext highlighter-rouge">ones().pad().flatten().shrink().reshape()</code></li><br />
  <li>Tensor.full_like - <code class="language-plaintext highlighter-rouge">full</code></li><br />
  <li>Tensor.zeros_like <code class="language-plaintext highlighter-rouge">full_like</code></li><br />
  <li>Tensor.ones_like <code class="language-plaintext highlighter-rouge">full_like</code></li><br />
</ul><br />
<br />
<p>all Tensor constructors that aren’t random build on the <code class="language-plaintext highlighter-rouge">Tensor.full(shape, fill_value)</code> function, which first <em>reshapes</em> the Tensor with 1 element (fill_value) to the target number of dimensions.<br />
<code class="language-plaintext highlighter-rouge">Tensor.reshape</code> calls <code class="language-plaintext highlighter-rouge">F.Reshape.apply(self, new_shape)</code> from <code class="language-plaintext highlighter-rouge">function.py</code>, which inherits from <code class="language-plaintext highlighter-rouge">class Function</code> in <code class="language-plaintext highlighter-rouge">tensor.py</code>.</p><br />
<br />
<p>all <code class="language-plaintext highlighter-rouge">Function</code> “children”, in their <code class="language-plaintext highlighter-rouge">apply</code>function, create a new Tensor and populate it with new <code class="language-plaintext highlighter-rouge">lazydata</code>, <code class="language-plaintext highlighter-rouge">requires_grad</code>, <code class="language-plaintext highlighter-rouge">grad=None</code> and <code class="language-plaintext highlighter-rouge">_ctx</code> if <code class="language-plaintext highlighter-rouge">requires_grad</code> is True. <code class="language-plaintext highlighter-rouge">_ctx</code> contains the function that was called, which also contains the parent Tensors.</p><br />
<br />
<p>the <code class="language-plaintext highlighter-rouge">forward</code> method for <code class="language-plaintext highlighter-rouge">F.Reshape()</code> is called on the <code class="language-plaintext highlighter-rouge">lazydata</code>.<br />
<code class="language-plaintext highlighter-rouge">lazydata.reshape</code> turns into <code class="language-plaintext highlighter-rouge">self._view(st.reshape())</code> (st = ShapeTracker) in <code class="language-plaintext highlighter-rouge">lazy.py</code>.<br />
<code class="language-plaintext highlighter-rouge">ShapeTracker.reshape()</code> returns a new <code class="language-plaintext highlighter-rouge">ShapeTracker</code> with (by default) its latest <code class="language-plaintext highlighter-rouge">views</code> replaced by a new one with the new shape. if <code class="language-plaintext highlighter-rouge">MERGE_VIEWS=0</code>, the new view is appended to <code class="language-plaintext highlighter-rouge">views</code> instead.<br />
In the current case, the previous View with shape <code class="language-plaintext highlighter-rouge">(1,)</code> is directly replaced by the new one <code class="language-plaintext highlighter-rouge">(1,)*len(new_shape)</code>.<br />
finally, the tensor gets a new <code class="language-plaintext highlighter-rouge">LazyBuffer</code> from  <code class="language-plaintext highlighter-rouge">create_lazybuffer(self.device, new_st, self.dtype, base=self.base)</code></p><br />
<br />
<p>after the reshape, the dimension use <code class="language-plaintext highlighter-rouge">Tensor.expand(new_shape)</code> to get the now correct number of dimensions to the final shape.</p><br />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">_broadcast_to</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">from_</span> <span class="k">if</span> <span class="n">to</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">to</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">to</span> <span class="k">for</span> <span class="n">from_</span><span class="p">,</span> <span class="n">to</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">_pad_left</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">))))))</span><br />
</code></pre></div></div><br />
<p><code class="language-plaintext highlighter-rouge">argfix</code> ensures the function works even if the shape was not input as a tuple but through multiple arguments like <code class="language-plaintext highlighter-rouge">reshape(2,2,2)</code>.<br />
<code class="language-plaintext highlighter-rouge">_pad_left</code> gets inputs to the same number of dimensions.<br />
<code class="language-plaintext highlighter-rouge">*</code> unpacks the tuple with both shapes that <code class="language-plaintext highlighter-rouge">_pad_left</code> returns</p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor._broadcast_to(self, shape)</code> runs <code class="language-plaintext highlighter-rouge">_pad_left</code> again<br />
runs <code class="language-plaintext highlighter-rouge">self.reshape</code> again to the “padded” shape<br />
then <code class="language-plaintext highlighter-rouge">F.Expand.apply()</code> -&gt; <code class="language-plaintext highlighter-rouge">lazybuffer.expand()</code> -&gt; <code class="language-plaintext highlighter-rouge">shapetracker.expand()</code> -&gt; <code class="language-plaintext highlighter-rouge">View.expand()</code> which producees  a new <code class="language-plaintext highlighter-rouge">View</code> with the new shape and everything else being equal. returns a new <code class="language-plaintext highlighter-rouge">ShapeTracker</code>, returns a new <code class="language-plaintext highlighter-rouge">LazyBuffer</code>, returns a new <code class="language-plaintext highlighter-rouge">Tensor</code></p><br />
<br />
<p>Tensor.arange offers new stuff, calling <code class="language-plaintext highlighter-rouge">Tensor._cumsum()</code>, using Tensor-Int addition and casting the Tensor.<br />
from <code class="language-plaintext highlighter-rouge">Tensor._cumsum()</code>:</p><br />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">pad2d</span><span class="p">((</span><span class="n">pl_sz</span><span class="p">,</span><span class="o">-</span><span class="nb">int</span><span class="p">(</span><span class="n">_first_zero</span><span class="p">))).</span><span class="n">_pool</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">],)).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><br />
</code></pre></div></div><br />
<p>where <code class="language-plaintext highlighter-rouge">axis</code> is 0 and <code class="language-plaintext highlighter-rouge">pl_sz</code> will in this case be <code class="language-plaintext highlighter-rouge">self.shape[0] - 1</code></p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor.transpose(0, -1)</code>, which translates to <code class="language-plaintext highlighter-rouge">Tensor.permute(order)</code> where in the order dim 0 and the last dim were swapped. <code class="language-plaintext highlighter-rouge">permute</code> resolves orders with negative dim indices, error checks and runs <code class="language-plaintext highlighter-rouge">F.Permute.apply(self, order=resolve_order)</code> -&gt; <code class="language-plaintext highlighter-rouge">lazybuffer.permute(order)</code> -&gt; <code class="language-plaintext highlighter-rouge">ShapeTracker.permute(order)</code> -&gt; <code class="language-plaintext highlighter-rouge">View.permute(axis=order)</code> -&gt; <code class="language-plaintext highlighter-rouge">View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)</code><br />
returns a new <code class="language-plaintext highlighter-rouge">View</code>in a new <code class="language-plaintext highlighter-rouge">ShapeTracker</code> in a new <code class="language-plaintext highlighter-rouge">lazybuffer</code> in a new <code class="language-plaintext highlighter-rouge">Tensor</code><br />
this transpose changes nothing because the input was a 1D Tensor.</p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor.pad2d(self.shape[0] - 1, 0)</code> adds <code class="language-plaintext highlighter-rouge">self.shape[0] - 1</code> 0s to the left on the lowest dimension. Using <code class="language-plaintext highlighter-rouge">pad2d()</code> seems crazy here, it goes through <code class="language-plaintext highlighter-rouge">Tensor._slice()</code>, which eventually calls <code class="language-plaintext highlighter-rouge">Tensor.pad((self.shape[0] - 1, 0))</code> which is even crazier, which calls <code class="language-plaintext highlighter-rouge">F.Pad.apply(...)</code> which goes on the tour again.<br />
<code class="language-plaintext highlighter-rouge">LazyBuffer.pad()</code> -&gt; <code class="language-plaintext highlighter-rouge">ShapeTracker.pad()</code> -&gt; <code class="language-plaintext highlighter-rouge">View.pad()</code><br />
where <code class="language-plaintext highlighter-rouge">(self.shape[0] - 1, 0)</code> turns into  <code class="language-plaintext highlighter-rouge">(-self.shape[0] - 1, self.shape)</code>, which was already calculated in <code class="language-plaintext highlighter-rouge">Tensor.pad2d</code> for some reason.<br />
A mask is created: <code class="language-plaintext highlighter-rouge">((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))</code><br />
calling a trustworthy <code class="language-plaintext highlighter-rouge">View.__unsafe_resize(evernew_arg, new_mask)</code> where a new <code class="language-plaintext highlighter-rouge">View</code> is created with the extended <code class="language-plaintext highlighter-rouge">shape</code> (<code class="language-plaintext highlighter-rouge">self.shape[0] + self.shape[0] - 1</code>), <code class="language-plaintext highlighter-rouge">offset</code> of <code class="language-plaintext highlighter-rouge">-self.shape[0] - 1</code> and the <code class="language-plaintext highlighter-rouge">mask</code> as it was created. <code class="language-plaintext highlighter-rouge">contiguous</code> turns <code class="language-plaintext highlighter-rouge">False</code> whatever that means.</p><br />
<br />
<p>To see how mask, offset and maybe contiguous are interpreted, a detour to <code class="language-plaintext highlighter-rouge">Tensor.__getitem__()</code> follows. Or not, because <code class="language-plaintext highlighter-rouge">__getitem__</code> only returns more “metadata” and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from <code class="language-plaintext highlighter-rouge">Tensor.tolist()</code><br />
To return to: rest of <code class="language-plaintext highlighter-rouge">Tensor.arange</code>, other Tensor construction methods and random construction methods:</p><br />
<ul><br />
  <li>Tensor.manual_seed</li><br />
  <li>Tensor.rand</li><br />
  <li>Tensor.randn</li><br />
  <li>Tensor.randint</li><br />
  <li>Tensor.normal</li><br />
  <li>Tensor.uniform</li><br />
  <li>Tensor.scaled_uniform</li><br />
  <li>Tensor.glorot_uniform</li><br />
  <li>Tensor.kaiming_uniform</li><br />
  <li>Tensor.kaiming_normal</li><br />
</ul><br />
<br />
<h3 id='Realizing Tensors'>Realizing Tensors</h3><br />
<br />
<p>Starting from <code class="language-plaintext highlighter-rouge">Tensor([2,2,2]).pad(((2,0),)).tolist()</code></p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor.tolist()</code> -&gt; <code class="language-plaintext highlighter-rouge">Tensor.data().tolist()</code> -&gt; <code class="language-plaintext highlighter-rouge">Tensor._data().cast(self.dtype.fmt, self.shape).tolist()</code></p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor._data</code></p><br />
<ul><br />
  <li><code class="language-plaintext highlighter-rouge">Tensor.cast(self.dtype.scalar())</code> applies <code class="language-plaintext highlighter-rouge">F.Cast(dtype)</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">Tensor.contiguous()</code> applies <code class="language-plaintext highlighter-rouge">F.Contiguous()</code><br />
    <ul><br />
      <li><code class="language-plaintext highlighter-rouge">LazyBuffer.contigous()</code><br />
        <ul><br />
          <li><code class="language-plaintext highlighter-rouge">LazyBuffer.e(LoadOps.CONTIGUOUS)</code> in the current case<br />
            <ul><br />
              <li>makes sure dtypes and shapes(?) of all lazybuffers and their bases match</li><br />
              <li>“const folding”(?), which in the current case does nothing</li><br />
              <li>returns a new <code class="language-plaintext highlighter-rouge">LazyBuffer</code> with all sources (self and bases, in this case only self) in the <code class="language-plaintext highlighter-rouge">srcs</code> attribute</li><br />
            </ul><br />
          </li><br />
          <li>stores a reference and something in self.base.contiguous_child (?)</li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
  <li><code class="language-plaintext highlighter-rouge">Tensor.to("CLANG")</code><br />
    <ul><br />
      <li>if it is not already on CLANG, it makes a new Tensor with the same lazydata, but a different device, so it makes a copy.</li><br />
    </ul><br />
  </li><br />
  <li><code class="language-plaintext highlighter-rouge">Tensor.realize()</code><br />
    <ul><br />
      <li><code class="language-plaintext highlighter-rouge">run_schedule(*self.schedule_with_vars(), do_update_stats = True)</code><br />
        <ul><br />
          <li><code class="language-plaintext highlighter-rouge">self.schedule_with_vars()</code><br />
            <ul><br />
              <li>`create_schedule_with_vars(flatten(self.lazydata.lbs), seen=None)<br />
                <ul><br />
                  <li><code class="language-plaintext highlighter-rouge">flatten()</code> (comes from <code class="language-plaintext highlighter-rouge">helpers.py</code>) does nothing with the Tensors, just makes the lazybuffers not be nested in multiple lists</li><br />
                  <li><code class="language-plaintext highlighter-rouge">_graph_schedule(outs:List[LazyBuffer], seen=set())</code><br />
                    <ul><br />
                      <li><code class="language-plaintext highlighter-rouge">realises: Dict[LazyBuffer, None]</code> holds all unrealized lazybuffers</li><br />
                      <li><code class="language-plaintext highlighter-rouge">for out in outs: _recurse_lb(out.base, realizes, allbuffs = {}, simple_pads = set(), children = defaultdict, scheduled=True)</code></li><br />
                    </ul><br />
                  </li><br />
                </ul><br />
              </li><br />
            </ul><br />
          </li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
</ul><br />

  </div>
</article>
      </main>
    </div>
  </body>
</html>
