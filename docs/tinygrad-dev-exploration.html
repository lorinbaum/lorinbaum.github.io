<!DOCTYPE html>
<html lang="en" data-theme="dark-poole"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    tinygrad dev exploration
  </title>

  <link rel="stylesheet" href="/styles.css">
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/apple-touch-icon-precomposed.png"> -->
  <link rel="shortcut icon" href="/assets/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="" href="/atom.xml">


   <!-- for mathjax support --></head>
<body>
    <div class="container content">
      <main id="content">
        <nav>
  <a href='https://lorinbaumgarten.com//'>Entrance</a>
</nav>

<article class="post">
  <p class="post-date">Created <time datetime="2024-06-22T11:27:48+02:00">2024 06 22</time>, last changed <time datetime="2024-06-28T11:45:07+02:00">2024 06 28 </time></p>
  <div id="content">
    <h1 id='tinygrad dev exploration'>tinygrad dev exploration</h1><br />
<br />
<ul><br />
  <li><a href="#direction">Direction</a></li><br />
  <li><a href="#more%20refined">More refined</a></li><br />
  <li><a href="#less%20refined">Less refined</a><br />
    <ul><br />
      <li><a href="#tinycorp%20mission">tinycorp mission</a></li><br />
      <li><a href="#encountered%20python">encountered python</a></li><br />
      <li><a href="#creating%20a%20Tensor">creating a Tensor</a><br />
        <ul><br />
          <li><a href="#creating%20tensors%20with%20constructors">creating tensors with constructors</a></li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<h2 id='Direction'>Direction</h2><br />
<br />
<p>read tensor.py<br />
explore anything unfamiliar</p><br />
<br />
<h2 id='More refined'>More refined</h2><br />
<br />
<h2 id='Less refined'>Less refined</h2><br />
<br />
<h3 id='tinycorp mission'>tinycorp mission</h3><br />
<br />
<p>accelerate, commoditize the petaflop<br />
improve soft-hardware interface for AI computesoftware first<br />
funded by love and tinyboxes</p><br />
<br />
<p>factory -&gt; soft (tinygrad), hard (tinybox, tinychip)<br />
product -&gt; compiled models</p><br />
<br />
<p><em>tinygrad model –&gt; friendly C –&gt; standalone would be (is?) nice</em></p><br />
<br />
<p>AI compute = tensors = multidimensional lists of floats</p><br />
<br />
<h3 id='encountered python'>encountered python</h3><br />
<br />
<p><code class="language-plaintext highlighter-rouge">__slots__</code> lists the expected class attributes for fast access and memory savings <a href="https://stackoverflow.com/questions/472000/usage-of-slots">more</a><br />
<code class="language-plaintext highlighter-rouge">all()</code> is True of all arguments evaluate to True<br />
<code class="language-plaintext highlighter-rouge">WeakValueDictionary</code> for accessing values that can be garbage collected like the reference isn’t there</p><br />
<br />
<h3 id='creating a Tensor'>creating a Tensor</h3><br />
<br />
<p>in <code class="language-plaintext highlighter-rouge">tensor.py</code></p><br />
<br />
<p><code class="language-plaintext highlighter-rouge">Tensor(data, device=None, dtype=None, requires_grad=None)</code></p><br />
<br />
<p>determine device for the Tensor using <code class="language-plaintext highlighter-rouge">Device.canonicalize()</code></p><br />
<ul><br />
  <li>eligible devices are those for which exists a <code class="language-plaintext highlighter-rouge">runtime/ops_{device}.py</code></li><br />
  <li>if <code class="language-plaintext highlighter-rouge">device</code> is <code class="language-plaintext highlighter-rouge">None</code> and so cannot be canonicalized, it is set to the returned string from <code class="language-plaintext highlighter-rouge">Device.DEFAULT</code><br />
    <ul><br />
      <li>returns the device that is set to 1 as an environment variable</li><br />
      <li>if it finds none <code class="language-plaintext highlighter-rouge">{device}Device.__init__({device})</code> is tried for <code class="language-plaintext highlighter-rouge">METAL</code>,<code class="language-plaintext highlighter-rouge">AMD</code>,<code class="language-plaintext highlighter-rouge">CUDA</code>, <code class="language-plaintext highlighter-rouge">GPU</code>, <code class="language-plaintext highlighter-rouge">CLANG</code>, <code class="language-plaintext highlighter-rouge">LLVM</code> in their respective <code class="language-plaintext highlighter-rouge">runtime/ops_{device}.py</code><br />
        <ul><br />
          <li>for each device: <code class="language-plaintext highlighter-rouge">Compiled.__init__(device, MallocAllocator or {device allocator}, {Device renderer}, {Device compiler}, {device runtime}, {device graph})</code></li><br />
        </ul><br />
      </li><br />
      <li>the name of the first device this causes no problems with, is returned from <code class="language-plaintext highlighter-rouge">Device.DEFAULT</code> and set to 1 as an environment variable.</li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<p><em>TODO: if <code class="language-plaintext highlighter-rouge">DEBUG</code> &gt; 1, a message is printed informing which device was initialized</em></p><br />
<br />
<p>depending on type of data, sets data to return value of helper functions <code class="language-plaintext highlighter-rouge">_loadop()</code> or <code class="language-plaintext highlighter-rouge">_frompy</code><br />
Both return a <code class="language-plaintext highlighter-rouge">LazyBuffer</code> from calling:<br />
<code class="language-plaintext highlighter-rouge">LazyBuffer.loadop(op, shape, dtype, device, arg=None, src=(), enable_cache=False)</code></p><br />
<ul><br />
  <li><code class="language-plaintext highlighter-rouge">op</code> is either <code class="language-plaintext highlighter-rouge">LoadOps.EMPTY</code> or <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code>, which are just numbers (0 and 1 respectively) from the LoadOps Enumerator in <code class="language-plaintext highlighter-rouge">ops.py</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">shape</code> is <code class="language-plaintext highlighter-rouge">tuple()</code>or <code class="language-plaintext highlighter-rouge">(0,)</code> if <code class="language-plaintext highlighter-rouge">LoadOps.EMPTY</code> or the actual shape if the input was <code class="language-plaintext highlighter-rouge">tuple</code> or <code class="language-plaintext highlighter-rouge">list</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">dtype</code>is always tinygrad.dtype, either given or determined from data</li><br />
  <li><code class="language-plaintext highlighter-rouge">device</code> is what was determined above or <code class="language-plaintext highlighter-rouge">NPY</code> if the input is a list, tuple or ndarray and so is numpy. Since numpy is removed soon from tinygrad, I ingore this detail.</li><br />
  <li><code class="language-plaintext highlighter-rouge">arg</code> is <code class="language-plaintext highlighter-rouge">data</code> passed to the Tensor<br />
<code class="language-plaintext highlighter-rouge">LazyBuffer.loadop()</code> checks that src is a tuple and gets <code class="language-plaintext highlighter-rouge">ShapeTracker.from_shape(shape)</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">ShapeTracker((View.create(shape),))</code> to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function <code class="language-plaintext highlighter-rouge">strides_for_shape(shape)</code>, then canonicalized. Then <code class="language-plaintext highlighter-rouge">View(shape, stride, offset=0, mask=None, contiguous=True)</code> with these default arguments<br />
ShapeTracker is passed as an argument to the helper function <code class="language-plaintext highlighter-rouge">create_lazybuffer(device, ShapeTracker, dtype, op, arg, src, enable_cache)</code></li><br />
  <li>if <code class="language-plaintext highlighter-rouge">op==LoadOps.EMPTY</code>, the <code class="language-plaintext highlighter-rouge">size</code> of the ShapeTracker will be 0 and <code class="language-plaintext highlighter-rouge">op</code>will turn to <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code> and unless Tensor data was <code class="language-plaintext highlighter-rouge">Variable</code>.</li><br />
  <li>For reasons yet unknown, if <code class="language-plaintext highlighter-rouge">LoadOps.CONST</code>(guaranteed at this point, unless data was <code class="language-plaintext highlighter-rouge">Variable</code>), then the data (now in <code class="language-plaintext highlighter-rouge">arg</code>) runs through pythons native <code class="language-plaintext highlighter-rouge">int()</code>, <code class="language-plaintext highlighter-rouge">float()</code> or <code class="language-plaintext highlighter-rouge">bool()</code> functions depending on its dtype.</li><br />
  <li>if the <code class="language-plaintext highlighter-rouge">LazyBuffer</code> is already <code class="language-plaintext highlighter-rouge">lazycache</code> and <code class="language-plaintext highlighter-rouge">enable_cache</code> is True, use that one.</li><br />
  <li>else create one: <code class="language-plaintext highlighter-rouge">LazyBuffer(device, st, dtype, op, arg, srcs, base=base)</code> (<code class="language-plaintext highlighter-rouge">base</code> is <code class="language-plaintext highlighter-rouge">None</code>)<br />
    <ul><br />
      <li>Creates a <code class="language-plaintext highlighter-rouge">Buffer(device, st.size, dtype)</code> with additional properties:<br />
        <ul><br />
          <li><code class="language-plaintext highlighter-rouge">self.options = None</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self.offset = 0</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self._base = None</code></li><br />
          <li><code class="language-plaintext highlighter-rouge">self.lb_refcount = 1</code><br />
The created <code class="language-plaintext highlighter-rouge">LazyBuffer</code> is stored in <code class="language-plaintext highlighter-rouge">Tensor.lazydata</code> after making sure it is on the right device</li><br />
        </ul><br />
      </li><br />
    </ul><br />
  </li><br />
</ul><br />
<br />
<h4 id='creating tensors with constructors'>creating tensors with constructors</h4><br />
<br />
<ul><br />
  <li>empty - no new ops</li><br />
  <li>zeros - <code class="language-plaintext highlighter-rouge">full(shape, 0, ...)</code></li><br />
  <li>ones - <code class="language-plaintext highlighter-rouge">full(shape, 1, ...)</code></li><br />
  <li><code class="language-plaintext highlighter-rouge">full(shape, fill_value)</code>:<br />
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Tensor</span><span class="p">(</span><span class="n">fill_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">).</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">new_shape</span> <span class="p">:</span><span class="o">=</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">))).</span><span class="n">expand</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span><br />
</code></pre></div>    </div><br />
  </li><br />
  <li>arange - <code class="language-plaintext highlighter-rouge">full(shape, step, dtype, **kwargs)._cumsum() + (start - step)</code> -&gt; <code class="language-plaintext highlighter-rouge">.cast(dtype)</code></li><br />
  <li>eye - <code class="language-plaintext highlighter-rouge">ones().pad().flatten().shrink().reshape()</code></li><br />
  <li>full_like - <code class="language-plaintext highlighter-rouge">full</code></li><br />
  <li>zeros_like <code class="language-plaintext highlighter-rouge">full_like</code></li><br />
  <li>ones_like <code class="language-plaintext highlighter-rouge">full_like</code></li><br />
</ul><br />
<br />
<p>all Tensor constructors that aren’t random build on the <code class="language-plaintext highlighter-rouge">Tensor.full(shape, fill_value)</code> function, which first <em>reshapes</em> the Tensor with 1 element (fill_value) to the target number of dimensions.<br />
<code class="language-plaintext highlighter-rouge">Tensor.reshape</code> calls <code class="language-plaintext highlighter-rouge">F.Reshape.apply(self, new_shape)</code> from <code class="language-plaintext highlighter-rouge">function.py</code>, which inherits from <code class="language-plaintext highlighter-rouge">class Function</code> in <code class="language-plaintext highlighter-rouge">tensor.py</code>.</p><br />
<br />
<p>the <code class="language-plaintext highlighter-rouge">forward</code> function is called on the <code class="language-plaintext highlighter-rouge">lazydata</code>.<br />
<code class="language-plaintext highlighter-rouge">lazydata.reshape</code> turns into <code class="language-plaintext highlighter-rouge">self._view(st.reshape())</code> (st = ShapeTracker) in <code class="language-plaintext highlighter-rouge">lazy.py</code><br />
<code class="language-plaintext highlighter-rouge">ShapeTracker.reshape()</code> returns a new <code class="language-plaintext highlighter-rouge">ShapeTracker</code> with (by default) its latest <code class="language-plaintext highlighter-rouge">views</code> replaced by a new one with the new shape. if <code class="language-plaintext highlighter-rouge">MERGE_VIEWS</code>is 0, the new view is appended to <code class="language-plaintext highlighter-rouge">views</code> instead.<br />
In the current case, because the previous shape was <code class="language-plaintext highlighter-rouge">(1,)</code>, the new one <code class="language-plaintext highlighter-rouge">(1,)*len(new_shape)</code>, the new View directly replaces the old one.<br />
finally, the tensor gets a new <code class="language-plaintext highlighter-rouge">LazyBuffer</code> from  <code class="language-plaintext highlighter-rouge">create_lazybuffer(self.device, new_st, self.dtype, base=self.base)</code></p><br />
<br />
<p>all <code class="language-plaintext highlighter-rouge">Function</code> successors, in their <code class="language-plaintext highlighter-rouge">apply</code>function, create a new Tensor and populate it with new <code class="language-plaintext highlighter-rouge">lazydata</code>, <code class="language-plaintext highlighter-rouge">requires_grad</code>, <code class="language-plaintext highlighter-rouge">grad=None</code> and <code class="language-plaintext highlighter-rouge">_ctx</code>.</p><br />
<br />
<p>after the reshape, the dimension use <code class="language-plaintext highlighter-rouge">Tensor.expand(new_shape)</code> to get the now correct number of dimensions to the final shape.</p><br />
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">_broadcast_to</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">from_</span> <span class="k">if</span> <span class="n">to</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">to</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">to</span> <span class="k">for</span> <span class="n">from_</span><span class="p">,</span> <span class="n">to</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">_pad_left</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">))))))</span><br />
</code></pre></div></div><br />
<br />
<ul><br />
  <li>manual_seed</li><br />
  <li>rand</li><br />
  <li>randn</li><br />
  <li>randint</li><br />
  <li>normal</li><br />
  <li>uniform</li><br />
  <li>scaled_uniform</li><br />
  <li>glorot_uniform</li><br />
  <li>kaiming_uniform</li><br />
  <li>kaiming_normal</li><br />
</ul><br />

  </div>
</article>
      </main>
    </div>
  </body>
</html>
