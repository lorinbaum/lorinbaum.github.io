<!DOCTYPE html><html lang=en><head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>tinygrad dev exploration</title>
<link rel="stylesheet" href="../main.css">
<link rel="shortcut icon" href="../favicon.ico"></head><body><main><nav><a href='../index.html'>Entrance</a></nav><article><p class="post-date">Created <time datetime="2024-06-22T11:27:48+02:00">2024 06 22</time>, last changed <time datetime="2024-07-14T09:52:08.969142+00:00">2024 07 14</time></p>
<p>tinygrad tries to be simple. I like deleting things. See if I can't help delete in tinygrad. Seems to be a new and adventurous world on the other side.</p>
<h1 id="tinygrad%20dev%20exploration">tinygrad dev exploration</h1>
<div class="toc">
<ul>
<li><a href="#tinygrad%20dev%20exploration">tinygrad dev exploration</a><ul>
<li><a href="#Direction">Direction</a></li>
<li><a href="#More%20refined">More refined</a></li>
<li><a href="#Less%20refined">Less refined</a><ul>
<li><a href="#tinygrad%20inliner">tinygrad inliner</a></li>
<li><a href="#tinycorp%20mission">tinycorp mission</a></li>
<li><a href="#encountered%20python">encountered python</a></li>
<li><a href="#Importing%20Tensor">Importing Tensor</a></li>
<li><a href="#Creating%20a%20Tensor">Creating a Tensor</a></li>
<li><a href="#Adding%20to%20a%20Tensor">Adding to a Tensor</a></li>
<li><a href="#Realizing%20a%20Tensor">Realizing a Tensor</a></li>
<li><a href="#creating%20tensors%20through%20methods">creating tensors through methods</a></li>
<li><a href="#Detected%20room%20for%20improvement%20/%20questions">Detected room for improvement / questions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="Direction">Direction</h2>
<p>trace execution of a tinygrad script</p>
<ul>
<li>steps:<ul>
<li><code>from tinygrad.tensor import Tensor</code></li>
<li><code>Tensor([1,2,3])</code></li>
<li><code>Tensor([1,2,3]) + 2</code></li>
<li>`(Tensor([1,2,3]) + 2).tolist()</li>
</ul>
</li>
</ul>
<p>read tensor.py<br />
explore anything unfamiliar<br />
condense any writing<br />
create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</p>
<p>python inliner for tinygrad?</p>
<h2 id="More%20refined">More refined</h2>
<h2 id="Less%20refined">Less refined</h2>
<h3 id="tinygrad%20inliner">tinygrad inliner</h3>
<p>from reading its not obvious what happens<br />
could miss parts<br />
can't keep track of values<br />
reading too much that is irrelevant</p>
<p>inlined code would be a practical story through a structure of relationships<br />
tinygrad code lays out the structure directly</p>
<ul>
<li>write an executable python file that fulfills the same function as the traced script without calling functions</li>
<li>classes are maintained (Tensor, LazyBuffer(?))</li>
<li>function variables are renamed to be unique</li>
<li>function calls = indented comment = function name, source file, line number</li>
<li>return = indented comment</li>
<li>for loops and comprehensions are only shown once</li>
</ul>
<h3 id="tinycorp%20mission">tinycorp mission</h3>
<p>accelerate, commoditize the petaflop<br />
improve soft-hardware interface for tensor compute first<br />
funded by love and tinyboxes</p>
<p>factory -&gt; soft (tinygrad), hard (tinybox, tinychip?)<br />
product -&gt; compiled models?</p>
<p><em>tinygrad model --&gt; friendly C --&gt; standalone would be (is?) nice</em></p>
<h3 id="encountered%20python">encountered python</h3>
<p><code>__slots__</code> lists the expected class attributes for fast access and memory savings <a href="https://stackoverflow.com/questions/472000/usage-of-slots">more</a><br />
<code>all()</code> and <code>any()</code> for evaluating multiple bools.<br />
<code>WeakValueDictionary</code> for accessing values that can be garbage collected like the reference isn't there<br />
if there is an argument in a function definition like <code>*atuple</code>,  it becomes optional and returns an empty tuple (or list?) if not given<br />
<code>deque</code> from <code>collections</code> = data structure for efficient insertion and deletion from two ends of a list.</p>
<h3 id="Importing%20Tensor">Importing Tensor</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">tinygrad.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
</code></pre></div>

<p>sets the stage with 3749 lines of tinygrad code as determined through <code>sys.settrace</code> (2024-07-08 17:27)<br />
<img alt="" src="attachments/tinygrad_import_tensor.png" /><br />
Mostly <a href="https://docs.python.org/3/reference/import.html">imports</a> and the construction of the <code>PatternMatcher</code> in <code>tinygrad/codegen/uops.py</code> (marked with cyan left border)<br />
13: <code>helpery.py</code> </p>
<ul>
<li>makes <code>U</code> and <code>T</code> <code>TypeVar</code>s</li>
<li>determines if the computer runs OSX to set the location of tinygrads cache</li>
<li>sets and caches environment variables as <code>ContextVar</code> objects.<ul>
<li>DEBUG, IMAGE, BEAM, NOOPT, JIT</li>
<li>WINO, THREEFRY, CAPTURING</li>
<li>GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING</li>
<li>MULTIOUTPUT, PROFILE</li>
<li>this does not cover all environment variables relevant to tinygrad, not even those mentioned in the docs as <a href="https://docs.tinygrad.org/env_vars/#global-variables">global variables</a></li>
</ul>
</li>
<li>Global Counters: <code>global_ops</code>, <code>global_mem</code>, <code>time_sum_s</code>, <code>kernel_count</code>, <code>mem_used</code></li>
<li>ProfileLogger (?)</li>
<li>sets up cache db path, cachelevel and version (?)</li>
</ul>
<p>206: <code>dtype.py</code></p>
<ul>
<li><code>ConstType = Union[float, int, bool]</code></li>
<li>declares dtypes as DType Objects and some aliases:<ul>
<li>bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, float16, bfloat16, float32, float64</li>
<li>half = float16; float = float32; double = float64 </li>
<li>uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 </li>
<li>char = int8; short = int16; int = int32; long = int64</li>
</ul>
</li>
<li>sets default float by environment variable else <code>float32</code> and default int <code>int32</code></li>
<li><code>promo_lattice</code> that defines how different dtypes get promoted, presumably when different dtypes meet in an operation.</li>
<li><code>DTYPES_DICT</code> and <code>INVERSE_DTYPES_DICT</code> to translate between tinygrad dtypes and their names like "bool": dtypes.bool</li>
</ul>
<p>367: <code>shape/symbolic.py</code></p>
<ul>
<li><code>sint = Union[int, Variable, MulNode, SumNode]</code></li>
<li><code>render_python: Dict[Type, Callable[..., str]]</code>  where the callables return a string representing the Object in <code>Type</code>.</li>
</ul>
<p>581: <code>ops.py</code></p>
<ul>
<li>tinygrads ops are defined:<ul>
<li><code>UnaryOps(Enum)</code>: <code>EXP2</code>, <code>LOG2</code>, <code>CAST</code>, <code>BITCAST</code>, <code>SIN</code>, <code>SQRT</code>, <code>NEG</code>, <code>RECIP</code></li>
<li><code>BinaryOps(Enum)</code>: <code>ADD</code>, <code>MUL</code>, <code>IDIV</code>, <code>MAX</code>, <code>MOD</code>, <code>CMPLT</code>, <code>CMPNE</code>, <code>XOR</code>, <code>SHL</code>, <code>SHR</code>, <code>OR</code>, <code>AND</code></li>
<li><code>TernaryOps(Enum)</code>: <code>WHERE</code>, <code>MULACC</code></li>
<li><code>ReduceOps(Enum)</code>: <code>SUM</code>, <code>MAX</code></li>
<li><code>BufferOps(Enum)</code>: <code>LOAD</code>, <code>CONST</code>, <code>STORE</code></li>
<li><code>LoadOps(Enum)</code>: <code>EMPTY</code>, <code>CONST</code>, <code>COPY</code>, <code>CONTIGUOUS</code>, <code>CUSTOM</code>, <code>ASSIGN</code>, <code>VIEW</code></li>
</ul>
</li>
<li><code>Op = Union[UnaryOps, BinaryOps, ReduceOps, LoadOps, TernaryOps, BufferOps]</code></li>
<li><code>UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}</code></li>
<li><code>InterpretedFlopCounter: Dict[Op, Callable]</code> which generates <code>FlopCounter</code> objects with shape, flops and memory for various lazyops except <code>LoadOps</code>, <code>TernaryOps.MULACC</code></li>
<li><code>python_alu</code> implements lazyops using python and its math module. covers <code>UnaryOps</code> except <code>CAST</code> and <code>BITCAST</code>, <code>BinaryOps</code> and <code>TernaryOps.WHERE</code>.</li>
<li><code>truncate: Dict[DType, Callable]</code> providing functions to truncate any number to the desired dtype.</li>
</ul>
<p>754: <code>codegen/uops.py</code> (Note: quick reserach says UOps are really $\mu$ (micro) operations, UPat presumably is $\mu$ pattern)</p>
<ul>
<li>The <code>UOps(Enum)</code> class variables:<ul>
<li><code>SINK</code>,<code>VAR</code>,<code>DEFINE_GLOBAL</code>,<code>DEFINE_VAR</code>,<code>DEFINE_LOCAL</code>,<code>DEFINE_ACC</code>,<code>CONST</code>,<code>SPECIAL</code>,<code>NOOP</code>,<code>UNMUL</code>,<code>GEP</code></li>
<li><code>CAST</code>,<code>BITCAST</code>,<code>VECTORIZE</code>,<code>ALU</code>,<code>WMMA</code></li>
<li><code>LOAD</code>,<code>STORE</code>,<code>PHI</code></li>
<li><code>BARRIER</code>,<code>IF</code>,<code>RANGE</code></li>
<li><code>ENDRANGE</code>,<code>ENDIF</code></li>
</ul>
</li>
<li><code>TypeVar</code> <code>T</code></li>
<li><code>constant_folder</code> which constructs a <code>PatternMatcher</code> singleton with a <code>patterns:List[Tuple[Union[UPat, UOp], Callable]]</code> (~500 lines)</li>
<li><code>PatternMatcher</code>'s initialization takes ~1300 more lines as it constructs <code>UPat</code> objects and runs their <code>compile</code> function.</li>
</ul>
<p>2694: <code>device.py</code></p>
<ul>
<li><code>Device = _Device()</code> singleton, which populates <code>Device._devices</code> with strings of devices for which there is a <code>runtime/uops_{device}.py</code> file</li>
<li>sets defaults in <code>BufferOptions</code> class: <code>image = None</code>,<code>uncached</code>,<code>cpu_access</code>,<code>host</code>,<code>nolru</code> are all <code>False</code></li>
<li><code>MallocAllocator = _MallocAllocator()</code> singleton (no <code>__init__</code>)</li>
</ul>
<p>2816: <code>lazy.py</code></p>
<ul>
<li><code>lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()</code></li>
<li><code>view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}</code></li>
</ul>
<p>2920: <code>codegen/kernel.py</code></p>
<ul>
<li><code>OptOps(Enum)</code>: <code>TC</code>,<code>UPCAST</code>,<code>UPCASTMID</code>,<code>UNROLL</code>,<code>LOCAL</code>,<code>GROUP</code>,<code>GROUPTOP</code>,<code>NOLOCALS</code>,<code>PADTO</code></li>
<li><code>LocalBuffer</code> dataclass with <code>name</code>,<code>size</code>,<code>dtype=dtypes.float32</code>,<code>realized=None</code></li>
</ul>
<p>3007: <code>codegen/linearizer.py</code></p>
<ul>
<li><code>render_ops: Dict[Type, Callable[..., UOp]]</code><ul>
<li>for <code>NumNode</code>,<code>Variable</code>,<code>MulNode</code>,<code>DivNode</code>,<code>ModNode</code>,<code>LtNode</code>,<code>SumNode</code>,`AndNode</li>
</ul>
</li>
</ul>
<p>~3100: <code>engine/schedule.py</code></p>
<ul>
<li><code>SCHEDULES: List = []</code></li>
</ul>
<p>3299: <code>tensor.py</code></p>
<ul>
<li><code>Tensor</code> class with:<ul>
<li><code>__slots__ = "lazydata", "requires_grad", "grad", "_ctx"</code></li>
<li><code>__deletable__ = ('_ctx',)</code></li>
<li><code>training</code>, <code>no_grad</code> are <code>False</code></li>
<li><code>_seed = int(time.time())</code></li>
<li><code>_rng_counter = None</code></li>
</ul>
</li>
<li>produces methods on <code>Tensor</code>class for each device in <code>Device._devices</code> like <code>Tensor.cuda()</code> as aliases for <code>Tensor.to("cuda")</code></li>
<li>if <code>IMAGE</code> from environment variables <code>&gt;0</code>, creates more aliases for <code>Tensor.image_conv2d</code> and <code>Tensor.image_dot</code> by introducing <code>Tensor.conv2d</code> and <code>Tensor.dot</code> respectively.</li>
</ul>
<p>3646: <code>nn/state.py</code></p>
<ul>
<li><code>safe_dtypes</code>and <code>inverse_safe_dtype</code> dictionaries for translating between some naming (?) to tinygrad dtypes and back (inverse)</li>
</ul>
<p>3728: <code>engine/jit.py</code></p>
<ul>
<li><code>ReturnType = TypeVar("ReturnType")</code></li>
</ul>
<h3 id="Creating%20a%20Tensor">Creating a Tensor</h3>
<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="n">ConstType</span><span class="p">,</span>
        <span class="n">List</span><span class="p">,</span>
        <span class="n">Tuple</span><span class="p">,</span>
        <span class="n">LazyBuffer</span><span class="p">,</span>
        <span class="n">ndarray</span><span class="p">,</span>
        <span class="nb">bytes</span><span class="p">,</span>
        <span class="n">MultiLazyBuffer</span><span class="p">,</span>
        <span class="n">Variable</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">tinygrad.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</code></pre></div>

<p><img alt="" src="attachments/tinygrad_construct_tensor.png" /><br />
9656 lines (the linearizer-lowerer commit (<a href="https://github.com/tinygrad/tinygrad/commit/6972a2569f5a848b101f4c9310d5de373328dbfb">#4957</a>) changed this, documentation is paused as this might be cleaned up), the cyan line marks the border between previous import code and new tensor construction code. most new code comes from <code>runtime/autogen/cuda.py</code>(magenta left border) because in this case, cuda is the device it finds for the Tensor.</p>
<p>determine device for the Tensor using <code>Device.canonicalize()</code>, which merely formats <code>device</code> if it's not <code>None</code>, but since it is, responsibility is handed to <code>Device.DEFAULT</code> to find one.</p>
<ul>
<li>it looks for <code>{DEVICE}=1</code> in environment variables</li>
<li><code>Device[{device}]</code> is tried for <code>METAL</code>,<code>AMD</code>,<code>CUDA</code>, <code>GPU</code>, <code>CLANG</code>, <code>LLVM</code>, -&gt; <code>Device.__get_canonicalized_item</code> -&gt; eventually tries <code>{device}Device.__init__({device})</code> (like <code>CUDADevice</code>) in their respective <code>runtime/ops_{device}.py</code> until it finds one that returns no errors.<ul>
<li><code>METAL</code> fails within 3 lines when it tries to import the <code>Metal</code> library.</li>
<li><code>AMD</code> imports the <code>AMDRenderer</code> from <code>renderer/cstyle.py</code> (runs ~300 lines of importing and classvariable definitions), then imports from <code>runtime/driver/hip_comgr.py</code> which tries <code>runtime/autogen/comgr.py</code> and fails within 15 lines.</li>
<li><code>CUDA</code> should fail within ~30 lines when it tries to get <code>libcuda.so</code> but in this case cuda is installed, so it imports from <code>runtime/ops_cuda.py</code>, <code>runtime/autogen/cuda.py</code> (4000+ lines of mysterious code) and <code>runtime/autogen/nvrtc.py</code><ul>
<li><code>from tinygrad.renderer.cstyle import CUDARenderer</code> which is already available from the AMD attempt earlier.</li>
<li><code>from tinygrad.renderer.assembly import PTXRenderer</code></li>
<li><code>PTXRenderer</code> has lots of class variables:<ul>
<li><code>device="CUDA"</code>, <code>suffix="PTX"</code></li>
<li><code>global_max = (2147483647, 65535, 65535)</code>, <code>local_max = (1024, 1024, 64)</code>, <code>shared_max = 49152</code></li>
<li><code>tensor_cores: List[TensorCore]</code></li>
<li><code>kernel_prefix</code>, <code>barrier</code>, <code>gid</code>, <code>gdim</code>, <code>lid</code></li>
<li><code>asm_for_op:Dict[Op, Callable]</code> by all appearances functions for op-&gt;assembly translation</li>
<li><code>supports_half: List[Op]</code> with a small selection of ops</li>
<li><code>types: Dict[DType, str]</code> and <code>men_types: Dict[DType, str]</code> (almost identical, except for 3 types(?)) to translate between tinygrad dtypes and apparently some other convention</li>
<li><code>const_requires_mov: List[DType] = [dtypes.half, dtypes.bool]</code></li>
</ul>
</li>
<li><code>ptx_matcher</code> is another <code>PatternMatcher</code></li>
<li><code>PTX = getenv("PTX")</code>, 0 if not given.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>CUDADevice.__init__</code> gets itself <code>device_id</code>, <code>cu_device</code>, <code>context</code>, <code>arch</code>, <code>pending_copyin</code>, checking that the interactions with cuda (<code>libcuda.so</code>) return no errors on multiple occasions.<br />
<code>CUDADevice.devices.append(self)</code><br />
9406: <code>from tinygrad.runtime.graph.cuda import CUDAGraph</code><br />
calls</p>
<div class="codehilite"><pre><span></span><code><span class="n">Compiled</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">CUDAAllocator</span><span class="p">,</span>
    <span class="n">PTXRenderer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">)</span> <span class="k">if</span> <span class="n">PTX</span> <span class="k">else</span> <span class="n">CUDARenderer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">)</span><span class="err">`</span><span class="p">,</span> <span class="c1"># PTX=0 (default)</span>
    <span class="n">PTXCompiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">)</span> <span class="k">if</span> <span class="n">PTX</span> <span class="k">else</span> <span class="n">CUDACompiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">arch</span><span class="p">),</span>
    <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">CUDAProgram</span><span class="p">,</span> <span class="bp">self</span><span class="p">),</span>
    <span class="n">graph</span><span class="o">=</span><span class="n">CUDAGraph</span>
<span class="p">)</span>
</code></pre></div>

<p>which is the superclass of <code>CUDADevice</code>, where <code>dname</code>(device name), <code>allocator</code>, <code>renderer</code>, <code>compiler</code>, <code>runtime</code>, <code>graph</code>  come together and are stored in <code>self</code> (ultimately in <code>CUDADevice</code> as it inherits these instance variables from its parent classes.</p>
<ul>
<li><code>CUDAAllocator</code> inherits from <code>LRUAllocator</code>, calls <code>super().__init__()</code> which only runs <code>self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)</code>(sidenote: <code>LRUAllocator</code> itself also inherits from <code>Allocator</code>).</li>
<li><code>CUDARenderer</code> initialization in this case stores <code>[]</code> in <code>self.tensor_cores</code></li>
<li><code>CUDACompiler</code> (child of <code>Compiler</code>) gets itself <code>self.arch</code>, <code>self.compile_options</code> and <code>super().__init__(f"compile_cuda_{self.arch}")</code> which sets <code>self.cachekey</code> unless explicitly preventes through env variable <code>DISABLE_COMPILER_CACHE</code></li>
<li><code>CUDAGraph</code>, notably is not initialized, the imported class is just passed on.</li>
</ul>
<p>in <code>Compiler.__init__()</code> if <code>compiler</code> was <code>None</code> it would be replaced by the generic <code>Compiler()</code> and <code>renderer</code> by <code>Renderer()</code>.</p>
<p><code>CudaDevice</code> returned to <code>Device.__get_canonicalized_item</code> and cached (<code>@functools.lru_cache(maxsize=None)</code> decorator):</p>
<div class="codehilite"><pre><span></span><code><span class="n">CUDADevice</span> <span class="p">{</span>
    <span class="s1">&#39;cu_device&#39;</span><span class="p">:</span> <span class="n">c_int</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="s1">&#39;context&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tinygrad</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">autogen</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">LP_struct_CUctx_st</span> <span class="n">at</span> <span class="mh">0x7f7a12c49a40</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;arch&#39;</span><span class="p">:</span> <span class="s1">&#39;sm_61&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pending_copyin&#39;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s1">&#39;dname&#39;</span><span class="p">:</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span>
    <span class="s1">&#39;allocator&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tinygrad</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">ops_cuda</span><span class="o">.</span><span class="n">CUDAAllocator</span> <span class="n">at</span> <span class="mh">0x7f7a12dad9f0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;compiler&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tinygrad</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">ops_cuda</span><span class="o">.</span><span class="n">CUDACompiler</span> <span class="n">at</span> <span class="mh">0x7f7a12cd7b20</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;runtime&#39;</span><span class="p">:</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">tinygrad</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">ops_cuda</span><span class="o">.</span><span class="n">CUDAProgram</span><span class="s1">&#39;&gt;, &lt;tinygrad.runtime.ops_cuda.CUDADevice object at 0x7f7a12daeb60&gt;),</span>
    <span class="s1">&#39;graph&#39;</span><span class="p">:</span> <span class="n">tinygrad</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">,</span>
    <span class="s1">&#39;renderer&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">tinygrad</span><span class="o">.</span><span class="n">renderer</span><span class="o">.</span><span class="n">cstyle</span><span class="o">.</span><span class="n">CUDARenderer</span> <span class="n">at</span> <span class="mh">0x7f7a12c244f0</span><span class="o">&gt;</span>
<span class="p">}</span>
</code></pre></div>

<p>also this <code>CUDADevice</code> is stored in classvariable <code>CUDADevice.devices:List[CUDADevice]</code><br />
if <code>DEBUG&gt;=1</code>, a message will inform that the device was opened.</p>
<p>for now, the returned <code>CUDADevice</code> only demonstrates that <code>CUDA</code> can be used as a device for the new Tensor. environmentvariable <code>CUDA</code> is set to <code>1</code> to save this work in the future.</p>
<p>In Tensor construction, depending on type of data input, <code>_loadop()</code>, <code>_fromnp</code> or <code>_frompy</code> create the tensors <code>LazyBuffer</code>.<br />
The example Tensor construction determines dtype (<code>dtypes.default_int</code>), then<br />
<code>data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))</code><br />
(numpy as a dependency is phased out, so this probably changes soon)<br />
<code>_from_np_dtype</code> uses a dictionary from <code>dtype.py</code> to translate the numpy dtype to a tinygrad <code>DType</code><br />
-&gt; <code>LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")</code></p>
<div class="codehilite"><pre><span></span><code><span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">loadop</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="n">sint</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="n">dtype</span><span class="p">:</span><span class="n">DType</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">arg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span><span class="o">=</span><span class="p">(),</span> <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LazyBuffer</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">create_lazybuffer</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">enable_cache</span><span class="o">=</span><span class="n">enable_cache</span><span class="p">)</span>
</code></pre></div>

<p><code>op</code> was given as <code>LoadOps.EMPTY</code></p>
<div class="codehilite"><pre><span></span><code><span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span><span class="n">Tuple</span><span class="p">[</span><span class="n">sint</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span> <span class="k">return</span> <span class="n">ShapeTracker</span><span class="p">((</span><span class="n">View</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">shape</span><span class="p">),))</span>
</code></pre></div>

<p><code>ShapeTracker((View.create(shape),))</code> to give the ShapeTracker a View. Since no stride is defined, it will be created using <code>strides_for_shape(shape)</code>, then canonicalized. Then <code>View(shape, stride, offset=0, mask=None, contiguous=True)</code> with these default arguments.</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">View</span><span class="p">:</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">sint</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">sint</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">offset</span><span class="p">:</span> <span class="n">sint</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">sint</span><span class="p">,</span> <span class="n">sint</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span>
    <span class="n">contiguous</span><span class="p">:</span> <span class="nb">bool</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ShapeTracker</span><span class="p">:</span>
    <span class="n">views</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">View</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">create_lazybuffer</span><span class="p">(</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">st</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">DType</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Op</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">arg</span><span class="p">:</span><span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">srcs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">enable_cache</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;LAZYCACHE&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div>

<p>in <code>create_lazybuffer</code> the <code>lazycache</code> is interacted with, which stores lazybuffers. a <code>cache_key</code> is generated from the lazybuffers parameters. If the key yields an existing <code>LazyBuffer</code> from <code>lazycache</code>, that one will return, otherwise a new one is created with this constructor, where it will pass <code>metadata=_METADATA.get()</code> as <code>metadata</code> (<a href="https://github.com/tinygrad/tinygrad/commit/9150a6be7a30bbd17f0b84f3352fac7af0c68b73">#5271</a>):</p>
<div class="codehilite"><pre><span></span><code><span class="n">LazyBuffer</span><span class="p">(</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">st</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">DType</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Op</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">arg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">srcs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Metadata</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p><code>st</code> is the <code>ShapeTracker</code> just created</p>
<p>In the lazybuffer's initialization, it finds that <code>base</code> is <code>None</code> and decides that an assignment to <code>self.buffer</code> is in order.<br />
Given the op <code>LoadOps.EMPTY</code>, it makes a <code>Buffer</code> (a class imported from <code>tinygrad.device</code>) through <code>Buffer(device, self.size, dtype)</code>. But creating it like that in this case does nothing except store the instance.<br />
the buffer's <code>_lb_refcount</code> property is incremented by 1<br />
the <code>contiguous_child</code> property (didn't exist before) is set to <code>None</code><br />
and <code>forced_realize</code> to <code>False</code><br />
the meaning of all 3 escapes me right now.</p>
<p>The <code>LazyBuffer</code> is done and returning to <code>_fromnp()</code> into the variable <code>ret</code> where:<br />
<code>ret.buffer.allocate(x)</code> (x is a numpy array) causes the buffer to find itself an <code>Allocator</code>:<br />
<code>self.allocator = Device[self.device].allocator</code>. Indexing into <code>Device</code> returns a <code>NpyDevice</code> (same as earlier when it was about finding an available device, but this time with <code>NPY</code>. This device is very minimal, has the default <code>Compiler</code> and <code>Renderer</code> and a mostly empty <code>NpyAllocator</code>)</p>
<p>on <code>buffer.allocate(x)</code> where <code>x</code> is the <code>np.ndarray</code>, <code>x</code> is just assigned to <code>buffer._buf</code>, without calling <code>Buffer.alloc</code> which is not implemented for this device.<br />
completing what is commented "fake realize" in <code>_fromnpy</code>, <code>del ret.srcs</code> (which was <code>()</code>) makes sure that <code>LazyBuffer.realized</code> will return <code>True</code>.<br />
Also adds the buffer's size to <code>GlobalCounters.mem_used</code></p>
<p>In the final step of <code>Tensor</code> initialization, the mismatching devices, one being the discovered one (<code>CUDA</code> in this case) and one being <code>NPY</code> are detected and <code>self.lazydata = data.copy_to_device(device)</code> takes care of it, <code>data</code> being the created <code>LazyBuffer</code> and <code>device</code> being the discovered device from the start.<br />
<code>LazyBuffer.copy_to_device(device)</code> in this case leads to <code>self.base._copy(device)._view(self.st)</code></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># LazyBuffer._copy:</span>
<span class="k">return</span> <span class="n">create_lazybuffer</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">nbytes</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="n">enable_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>assign a  <code>Buffer</code> to the <code>LazyBuffer</code>, because <code>base</code> is <code>None</code> again (the npy lazybuffer is stored in <code>srcs</code>).</p>
<p>the <code>._view(self.st)</code> that follows <code>._copy(device)</code>, does nothing here, because the new shapetracker has the same shape and is contiguous.</p>
<p>The final object looks like this:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span> <span class="p">{</span>
    <span class="s1">&#39;_ctx&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;requires_grad&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;lazydata&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span>
        <span class="s1">&#39;st&#39;</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
            <span class="s1">&#39;strides&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> 
            <span class="s1">&#39;offset&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;mask&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;contiguous&#39;</span><span class="p">:</span> <span class="kc">True</span>
            <span class="p">)</span>
        <span class="p">,)),</span>
        <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
        <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
        <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;_base&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;op&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="s1">&#39;arg&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
        <span class="s1">&#39;srcs&#39;</span><span class="p">:</span> <span class="n">LazyBuffer</span> <span class="p">{</span>
            <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;NPY&#39;</span><span class="p">,</span>
            <span class="s1">&#39;st&#39;</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span>
                <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                <span class="s1">&#39;strides&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                <span class="s1">&#39;offset&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s1">&#39;mask&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s1">&#39;contiguous&#39;</span><span class="p">:</span> <span class="kc">True</span>
                <span class="p">)</span>
            <span class="p">,)),</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
            <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;_base&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;op&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="s1">&#39;arg&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;buffer&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">True</span> <span class="n">device</span><span class="p">:</span><span class="n">NPY</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="s1">&#39;contiguous_child&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;forced_realize&#39;</span><span class="p">:</span> <span class="kc">False</span>
        <span class="p">},</span>      
        <span class="s1">&#39;buffer&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="s1">&#39;contiguous_child&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;forced_realize&#39;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p><code>Tensor</code> also has some classvariables, ignored here, can be seen in <a href="#Importing%20Tensor">Importing Tensor</a> at <code>tensor.py</code>.</p>
<h3 id="Adding%20to%20a%20Tensor">Adding to a Tensor</h3>
<div class="codehilite"><pre><span></span><code><span class="n">t</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="o">+</span> <span class="mi">2</span>
</code></pre></div>

<p>goes to <code>Tensor.add(self, x, reverse=False)</code><br />
-&gt; <code>return F.Add.apply(*self._broadcasted(x, reverse))</code></p>
<p><code>self._broadcasted</code> determines dtype then creates Tensor from <code>y</code> (2) using:<br />
<code>Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)</code><br />
where <code>dtypes.as_const()</code> casts the input using one of pythons <code>int</code>, <code>float</code>, <code>bool</code> functions. Reason still escapes me.</p>
<p>bypassing the whole numpy story because data is integer and not array this time, so lazybuffer comes more directly from <code>_loadop(LoadOps.CONST, tuple(), dtype, device, data)</code> where <code>data</code> eventually ends up as the lazybuffers <code>arg</code> property.</p>
<p>The <code>ShapeTracker</code> will be empty, because the provided shape is <code>tuple()</code>. (its a 0D Tensor)</p>
<p>Because <code>op</code> is <code>LoadOps.CONST</code> and dtype is <code>int</code> the data once again runs through <code>dtypes.as_const()</code> and <code>enable_cache</code> (-&gt; <code>lazycache</code>)  is enabled.</p>
<p>the returned <code>Tensor.lazydata</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span><span class="o">.</span><span class="n">lazydata</span> <span class="p">{</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span>
    <span class="s1">&#39;st&#39;</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(),</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">),)),</span>
    <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
    <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(),</span>
    <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;metadata&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;_base&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;op&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;arg&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;srcs&#39;</span><span class="p">:</span> <span class="p">(),</span>
    <span class="s1">&#39;buffer&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">1</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;contiguous_child&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s1">&#39;forced_realize&#39;</span><span class="p">:</span> <span class="kc">False</span>
<span class="p">}</span>
</code></pre></div>

<p>back in <code>_broadcasted</code>, dtypes of x and y are matched<br />
<code>_broadcast_shape(x.shape, y.shape)</code> determines a target shape<br />
and broadcast <code>x</code> and <code>y</code> to that shape (x is already that shape so nothing happens)</p>
<p><code>padded = _pad_left(y.shape, shape)</code> where <code>shape</code> is the target shape transforms <code>()</code> to <code>(1,)</code>, ready to be expanded through <code>F.Expand.apply(self.reshape(padded), shape=shape)</code></p>
<p><code>Tensor.reshape</code> calls <code>F.Reshape.apply(self, new_shape)</code> from <code>function.py</code>, which inherits from <code>class Function</code> in <code>tensor.py</code>.<br />
all <code>Function</code> "children", in their <code>apply</code>function, return a new Tensor and populate it with new <code>lazydata</code>, <code>requires_grad</code>, <code>grad=None</code> and <code>_ctx</code> if  applicable. <code>_ctx</code> contains the function that was called, which also contains the parent Tensors.<br />
<code>Function.apply()</code> calls the functions <code>forward</code> method on the <code>Tensor.lazydata</code></p>
<p><code>lazydata.reshape</code> turns into <code>self._view(st.reshape(newShape))</code> in <code>lazy.py</code>.<br />
In <code>st.reshape(newShape)</code> (<code>shapetracker.py</code>), by default, the new returned <code>ShapeTracker</code> will have its most recent view in <code>views</code> replaced by a new one, through <code>View.reshape(newShape)</code>.<br />
Environment variable <code>MERGE_VIEWS=0</code> changes this behaviour to including all previous views with the new one appended in the new shapetracker.</p>
<p><code>View.reshape(newShape)</code> in this case simply returns a new View from <code>View.create(newShape)</code><br />
strides for the new shape  are determined (<code>strides_for_shape(shape)</code> -&gt; <code>(1,)</code>) and canonicalized -&gt; <code>(0,)</code>.<br />
finally:</p>
<div class="codehilite"><pre><span></span><code><span class="n">contiguous</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">strides</span> <span class="o">==</span> <span class="n">strides_for_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="k">return</span> <span class="n">View</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">contiguous</span><span class="p">)</span>
</code></pre></div>

<p>back at <code>_view(newShapetracker)</code> in <code>lazy.py</code> a new lazybuffer comes from <code>create_lazybuffer(self.device, new_st, self.dtype, base=self.base)</code>.<br />
notably, <code>self.base</code> is just <code>self</code> because <code>self._base</code> is <code>None</code></p>
<div class="codehilite"><pre><span></span><code><span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">base</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LazyBuffer</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span>
</code></pre></div>

<p>next from <code>F.Expand.apply(self.reshape(padded), shape=(3,))</code>, where <code>self.reshape(padded)</code> has now returned the new Tensor. Expand similarly returns a new Tensor with a new LazyBuffer from  <code>LazyBuffer.expand</code> -&gt; <code>ShapeTracker.expand</code> -&gt; <code>View.expand</code> -&gt; <code>View.create(new_shape, self.strides, self.offset, mask)</code> -&gt; <code>View</code> -&gt; <code>ShapeTracker</code> -&gt; <code>LazyBuffer._view</code> -&gt; <code>createLazyBuffer</code> -&gt; <code>LazyBuffer</code></p>
<p>notably, <code>View.create</code> does not change strides and since no mask was given it also remains <code>None</code>. These lines:</p>
<div class="codehilite"><pre><span></span><code><span class="n">contiguous</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">strides</span> <span class="o">==</span> <span class="n">strides_for_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="k">return</span> <span class="n">View</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">contiguous</span><span class="p">)</span>
</code></pre></div>

<p>cause <code>contiguous</code> to be <code>False</code> because the unchaged stride is <code>(0,)</code>, but the appropriate stride for the new shape would be <code>(1,)</code><br />
Notably, <code>create_lazybuffer(self.device, new_st, self.dtype, base=self.base)</code> takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer. So in the final Tensor, there remains no reference to the reshape lazybuffer:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span><span class="p">:</span>
    <span class="s1">&#39;_ctx&#39;</span><span class="p">:</span> <span class="kc">None</span>
    <span class="s1">&#39;requires_grad&#39;</span> <span class="p">:</span> <span class="kc">None</span>
    <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="kc">None</span>
    <span class="s1">&#39;lazydata&#39;</span><span class="p">:</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s2">&quot;CUDA&quot;</span>
        <span class="s1">&#39;st&#39;</span> <span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:(</span><span class="mi">3</span><span class="p">,),</span>
            <span class="s1">&#39;strides&#39;</span><span class="p">:(</span><span class="mi">0</span><span class="p">,),</span>
            <span class="s1">&#39;offset&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;mask&#39;</span><span class="p">:</span><span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;contiguous&#39;</span><span class="p">:</span><span class="kc">False</span>
        <span class="p">),))</span>
        <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span>
        <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span>
        <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">3</span>
        <span class="s1">&#39;_base&#39;</span><span class="p">:</span> <span class="n">LazyBuffer</span><span class="p">:</span>
            <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s2">&quot;CUDA&quot;</span>
            <span class="s1">&#39;st&#39;</span><span class="p">:</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span>
                <span class="s1">&#39;shape&#39;</span><span class="p">:(),</span>
                <span class="s1">&#39;strides&#39;</span><span class="p">:(),</span>
                <span class="s1">&#39;offset&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
                <span class="s1">&#39;mask&#39;</span><span class="p">:</span><span class="kc">None</span><span class="p">,</span>
                <span class="s1">&#39;contiguous&#39;</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">),))</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">()</span>
            <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">1</span>
            <span class="s1">&#39;_base&#39;</span><span class="p">:</span> <span class="kc">None</span>
            <span class="s1">&#39;op&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span>
            <span class="s1">&#39;arg&#39;</span><span class="p">:</span> <span class="mi">2</span>
            <span class="s1">&#39;srcs&#39;</span><span class="p">:</span> <span class="p">()</span>
            <span class="s1">&#39;buffer&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">1</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span>
            <span class="s1">&#39;contiguous_child&#39;</span><span class="p">:</span> <span class="kc">None</span>
            <span class="s1">&#39;forced_realize&#39;</span><span class="p">:</span> <span class="kc">False</span>
</code></pre></div>

<p>Finally, <code>F.Add.apply</code> is called on the input tensor and the created Tensor.<br />
new tensor lazydata = <code>return x.e(BinaryOps.ADD, y)</code> where <code>BinaryOps.ADD</code>, like <code>LoadOps.CONST</code> is an entry in <code>class BinaryOps(Enum)</code></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">e</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> 
    <span class="n">op</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">LoadOps</span><span class="p">,</span> <span class="n">UnaryOps</span><span class="p">,</span> <span class="n">BinaryOps</span><span class="p">,</span> <span class="n">TernaryOps</span><span class="p">],</span>
    <span class="o">*</span><span class="n">in_srcs</span><span class="p">:</span><span class="n">LazyBuffer</span><span class="p">,</span>
    <span class="n">arg</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LazyBuffer</span>
</code></pre></div>

<p>gets <code>out_dtype</code> from input<br />
tries shortcuts if one of the operants is effectively 0</p>
<div class="codehilite"><pre><span></span><code><span class="n">create_lazybuffer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">out_dtype</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">srcs</span><span class="p">))</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">_ctx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">lazydata</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;CUDA&quot;</span>
        <span class="n">st</span> <span class="o">=</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">contiguous</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="p">),))</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span>
        <span class="n">_base</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">op</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span>
        <span class="n">arg</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">srcs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span> <span class="c1"># previously created lazybuffer [1,2,3] copied from NPY </span>
            <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span><span class="n">View</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">contiguous</span><span class="o">=</span><span class="kc">False</span><span class="p">),))</span><span class="o">&gt;</span> <span class="c1"># new lazybuffer from 2</span>
        <span class="p">)</span>
        <span class="n">buffer</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span>
        <span class="n">contiguous_child</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">forced_realize</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>

<p>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through <code>srcs</code> (in ADD in this case) or <code>_base</code> (in shape changes) and so form a graph.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">DEBUG</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from tinygrad.tensor import Tensor; (Tensor([1,2,3]) + 2).tolist()&quot;</span>
</code></pre></div>

<p>displays a graph that seem to echo this, though shape changes are apparently left out</p>
<div class="codehilite"><pre><span></span><code>  0  BufferOps.STORE MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))
  1   BinaryOps.ADD None
  2     BufferOps.LOAD MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))
  3     BufferOps.CONST ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)))
</code></pre></div>

<h3 id="Realizing%20a%20Tensor">Realizing a Tensor</h3>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div>

<p><code>Tensor.tolist()</code> = <code>Tensor.data().tolist()</code> = <code>Tensor._data().cast(self.dtype.fmt, self.shape).tolist()</code></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">memoryview</span><span class="p">:</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span> <span class="k">return</span> <span class="nb">memoryview</span><span class="p">(</span><span class="nb">bytearray</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="c1"># NOTE: this realizes on the object from as_buffer being a Python object</span>
    <span class="n">cpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">scalar</span><span class="p">())</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;CLANG&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">realize</span><span class="p">()</span>
    <span class="n">buf</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Buffer</span><span class="p">,</span> <span class="n">cast</span><span class="p">(</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">cpu</span><span class="o">.</span><span class="n">lazydata</span><span class="p">)</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">realized</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="s2">&quot;CLANG&quot;</span><span class="p">:</span> <span class="n">buf</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="n">BufferOptions</span><span class="p">(</span><span class="n">nolru</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">buf</span><span class="o">.</span><span class="n">as_buffer</span><span class="p">(</span><span class="n">allow_zero_copy</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="s2">&quot;CLANG&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p><code>Tensor.cast(self.dtype.scalar())</code> does nothing because <code>self.dtype == self.dtype.scalar()</code> in this case.<br />
<code>Tensor.contiguous()</code> -&gt; <code>lazydata.base.forced_realize = True</code>, otherwise nothing in this case, because not needed.<br />
<code>Tensor.to("CLANG")</code>. if it is not already on CLANG, it makes a new Tensor with the same lazydata, but <code>device="CLANG"</code>, so it add a <code>LoadOps.COPY</code> Lazybuffer to the graph.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">realize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">lst</span><span class="p">:</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">do_update_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">run_schedule</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">schedule_with_vars</span><span class="p">(</span><span class="o">*</span><span class="n">lst</span><span class="p">),</span> <span class="n">do_update_stats</span><span class="o">=</span><span class="n">do_update_stats</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">schedule_with_vars</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">lst</span><span class="p">:</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">seen</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]]</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ScheduleItem</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
    <span class="c1"># left out some lines that aren&#39;t executed</span>
    <span class="n">schedule</span><span class="p">,</span> <span class="n">var_vals</span> <span class="o">=</span> <span class="n">create_schedule_with_vars</span><span class="p">(</span><span class="n">flatten</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">lazydata</span><span class="o">.</span><span class="n">lbs</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span><span class="o">+</span><span class="n">lst</span><span class="p">]),</span> <span class="n">seen</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">memory_planner</span><span class="p">(</span><span class="n">schedule</span><span class="p">),</span> <span class="n">var_vals</span>
</code></pre></div>

<p>where <code>flatten</code> in this case returns a list with one entry: the "BinaryOps.ADD-lazybuffer" </p>
<p>from <code>engine/schedule.py</code></p>
<div class="codehilite"><pre><span></span><code><span class="n">SCHEDULES</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">def</span> <span class="nf">create_schedule_with_vars</span><span class="p">(</span>
    <span class="n">outs</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">],</span>
    <span class="n">seen</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]]</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ScheduleItem</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>

  <span class="k">if</span> <span class="n">seen</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">graph</span><span class="p">,</span> <span class="n">in_degree</span><span class="p">,</span> <span class="n">prescheduled</span> <span class="o">=</span> <span class="n">_graph_schedule</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="n">seen</span><span class="p">)</span>
</code></pre></div>

<p>from <code>engine/schedule.py</code></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_graph_schedule</span><span class="p">(</span>
    <span class="n">outs</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">],</span>
    <span class="n">seen</span><span class="p">:</span><span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]],</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">_LBScheduleItem</span><span class="p">]</span>
<span class="p">]:</span>

<span class="w">  </span><span class="sd">&quot;&quot;&quot;create a graph for realizing the outputs&quot;&quot;&quot;</span>
  <span class="c1"># start by just realizing the buffers passed in</span>
  <span class="n">realizes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="p">:</span><span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outs</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">}</span>
  <span class="n">allbufs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">simple_pads</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">children</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">:</span> <span class="n">_recurse_lb</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">allbufs</span><span class="p">,</span> <span class="n">simple_pads</span><span class="p">,</span> <span class="n">children</span><span class="p">,</span> <span class="n">scheduled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="err">```</span>
<span class="n">strange</span> <span class="n">that</span> <span class="n">it</span> <span class="n">uses</span> <span class="err">`</span><span class="n">out</span><span class="o">.</span><span class="n">base</span><span class="err">`</span> <span class="n">it</span> <span class="n">means</span> <span class="k">if</span> <span class="n">the</span> <span class="n">latest</span> <span class="n">lazybuffer</span> <span class="n">was</span> <span class="n">already</span> <span class="n">on</span> <span class="n">clang</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">reshape</span><span class="p">,</span> <span class="n">it</span> <span class="n">would</span> <span class="n">be</span> <span class="n">ignored</span> <span class="k">for</span> <span class="n">now</span><span class="o">.</span>

<span class="kn">from</span> <span class="err">`</span><span class="n">engine</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">py</span><span class="err">`</span>
<span class="err">```</span><span class="n">python</span>
<span class="k">def</span> <span class="nf">_recurse_lb</span><span class="p">(</span>
    <span class="n">buf</span><span class="p">:</span><span class="n">LazyBuffer</span><span class="p">,</span>
    <span class="n">realizes</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">allbufs</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">simple_pads</span><span class="p">:</span><span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">],</span>
    <span class="n">children</span><span class="p">:</span><span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span>
    <span class="n">scheduled</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>

<span class="w">  </span><span class="sd">&quot;&quot;&quot;recursively search the entire graph for all LazyBuffers, insert realizes after expands&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">allbufs</span> <span class="ow">or</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span>
  <span class="k">if</span> <span class="n">GRAPH</span><span class="p">:</span> <span class="n">log_lazybuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">scheduled</span><span class="p">)</span>
  <span class="c1"># view</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span> <span class="o">!=</span> <span class="n">buf</span><span class="p">:</span>
    <span class="c1"># fuse some pads</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">views</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">views</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">all_int</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="n">prod</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">prod</span><span class="p">([</span><span class="n">y</span><span class="o">-</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">views</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mask</span><span class="p">]):</span>
      <span class="n">simple_pads</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="p">)</span>
    <span class="c1"># realize all expands</span>
    <span class="k">elif</span> <span class="n">prod</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">prod</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">ReduceOps</span> <span class="ow">and</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span>
        <span class="k">pass</span> <span class="c1"># don&#39;t realize reduceops on const (unless base is forced_realize)</span>
      <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">UnaryOps</span><span class="o">.</span><span class="n">CAST</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">ImageDType</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">arg</span><span class="p">,</span> <span class="n">ImageDType</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># don&#39;t realize image to image casts. this is part of a larger problem</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">realizes</span><span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># check all other pads for safe fusion</span>
    <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">views</span><span class="p">):</span> <span class="n">simple_pads</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_recurse_lb</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">allbufs</span><span class="p">,</span> <span class="n">simple_pads</span><span class="p">,</span> <span class="n">children</span><span class="p">)</span>
  <span class="c1"># base</span>
  <span class="n">allbufs</span><span class="p">[</span><span class="n">buf</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">forced_realize</span><span class="p">:</span> <span class="n">realizes</span><span class="p">[</span><span class="n">buf</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">LoadOps</span><span class="p">:</span> <span class="n">realizes</span><span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">base</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">contiguous</span> <span class="ow">and</span> <span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="s2">&quot;can only copy contig&quot;</span>
    <span class="n">realizes</span><span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">base</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">VIEW</span><span class="p">:</span> <span class="n">realizes</span><span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">base</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">children</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="p">][</span><span class="n">buf</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_recurse_lb</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">allbufs</span><span class="p">,</span> <span class="n">simple_pads</span><span class="p">,</span> <span class="n">children</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_is_padding_okay</span><span class="p">(</span><span class="n">buf</span><span class="p">:</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">realizes</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">realizes</span> <span class="ow">or</span> <span class="n">buf</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="kc">True</span>
  <span class="c1"># NOTE: this broke to_image_idx and coder with JIT</span>
  <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">UNSAFE_PAD_OPS</span><span class="p">:</span> <span class="k">return</span> <span class="kc">False</span>
  <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">_is_padding_okay</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">realizes</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">srcs</span><span class="p">)</span>
</code></pre></div>

<p><code>realizes</code> = lbs with <code>self.forced_realize</code> or that are <code>LoadOps</code> or source of <code>LoadOps.COPY</code> and base of view lbs if the lb was expanded compared to its base, unless exceptions.</p>
<div class="codehilite"><pre><span></span><code><span class="n">realizes</span> <span class="o">=</span> <span class="p">{</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># copy</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># source of copy</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># copy</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">NPY</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">True</span> <span class="n">device</span><span class="p">:</span><span class="n">NPY</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># src of copy</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">()</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># base of view lb</span>
<span class="p">}</span>
</code></pre></div>

<p><code>allbufs</code> = base lbs (no view lazybuffers).<br />
the NPY LoadOps.EMPTY lazybuffer isn't included because for it <code>self.realized</code> returns true which returns from <code>_recurse_lb</code> before it could be added to <code>allbufs</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">allbufs</span> <span class="o">=</span> <span class="p">{</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">()</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">}</span>
</code></pre></div>

<p><code>simple_pads</code> = lb base if there is a mask  = <code>{}</code></p>
<p><code>children</code> = unrealized lbs in <code>srcs</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">children</span> <span class="o">=</span> <span class="p">{</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">()</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">}</span>
</code></pre></div>

<p>back in <code>_graph_schedule</code>:</p>
<div class="codehilite"><pre><span></span><code>  <span class="n">assign_targets</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">realizes</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">ASSIGN</span> <span class="ow">and</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">}</span>

  <span class="c1"># check if we have to realize pads</span>
  <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">simple_pads</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_padding_okay</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">realizes</span><span class="p">):</span>
      <span class="n">realizes</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="c1"># find all reduces, and pair them to a elementwise op. if they can&#39;t be cleanly paired, force realize the reduce (or a contig child)</span>
  <span class="n">reduce_for_op</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">allbufs</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ReduceOps</span> <span class="ow">or</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">realizes</span><span class="p">:</span> <span class="k">continue</span>

    <span class="n">group</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">_recursive_group</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">st</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">children</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">reduce_for_op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="c1"># max one reduceop per kernel</span>
    <span class="n">can_chase</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">tr</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">reduce_for_op</span> <span class="k">for</span> <span class="n">tr</span> <span class="ow">in</span> <span class="n">group</span><span class="p">)</span>
    <span class="c1"># TODO: forced_realize exists because the scheduler is incapable of checking for self-contained DAGs</span>
    <span class="n">forced_realize</span> <span class="o">=</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">group</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">forced_realize</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># create a multi output kernel if the LazyBufferss can cleanly group</span>
      <span class="n">rc_parents</span><span class="p">,</span> <span class="n">rc_children</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">group</span><span class="p">),</span> <span class="n">deque</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
      <span class="k">while</span> <span class="n">rc_parents</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">forced_realize</span><span class="p">:</span>
        <span class="c1"># max one reduceop per kernel</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">:=</span><span class="n">rc_parents</span><span class="o">.</span><span class="n">pop</span><span class="p">())</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">ReduceOps</span><span class="p">:</span> <span class="n">forced_realize</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span> <span class="n">rc_parents</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">base</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">srcs</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">r</span><span class="p">)</span>
      <span class="c1"># search descendants of the reduceop that can cleanly group</span>
      <span class="n">realized_descendants</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
      <span class="k">while</span> <span class="n">rc_children</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">forced_realize</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">c</span><span class="o">:=</span><span class="n">rc_children</span><span class="o">.</span><span class="n">pop</span><span class="p">())</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">ReduceOps</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">c</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">contiguous</span> <span class="ow">or</span> <span class="n">c</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">r</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">size</span> <span class="ow">or</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">reduce_for_op</span><span class="p">:</span>
          <span class="n">realized_descendants</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
          <span class="k">break</span>
        <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">realizes</span> <span class="ow">and</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group</span><span class="p">:</span> <span class="n">realized_descendants</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">rc_children</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">children</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">r</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="n">group</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">realized_descendants</span><span class="p">)</span>
    <span class="c1"># can only fuse assign if no other assign_target is used in the kernel</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">forced_realize</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">ASSIGN</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">group</span><span class="p">):</span>
      <span class="n">parents</span> <span class="o">=</span> <span class="n">deque</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">group</span><span class="p">))</span>
      <span class="k">while</span> <span class="n">parents</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">forced_realize</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">:=</span><span class="n">parents</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span><span class="o">.</span><span class="n">base</span><span class="p">)</span><span class="o">.</span><span class="n">realized</span> <span class="ow">or</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">realizes</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">assign_targets</span> <span class="ow">and</span> <span class="n">assign_targets</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group</span><span class="p">:</span> <span class="n">forced_realize</span><span class="p">,</span> <span class="n">can_chase</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span>
          <span class="k">continue</span>
        <span class="n">parents</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">srcs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">forced_realize</span><span class="p">:</span>
      <span class="n">tr</span> <span class="o">=</span> <span class="n">r</span>
      <span class="k">if</span> <span class="n">can_chase</span><span class="p">:</span>
        <span class="c1"># can chase this down to contiguous children</span>
        <span class="n">st</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">st</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">children</span><span class="p">[</span><span class="n">tr</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="n">tr_next</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">children</span><span class="p">[</span><span class="n">tr</span><span class="p">]))</span>
          <span class="n">st_childs</span> <span class="o">=</span> <span class="n">dedup</span><span class="p">(</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tr_next</span><span class="o">.</span><span class="n">srcs</span> <span class="k">if</span> <span class="n">s</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="n">tr</span><span class="p">)</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">st_childs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span> <span class="k">break</span>
          <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">st_childs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">size</span><span class="p">:</span> <span class="k">break</span>
          <span class="n">st</span> <span class="o">=</span> <span class="n">st</span> <span class="o">+</span> <span class="n">st_childs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">st</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="n">st</span><span class="o">.</span><span class="n">contiguous</span> <span class="ow">or</span> <span class="n">tr_next</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">ReduceOps</span><span class="p">:</span> <span class="k">break</span>
          <span class="n">tr</span> <span class="o">=</span> <span class="n">tr_next</span>
        <span class="c1"># don&#39;t cast to higher size before store (tr cannot be realized if forced_realize)</span>
        <span class="k">if</span> <span class="n">tr</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">UnaryOps</span><span class="o">.</span><span class="n">CAST</span> <span class="ow">and</span> <span class="n">tr</span><span class="o">.</span><span class="n">arg</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">&gt;</span> <span class="n">tr</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">itemsize</span><span class="p">:</span>
          <span class="n">tr</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">base</span>
        <span class="n">reduce_for_op</span><span class="p">[</span><span class="n">tr</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
      <span class="n">realizes</span><span class="p">[</span><span class="n">tr</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span> <span class="n">reduce_for_op</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">tr</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">tr</span> <span class="ow">in</span> <span class="n">group</span><span class="p">)</span>

  <span class="n">output_groups</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">realizes</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">realized</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">buf</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">CONST</span> <span class="ow">or</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span> <span class="k">continue</span>
    <span class="n">output_groups</span><span class="p">[</span><span class="n">reduce_for_op</span><span class="p">[</span><span class="n">buf</span><span class="p">]</span> <span class="k">if</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">reduce_for_op</span> <span class="ow">and</span> <span class="n">MULTIOUTPUT</span> <span class="k">else</span> <span class="n">buf</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>

    <span class="c1"># make things that can&#39;t be images not images</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">ImageDType</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prod</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="n">prod</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">or</span>
                                              <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">%</span><span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">buf</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">unit_stride_axes</span><span class="p">())):</span>
      <span class="k">if</span> <span class="n">DEBUG</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;forcing image </span><span class="si">{</span><span class="n">buf</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> with shape </span><span class="si">{</span><span class="n">buf</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> to float32&quot;</span><span class="p">)</span>
      <span class="n">buf</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>
      <span class="c1"># hack the underlying buffer too</span>
      <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="n">buf</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="s1">&#39;_buf&#39;</span><span class="p">),</span> <span class="s2">&quot;can&#39;t fixup allocated buffer&quot;</span>
        <span class="n">buf</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>
        <span class="n">buf</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="kc">None</span>
 <span class="c1"># preschedule all buffers in realizes</span>
  <span class="n">prescheduled</span> <span class="o">=</span> <span class="p">{</span><span class="n">group</span><span class="p">[</span><span class="mi">0</span><span class="p">]:(</span><span class="n">group</span><span class="p">,</span> <span class="o">*</span><span class="n">_lower_lazybuffer</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">reduce_for_op</span><span class="p">))</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">output_groups</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span>
</code></pre></div>

<p>(current value): </p>
<div class="codehilite"><pre><span></span><code><span class="n">output_groups</span> <span class="o">=</span> <span class="p">{</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">[</span><span class="n">same</span> <span class="n">buffer</span><span class="p">]</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">[</span><span class="n">same</span> <span class="n">buffer</span><span class="p">]</span>
  <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">[</span><span class="n">same</span> <span class="n">buffer</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_lower_lazybuffer</span><span class="p">(</span><span class="n">outs</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">],</span> <span class="n">realizes</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">reduce_for_op</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">LazyBuffer</span><span class="p">]):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;describe the computation for a LazyBuffer with LazyOp + inputs + var_vals&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">out</span><span class="o">:=</span><span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span> <span class="ow">and</span> <span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;USE_COPY_KERNEL&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">rd</span> <span class="o">=</span> <span class="n">LazyOp</span><span class="p">(</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">LOAD</span><span class="p">,</span> <span class="p">(),</span> <span class="n">MemBuffer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">st</span><span class="o">:=</span><span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">((</span><span class="n">out</span><span class="o">.</span><span class="n">arg</span><span class="p">,))))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">STORE</span><span class="p">,</span> <span class="p">(</span><span class="n">rd</span><span class="p">,),</span> <span class="n">MemBuffer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">st</span><span class="p">)),</span> <span class="p">),</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">base</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">srcs</span><span class="p">],</span> <span class="p">{},</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="p">{</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">CUSTOM</span><span class="p">,</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">,</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">VIEW</span><span class="p">}:</span> <span class="k">return</span> <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="p">(),</span> <span class="n">out</span><span class="o">.</span><span class="n">arg</span><span class="p">),</span> <span class="p">),</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">base</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">srcs</span><span class="p">],</span> <span class="p">{},</span> <span class="p">[]</span>
  <span class="n">var_vals</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">merge_dicts</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">st</span><span class="o">.</span><span class="n">var_vals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">])</span>
  <span class="n">assign_targets</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">srcs</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outs</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">ASSIGN</span><span class="p">}</span>
  <span class="n">cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">ShapeTracker</span><span class="p">],</span> <span class="n">LazyOp</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">ast</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LazyOp</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outs</span><span class="p">):</span>
    <span class="n">output_st</span> <span class="o">=</span> <span class="n">ShapeTracker</span><span class="o">.</span><span class="n">from_shape</span><span class="p">(</span><span class="n">reduce_for_op</span><span class="p">[</span><span class="n">out</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">reduce_for_op</span> <span class="k">else</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">output_view</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">arg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">op</span> <span class="ow">is</span> <span class="n">LoadOps</span><span class="o">.</span><span class="n">ASSIGN</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">arg</span> <span class="k">else</span> <span class="n">output_st</span>
    <span class="n">lop</span> <span class="o">=</span> <span class="n">_recursive_lazyop</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outs</span><span class="p">),</span> <span class="n">var_vals</span><span class="p">,</span> <span class="n">output_st</span><span class="p">,</span> <span class="n">realizes</span><span class="p">,</span> <span class="n">assign_targets</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>
    <span class="n">output_view</span><span class="p">,</span> <span class="n">vv</span> <span class="o">=</span> <span class="n">output_view</span><span class="o">.</span><span class="n">simplify</span><span class="p">()</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">vv</span><span class="p">:</span> <span class="n">var_vals</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">vv</span><span class="p">)</span>
    <span class="n">ast</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">STORE</span><span class="p">,</span> <span class="p">(</span><span class="n">lop</span><span class="p">,</span> <span class="p">),</span> <span class="n">MemBuffer</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">output_view</span><span class="p">)))</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ast</span><span class="p">),</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">var_vals</span><span class="p">,</span> <span class="n">dedup</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cache</span> <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LazyOp</span><span class="p">:</span>
  <span class="n">op</span><span class="p">:</span> <span class="n">Op</span>
  <span class="n">src</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">LazyOp</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">()</span>
  <span class="n">arg</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MemBuffer</span><span class="p">:</span>
  <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">dtype</span><span class="p">:</span> <span class="n">DType</span>
  <span class="n">st</span><span class="p">:</span> <span class="n">ShapeTracker</span>

<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ConstBuffer</span><span class="p">:</span>
  <span class="n">val</span><span class="p">:</span> <span class="n">ConstType</span> <span class="o">|</span> <span class="n">Variable</span>
  <span class="n">dtype</span><span class="p">:</span> <span class="n">DType</span>
  <span class="n">st</span><span class="p">:</span> <span class="n">ShapeTracker</span>
</code></pre></div>

<p><code>_lower_lazybuffer</code> returns:</p>
<ul>
<li><code>(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], {}, []</code> for <code>CLANG</code> copy </li>
<li>enters <code>_recursive_lazyop</code> when processing <code>output_groups[1]</code></li>
<li><code>(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], {}, []</code> again for the <code>CUDA</code> copy </li>
</ul>
<p><code>_recursive_lazyop</code> returns <code>LazyOp</code> for the two copy lbs and the add lb in <code>output_groups</code>.<br />
in the add lb it recurses trough its sources:</p>
<ul>
<li><code>&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3&gt;, None)&gt;</code><ul>
<li>simplify and unbind shapetracker (simplify does nothing here because the shapetracker has only one view. Unbind seems to act on variables, of which there aren't any here).</li>
<li>append the lb to <code>inputs</code></li>
<li>return `LazyOp(BufferOps.LOAD, (), MemBuffer(len(outputs)+inputs.index(buf), buf.dtype, unbound_st)))</li>
</ul>
</li>
<li><code>&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))&gt;)</code><ul>
<li>switch to its base `<LB CUDA () int (\<LoadOps.CONST: 2>, None)></li>
<li>return <code>LazyOp(BufferOps.CONST, (), ConstBuffer(val, buf.dtype, unbound_st))</code> where <code>val</code> is <code>arg</code>, which is 2.</li>
</ul>
</li>
</ul>
<p><code>ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))'</code>return tuple(ast), inputs, var_vals,<code>+ metadata stuff, ignored for now.</code>var_vals<code>is</code>{}` because nothing symbolic in this case.</p>
<div class="codehilite"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">prescheduled:List[Tuple[]] {</span>
<span class="sd">    group[0]:LazyBuffer (</span>
<span class="sd">        group: List[LazyBuffer],</span>
<span class="sd">        abstract syntax tree (ast): Tuple[LazyOp],</span>
<span class="sd">        inputs: List[LazyBuffer]</span>
<span class="sd">        variable values: Dict[Variable, int],</span>
<span class="sd">        metadata: List[?]</span>
<span class="sd">    )</span>
<span class="sd">}</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">prescheduled</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">(</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="p">(),</span> <span class="n">arg</span><span class="o">=</span><span class="mi">12</span><span class="p">),),</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">{},</span>
        <span class="p">[]</span>
    <span class="p">),</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">(</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span>
            <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">STORE</span>
            <span class="n">src</span><span class="o">=</span><span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span>
                <span class="n">op</span><span class="o">=</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">,</span>
                <span class="n">src</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">LazyOp</span><span class="p">(</span>
                        <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">LOAD</span><span class="p">,</span>
                        <span class="n">src</span><span class="o">=</span><span class="p">(),</span>
                        <span class="n">arg</span><span class="o">=</span><span class="n">MemBuffer</span><span class="p">(</span>
                            <span class="n">idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                            <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                                <span class="n">View</span><span class="p">(</span>
                                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                                    <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span>
                                <span class="p">),</span>
                            <span class="p">))</span>
                        <span class="p">)</span>
                    <span class="p">),</span>
                    <span class="n">LazyOp</span><span class="p">(</span>
                        <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">CONST</span>
                        <span class="n">src</span><span class="o">=</span><span class="p">()</span>
                        <span class="n">arg</span><span class="o">=</span><span class="n">ConstBuffer</span><span class="p">(</span>
                            <span class="n">val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                            <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                                <span class="n">View</span><span class="p">(</span>
                                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span>
                                    <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">contiguous</span><span class="o">=</span><span class="kc">False</span>
                                <span class="p">),</span>
                            <span class="p">))</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="n">arg</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">),),</span>
            <span class="n">arg</span><span class="o">=</span><span class="n">MemBuffer</span><span class="p">(</span>
                <span class="n">idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">View</span><span class="p">(</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                        <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                        <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">),</span>
                <span class="p">))</span>
            <span class="p">)</span>
        <span class="p">),)</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">{},</span>
        <span class="p">[</span><span class="fm">__add__</span> <span class="o">-</span> <span class="n">__main__</span><span class="p">:</span><span class="mi">3</span><span class="p">::</span><span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">(</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="p">(),</span> <span class="n">arg</span><span class="o">=</span><span class="mi">12</span><span class="p">),),</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">NPY</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">True</span> <span class="n">device</span><span class="p">:</span><span class="n">NPY</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">{},</span>
        <span class="p">[]</span>
    <span class="p">)</span>
<span class="p">}</span>
</code></pre></div>

<p>back in <code>_graph_schedule</code></p>
<div class="codehilite"><pre><span></span><code>  <span class="n">schedule_targets</span> <span class="o">=</span> <span class="p">{</span><span class="n">out</span><span class="p">:</span><span class="n">ps</span> <span class="k">for</span> <span class="n">ps</span> <span class="ow">in</span> <span class="n">prescheduled</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="p">}</span>

  <span class="n">graph</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="n">in_degree</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">LazyBuffer</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">lsi</span> <span class="ow">in</span> <span class="n">prescheduled</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">in_degree</span><span class="p">:</span> <span class="n">in_degree</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># realize outputs after all parents are realized</span>
    <span class="n">scheduled_parents</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">schedule_targets</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lsi</span><span class="o">.</span><span class="n">inputs</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">schedule_targets</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">scheduled_parents</span><span class="p">:</span>
      <span class="n">graph</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="n">in_degree</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># realize outputs before a parent is assigned to</span>
    <span class="n">parents_assigns</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">schedule_targets</span><span class="p">[</span><span class="n">assign_targets</span><span class="p">[</span><span class="n">x</span><span class="p">]]</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lsi</span><span class="o">.</span><span class="n">inputs</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">assign_targets</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">assign</span> <span class="ow">in</span> <span class="n">parents_assigns</span><span class="p">:</span>
      <span class="n">graph</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">assign</span><span class="p">)</span>
      <span class="n">in_degree</span><span class="p">[</span><span class="n">assign</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="n">in_degree</span><span class="p">,</span> <span class="n">prescheduled</span>
</code></pre></div>

<p><code>lsi</code> = LazyScheduleItem?<br />
<code>schedule_targets</code> makes an entry for every item in every group and assigns it the tuple in <code>prescheduled</code> that it is part of.</p>
<p><code>scheduled_parents = set(schedule_targets[x][0][0] for x in lsi[2] if x in schedule_targets)</code><br />
<code>lsi[2]</code> is inputs, so if an input is one of the entries in a lazybuffer group, add the tuple with its info.<br />
this returns an empty set for the third group, because its input (the <code>NPY</code> lazybuffer) is not in any group (= not in <code>output_groups</code> because it is already realized)</p>
<p>the input group's <code>group[0]</code> as a key in <code>graph</code> and append the current prescheduled <code>key</code><br />
some detailed explanation: The <code>ADD</code> lb is the first key in <code>graph</code> because it is the input of the first group (where the key is the <code>COPY</code> lb) in <code>prescheduled</code> that is also part of group itself. The value it gets assigned is the first item in the group that it was an input of, so, the <code>COPY</code> lb.<br />
this way, graph "points" from the inputs to the groups that depend on them.<br />
every time an input of a group is added to graph this way, the groups key in the <code>in_degree</code> dictionary increases by 1.</p>
<div class="codehilite"><pre><span></span><code><span class="n">graph</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">in_degree</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CLANG</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">}</span>
</code></pre></div>

<p>back in <code>create_schedule_with_vars</code></p>
<div class="codehilite"><pre><span></span><code>  <span class="n">queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">si</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">si</span> <span class="ow">in</span> <span class="n">prescheduled</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">in_degree</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">schedule</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ScheduleItem</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">var_vals</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Variable</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">kernel_number</span> <span class="o">=</span> <span class="n">GlobalCounters</span><span class="o">.</span><span class="n">kernel_count</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span> <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">GRAPH</span><span class="p">:</span>
      <span class="n">kernel_number</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span> <span class="n">realized_lazybuffer</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">kernel_number</span><span class="p">)</span>
    <span class="n">var_vals</span> <span class="o">=</span> <span class="n">merge_dicts</span><span class="p">([</span><span class="n">var_vals</span><span class="p">,</span> <span class="n">ps</span><span class="o">.</span><span class="n">var_vals</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span> <span class="k">del</span> <span class="n">out</span><span class="o">.</span><span class="n">srcs</span>  <span class="c1"># can only schedule once</span>
    <span class="n">schedule</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">si</span><span class="o">:=</span><span class="n">ScheduleItem</span><span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">ast</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="o">+</span><span class="n">ps</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">logops</span> <span class="ow">and</span> <span class="n">si</span><span class="o">.</span><span class="n">ast</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">LoadOps</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;DISK:&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">si</span><span class="o">.</span><span class="n">inputs</span><span class="p">):</span> <span class="n">logops</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">si</span><span class="o">.</span><span class="n">ast</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">[</span><span class="n">ps</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]]:</span>
      <span class="n">in_degree</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">in_degree</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prescheduled</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

  <span class="k">if</span> <span class="n">SAVE_SCHEDULE</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_save</span><span class="p">():</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;saving </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">SCHEDULES</span><span class="p">)</span><span class="si">}</span><span class="s2"> schedule graphs to&quot;</span><span class="p">,</span> <span class="n">fp</span><span class="o">:=</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SAVE_SCHEDULE_PATH&quot;</span><span class="p">,</span> <span class="s2">&quot;schedule.pkl&quot;</span><span class="p">))</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">SCHEDULES</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">SCHEDULES</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">_save</span><span class="p">)</span>
    <span class="n">SCHEDULES</span><span class="o">.</span><span class="n">extend</span><span class="p">((</span><span class="n">ps</span><span class="o">.</span><span class="n">ast</span> <span class="k">for</span> <span class="n">ps</span> <span class="ow">in</span> <span class="n">prescheduled</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;CAPTURE_AST&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="p">[(</span><span class="n">graph</span><span class="p">,</span> <span class="n">prescheduled</span><span class="p">)])</span>
  <span class="c1"># confirm everything was scheduled correctly</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">degree</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">in_degree</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">prescheduled</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedule</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cycle detected in graph, prescheduled </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prescheduled</span><span class="p">)</span><span class="si">}</span><span class="s2"> but only scheduled </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">schedule</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">DEBUG</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedule</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;scheduled </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">schedule</span><span class="p">)</span><span class="si">}</span><span class="s2"> kernels&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">schedule</span><span class="p">,</span> <span class="n">var_vals</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([</span>
   <span class="p">(</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">CUDA</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">:</span> <span class="mi">3</span><span class="o">&gt;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">(</span><span class="n">LazyOp</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="p">(),</span> <span class="n">arg</span><span class="o">=</span><span class="mi">12</span><span class="p">),),</span>
        <span class="p">[</span><span class="o">&lt;</span><span class="n">LB</span> <span class="n">NPY</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)</span> <span class="nb">int</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">True</span> <span class="n">device</span><span class="p">:</span><span class="n">NPY</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">)</span><span class="o">&gt;</span><span class="p">],</span>
        <span class="p">{},</span>
        <span class="p">[]</span>
    <span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<p>adds any buffers of the group to <code>seen</code>.<br />
deletes <code>srcs</code> of lazybuffers in the group</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ScheduleItem</span><span class="p">:</span>
  <span class="n">ast</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">LazyOp</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
  <span class="n">bufs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Buffer</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
  <span class="n">metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Metadata</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">schedule</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">si</span><span class="o">:=</span><span class="n">ScheduleItem</span><span class="p">(</span>
    <span class="n">ps</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">ps</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">ps</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="p">))</span>
</code></pre></div>

<p>then finds the lb it just made a <code>ScheduleItem</code> from in <code>graph</code>, which returns the <code>group[0]</code> item of the groups that depend on the just processed one.<br />
Add the group tuple from <code>prescheduled</code> to <code>queue</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">schedule</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">ScheduleItem</span><span class="p">(</span>
        <span class="n">ast</span><span class="o">=</span><span class="p">(</span>
            <span class="n">LazyOp</span><span class="p">(</span>
                <span class="n">op</span><span class="o">=</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span>
                <span class="n">src</span><span class="o">=</span><span class="p">(),</span>
                <span class="n">arg</span><span class="o">=</span><span class="mi">12</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="n">bufs</span><span class="o">=</span><span class="p">(</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">True</span> <span class="n">device</span><span class="p">:</span><span class="n">NPY</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span>
        <span class="p">),</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">[]</span>
    <span class="p">),</span>
    <span class="n">ScheduleItem</span><span class="p">(</span>
        <span class="n">ast</span><span class="o">=</span><span class="p">(</span>
            <span class="n">LazyOp</span><span class="p">(</span>
                <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">STORE</span><span class="p">,</span>
                <span class="n">src</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">LazyOp</span><span class="p">(</span>
                        <span class="n">op</span><span class="o">=</span><span class="n">BinaryOps</span><span class="o">.</span><span class="n">ADD</span><span class="p">,</span>
                        <span class="n">src</span><span class="o">=</span><span class="p">(</span>
                            <span class="n">LazyOp</span><span class="p">(</span>
                                <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">LOAD</span><span class="p">,</span>
                                <span class="n">src</span><span class="o">=</span><span class="p">(),</span>
                                <span class="n">arg</span><span class="o">=</span><span class="n">MemBuffer</span><span class="p">(</span>
                                    <span class="n">idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                                    <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                                        <span class="n">View</span><span class="p">(</span>
                                            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                                            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                                            <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                            <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                            <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span>
                                        <span class="p">),</span>
                                    <span class="p">))</span>
                                <span class="p">)</span>
                            <span class="p">),</span>
                            <span class="n">LazyOp</span><span class="p">(</span>
                                <span class="n">op</span><span class="o">=</span><span class="n">BufferOps</span><span class="o">.</span><span class="n">CONST</span><span class="p">,</span>
                                <span class="n">src</span><span class="o">=</span><span class="p">(),</span>
                                <span class="n">arg</span><span class="o">=</span><span class="n">ConstBuffer</span><span class="p">(</span>
                                    <span class="n">val</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                                    <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                                        <span class="n">View</span><span class="p">(</span>
                                            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                                            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span>
                                            <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                            <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                            <span class="n">contiguous</span><span class="o">=</span><span class="kc">False</span>
                                        <span class="p">),</span>
                                    <span class="p">))</span>
                                <span class="p">)</span>
                            <span class="p">)</span>
                        <span class="p">),</span>
                        <span class="n">arg</span><span class="o">=</span><span class="kc">None</span>
                    <span class="p">),</span>
                <span class="p">),</span>
                <span class="n">arg</span><span class="o">=</span><span class="n">MemBuffer</span><span class="p">(</span>
                    <span class="n">idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
                    <span class="n">st</span><span class="o">=</span><span class="n">ShapeTracker</span><span class="p">(</span><span class="n">views</span><span class="o">=</span><span class="p">(</span>
                        <span class="n">View</span><span class="p">(</span>
                            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
                            <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
                            <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">contiguous</span><span class="o">=</span><span class="kc">True</span>
                        <span class="p">),</span>
                    <span class="p">))</span>
                <span class="p">)</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="n">bufs</span><span class="o">=</span><span class="p">(</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span>
        <span class="p">),</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">[</span><span class="fm">__add__</span> <span class="o">-</span> <span class="n">__main__</span><span class="p">:</span><span class="mi">3</span><span class="p">::</span><span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">ScheduleItem</span><span class="p">(</span>
        <span class="n">ast</span><span class="o">=</span><span class="p">(</span>
            <span class="n">LazyOp</span><span class="p">(</span>
                <span class="n">op</span><span class="o">=</span><span class="n">LoadOps</span><span class="o">.</span><span class="n">COPY</span><span class="p">,</span>
                <span class="n">src</span><span class="o">=</span><span class="p">(),</span>
                <span class="n">arg</span><span class="o">=</span><span class="mi">12</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="n">bufs</span><span class="o">=</span><span class="p">(</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CLANG</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="o">&lt;</span><span class="n">buf</span> <span class="n">real</span><span class="p">:</span><span class="kc">False</span> <span class="n">device</span><span class="p">:</span><span class="n">CUDA</span> <span class="n">size</span><span class="p">:</span><span class="mi">3</span> <span class="n">dtype</span><span class="p">:</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int</span> <span class="n">offset</span><span class="p">:</span><span class="mi">0</span><span class="o">&gt;</span>
        <span class="p">),</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">[]</span>
    <span class="p">)</span>
<span class="p">]</span>
</code></pre></div>

<p>with <code>GRAPH=1</code>, tinygrad produces output that reflects this schedule:<br />
<img alt="200" src="attachments/net_1.svg" /></p>
<p>back in <code>schedule_with_vars</code></p>
<div class="codehilite"><pre><span></span><code><span class="k">return</span> <span class="n">memory_planner</span><span class="p">(</span><span class="n">schedule</span><span class="p">),</span> <span class="n">var_vals</span>
</code></pre></div>

<p>then back in <code>Tensor.realize</code></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">realize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">lst</span><span class="p">:</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">do_update_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">run_schedule</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">schedule_with_vars</span><span class="p">(</span><span class="o">*</span><span class="n">lst</span><span class="p">),</span> <span class="n">do_update_stats</span><span class="o">=</span><span class="n">do_update_stats</span><span class="p">)</span>
</code></pre></div>

<hr />
<p>watch out for garbo below</p>
<h3 id="creating%20tensors%20through%20methods">creating tensors through methods</h3>
<ul>
<li>Tensor.empty - no new ops</li>
<li>Tensor.zeros - <code>full(shape, 0, ...)</code></li>
<li>Tensor.ones - <code>full(shape, 1, ...)</code></li>
<li><code>full(shape, fill_value)</code>:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">Tensor</span><span class="p">(</span><span class="n">fill_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">new_shape</span> <span class="o">:=</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">)))</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
</code></pre></div>

<ul>
<li>Tensor.arange - <code>full(shape, step, dtype, **kwargs)._cumsum() + (start - step)</code> -&gt; <code>.cast(dtype)</code></li>
<li>Tensor.eye - <code>ones().pad().flatten().shrink().reshape()</code></li>
<li>Tensor.full_like - <code>full</code></li>
<li>Tensor.zeros_like <code>full_like</code></li>
<li>Tensor.ones_like <code>full_like</code></li>
</ul>
<p>all Tensor constructors that aren't random build on the <code>Tensor.full(shape, fill_value)</code> function, which first <em>reshapes</em> the Tensor with 1 element (fill_value) to the target number of dimensions.<br />
<code>Tensor.reshape</code> calls <code>F.Reshape.apply(self, new_shape)</code> from <code>function.py</code>, which inherits from <code>class Function</code> in <code>tensor.py</code>.</p>
<p>all <code>Function</code> "children", in their <code>apply</code>function, create a new Tensor and populate it with new <code>lazydata</code>, <code>requires_grad</code>, <code>grad=None</code> and <code>_ctx</code> if <code>requires_grad</code> is True. <code>_ctx</code> contains the function that was called, which also contains the parent Tensors.</p>
<p>the <code>forward</code> method for <code>F.Reshape()</code> is called on the <code>lazydata</code>.<br />
<code>lazydata.reshape</code> turns into <code>self._view(st.reshape())</code> (st = ShapeTracker) in <code>lazy.py</code>.<br />
<code>ShapeTracker.reshape()</code> returns a new <code>ShapeTracker</code> with (by default) its latest <code>views</code> replaced by a new one with the new shape. if <code>MERGE_VIEWS=0</code>, the new view is appended to <code>views</code> instead.<br />
In the current case, the previous View with shape <code>(1,)</code> is directly replaced by the new one <code>(1,)*len(new_shape)</code>.<br />
finally, the tensor gets a new <code>LazyBuffer</code> from  <code>create_lazybuffer(self.device, new_st, self.dtype, base=self.base)</code></p>
<p>after the reshape, the dimension use <code>Tensor.expand(new_shape)</code> to get the now correct number of dimensions to the final shape.</p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">_broadcast_to</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">from_</span> <span class="k">if</span> <span class="n">to</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">to</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">to</span> <span class="k">for</span> <span class="n">from_</span><span class="p">,</span> <span class="n">to</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">_pad_left</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">argfix</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">))))))</span>
</code></pre></div>

<p><code>argfix</code> ensures the function works even if the shape was not input as a tuple but through multiple arguments like <code>reshape(2,2,2)</code>.<br />
<code>_pad_left</code> gets inputs to the same number of dimensions.<br />
<code>*</code> unpacks the tuple with both shapes that <code>_pad_left</code> returns</p>
<p><code>Tensor._broadcast_to(self, shape)</code> runs <code>_pad_left</code> again<br />
runs <code>self.reshape</code> again to the "padded" shape<br />
then <code>F.Expand.apply()</code> -&gt; <code>lazybuffer.expand()</code> -&gt; <code>shapetracker.expand()</code> -&gt; <code>View.expand()</code> which producees  a new <code>View</code> with the new shape and everything else being equal. returns a new <code>ShapeTracker</code>, returns a new <code>LazyBuffer</code>, returns a new <code>Tensor</code></p>
<p>Tensor.arange offers new stuff, calling <code>Tensor._cumsum()</code>, using Tensor-Int addition and casting the Tensor.<br />
from <code>Tensor._cumsum()</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pad2d</span><span class="p">((</span><span class="n">pl_sz</span><span class="p">,</span><span class="o">-</span><span class="nb">int</span><span class="p">(</span><span class="n">_first_zero</span><span class="p">)))</span><span class="o">.</span><span class="n">_pool</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">],))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p>where <code>axis</code> is 0 and <code>pl_sz</code> will in this case be <code>self.shape[0] - 1</code></p>
<p><code>Tensor.transpose(0, -1)</code>, which translates to <code>Tensor.permute(order)</code> where in the order dim 0 and the last dim were swapped. <code>permute</code> resolves orders with negative dim indices, error checks and runs <code>F.Permute.apply(self, order=resolve_order)</code> -&gt; <code>lazybuffer.permute(order)</code> -&gt; <code>ShapeTracker.permute(order)</code> -&gt; <code>View.permute(axis=order)</code> -&gt; <code>View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)</code><br />
returns a new <code>View</code>in a new <code>ShapeTracker</code> in a new <code>lazybuffer</code> in a new <code>Tensor</code><br />
this transpose changes nothing because the input was a 1D Tensor.</p>
<p><code>Tensor.pad2d(self.shape[0] - 1, 0)</code> adds <code>self.shape[0] - 1</code> 0s to the left on the lowest dimension. Using <code>pad2d()</code> seems crazy here, it goes through <code>Tensor._slice()</code>, which eventually calls <code>Tensor.pad((self.shape[0] - 1, 0))</code> which is even crazier, which calls <code>F.Pad.apply(...)</code> which goes on the tour again.<br />
<code>LazyBuffer.pad()</code> -&gt; <code>ShapeTracker.pad()</code> -&gt; <code>View.pad()</code><br />
where <code>(self.shape[0] - 1, 0)</code> turns into  <code>(-self.shape[0] - 1, self.shape)</code>, which was already calculated in <code>Tensor.pad2d</code> for some reason.<br />
A mask is created: <code>((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))</code><br />
calling a trustworthy <code>View.__unsafe_resize(evernew_arg, new_mask)</code> where a new <code>View</code> is created with the extended <code>shape</code> (<code>self.shape[0] + self.shape[0] - 1</code>), <code>offset</code> of <code>-self.shape[0] - 1</code> and the <code>mask</code> as it was created. <code>contiguous</code> turns <code>False</code> whatever that means.</p>
<p>To see how mask, offset and maybe contiguous are interpreted, a detour to <code>Tensor.__getitem__()</code> follows. Or not, because <code>__getitem__</code> only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from <code>Tensor.tolist()</code><br />
To return to later: rest of <code>Tensor.arange</code>, other Tensor construction methods and random construction methods:</p>
<ul>
<li>Tensor.manual_seed</li>
<li>Tensor.rand</li>
<li>Tensor.randn</li>
<li>Tensor.randint</li>
<li>Tensor.normal</li>
<li>Tensor.uniform</li>
<li>Tensor.scaled_uniform</li>
<li>Tensor.glorot_uniform</li>
<li>Tensor.kaiming_uniform</li>
<li>Tensor.kaiming_normal</li>
</ul>
<p></p>
<h3 id="Detected%20room%20for%20improvement%20/%20questions">Detected room for improvement / questions</h3>
<p>Some environment variables are stored in <code>ContexVar._cache</code> and as <code>ContextVar</code> instances and can be imported from <code>tinygrad.helpers</code> but others are determined dynamically through <code>getenv</code> which is also imported from <code>tinygrad.helpers</code> and used like <code>getenv("LAZYCACHE", 1)</code>. Not obvious why this added complexity.</p>
<p><code>tensor.py</code> too big, methods more around imitating style than being nicely categorized? Remove stuff like <code>Tensor.ones</code> or duplication of <code>Tensor.transpose</code> and <code>Tensor.T</code></p>
<p><code>Tensor(2).lazydata.contiguous_child</code> is <code>None</code> but<br />
<code>Tensor(1).lazydata.contiguous_child</code> is a tuple of weakref to some lazybuffer and its own ShapeTracker ??</p>
<p>beautiful lazy graph and linearized graph in DEBUG=4</p>
<p>trying the AMD device takes a lot of lines, importing from <code>renderer/cstyle.py</code>. can be solved by switching lines in import</p>
<p>can create a Tensor on a device that does not actually work and will only cause an error when realized (not when realized even, but tolist does not work. where do they fail, how much work is wasted on it?</p>
<p>if <code>CUDA</code>, <code>ptx_matcher:PatternMatcher</code> might replace the other pattern matcher that was laboriously created when importing tensor?</p>
<p>how good is tinygrad introspection? feel need for an inliner to be rooted in base reality.</p>
<p></p>
</article></main><script>MathJax = { tex: {inlineMath: [['$', '$']],displayMath: [['$$', '$$']]}};</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script></body></html>