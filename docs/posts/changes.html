<!DOCTYPE html><html lang=en><head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Changes</title>
<link rel="stylesheet" href="../main.css">
<link rel="shortcut icon" href="../favicon.ico"></head><body><main><nav><a href='../index.html'>Entrance</a></nav><article>
<h1>Changes</h1>
<p>Changes to all notes sorted like: date > note > heading > changed lines (gray lines represent deletions, orange lines replacements or new additions).</p>
<p>
2024 08 12
<a href='#t2024-08-12-20:12'>20:12</a>
|
2024 08 11
<a href='#t2024-08-11-11:15'>11:15</a>
|
2024 07 16
<a href='#t2024-07-16-15:35'>15:35</a>
|
2024 07 14
<a href='#t2024-07-14-11:53'>11:53</a>
|
2024 07 10
<a href='#t2024-07-10-10:47'>10:47</a>
|
2024 07 09
<a href='#t2024-07-09-08:25'>08:25</a>
|
2024 07 08
<a href='#t2024-07-08-18:39'>18:39</a>
|
2024 07 07
<a href='#t2024-07-07-20:07'>20:07</a>
<a href='#t2024-07-07-13:58'>13:58</a>
<a href='#t2024-07-07-13:47'>13:47</a>
<a href='#t2024-07-07-12:25'>12:25</a>
<a href='#t2024-07-07-12:22'>12:22</a>
|
2024 06 30
<a href='#t2024-06-30-18:05'>18:05</a>
|
2024 06 29
<a href='#t2024-06-29-19:28'>19:28</a>
|
2024 06 28
<a href='#t2024-06-28-11:45'>11:45</a>
<a href='#t2024-06-28-10:58'>10:58</a>
<a href='#t2024-06-28-09:34'>09:34</a>
|
2024 06 06
<a href='#t2024-06-06-13:44'>13:44</a>
|
2024 05 28
<a href='#t2024-05-28-13:40'>13:40</a>
|
2024 05 18
<a href='#t2024-05-18-22:30'>22:30</a>
|
2024 05 15
<a href='#t2024-05-15-08:45'>08:45</a>
|
2024 05 11
<a href='#t2024-05-11-12:07'>12:07</a>
|
2024 05 10
<a href='#t2024-05-10-13:18'>13:18</a>
|
2024 05 05
<a href='#t2024-05-05-18:09'>18:09</a>
|
2024 05 04
<a href='#t2024-05-04-13:58'>13:58</a>
|
2024 04 28
<a href='#t2024-04-28-12:01'>12:01</a>
|
2024 04 26
<a href='#t2024-04-26-11:19'>11:19</a>
|
2024 04 25
<a href='#t2024-04-25-13:28'>13:28</a>
|
2024 04 21
<a href='#t2024-04-21-18:46'>18:46</a>
|
2024 04 19
<a href='#t2024-04-19-14:39'>14:39</a>
|
2024 04 18
<a href='#t2024-04-18-15:16'>15:16</a>
|
2024 04 14
<a href='#t2024-04-14-14:32'>14:32</a>
|
2024 04 07
<a href='#t2024-04-07-16:26'>16:26</a>
|
2024 03 29
<a href='#t2024-03-29-14:04'>14:04</a>
|
2024 03 22
<a href='#t2024-03-22-08:21'>08:21</a>
|
2024 03 07
<a href='#t2024-03-07-20:49'>20:49</a>
<a href='#t2024-03-07-20:47'>20:47</a>
<a href='#t2024-03-07-20:46'>20:46</a>
<a href='#t2024-03-07-20:36'>20:36</a>
<a href='#t2024-03-07-10:47'>10:47</a>
|
2024 02 28
<a href='#t2024-02-28-21:21'>21:21</a>
<a href='#t2024-02-28-20:54'>20:54</a>
|
2024 02 20
<a href='#t2024-02-20-21:51'>21:51</a>
<a href='#t2024-02-20-16:26'>16:26</a>
|
2024 02 19
<a href='#t2024-02-19-22:21'>22:21</a>
<a href='#t2024-02-19-21:49'>21:49</a>
<a href='#t2024-02-19-21:48'>21:48</a>
<a href='#t2024-02-19-21:36'>21:36</a>
<a href='#t2024-02-19-20:27'>20:27</a>
<a href='#t2024-02-19-12:21'>12:21</a>
</p>
<span class='date' id='t2024-08-12-20:12'>2024 08 12 20:12</span><div class='indent'>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>read</span>
<span class='add'>read using [pyIntroducer](https://github.com/lorinbaum/pyintroducer)</span>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='add'>`tempfile` module for temporary files that automatically delete after close</span>
</div>
<span class='hdg'>Notes on introduction of tensor.tolist() on CLANG</span>
<div class='indent'>
<span class='hdg'>Realize</span>
<div class='indent'>
<span class='hdg'>lower schedule</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>second scheduleItem:</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>lazyops are recursively verfied `verify_lazyop` (opy.py)</span>
<span class='add'></span>
<span class='add'>`hand_coded_optimizations` if no tensor cores. But they don't do anything on the add kernel.</span>
<span class='add'>would do BEAM here, but isn't enabled for this one.</span>
<span class='add'></span>
<span class='add'>-> `Kernel.to_program`</span>
<span class='add'></span>
<span class='add'>`get_optimized_ast` recursively goes through each op.</span>
<span class='add'>gives `MetaOps.KERNEL` an `arg` with `KernelInfo` dataclass</span>
<span class='add'>gives `BufferOps` arg (MemBuffer or ConstBuffer in the current Kernel) "new" shapetrackerse, which in this case are the same as before.</span>
<span class='add'></span>
<span class='add'>then verfiyes new ast -> `veryify_lazyops`</span>
<span class='add'></span>
<span class='add'>generate the UOpGraph</span>
<span class='add'>-> `lazyop_to_uop`</span>
<span class='add'></span>
<span class='add'>`sink` that is passed to UOpGraph:</span>
<span class='add'>```python</span>
<span class='add'>UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.bigint, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.bigint, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bool, arg=True, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>))</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>which is then linearized through `UOpGraph.linearize`</span>
<span class='add'></span>
<span class='add'>patternmatcher comes in. a new pattern matcher is used that merges patterns from const_folder and transcendental_folding</span>
<span class='add'></span>
<span class='add'>```python</span>
</div>
</div>
</div>
</div>
</div>
<span class='add'>tinygrad/codegen/uops.py</span>
<div class='indent'>
<span class='add'>def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for pat/uop and recursively their source pats and uops:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if pat and uop are valid / match: add uop to store at pat.name if there is no other uop for pat.name</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>for any matches, the callable stored in the pattern matcher will return the replacement uop.</span>
<span class='add'></span>
<span class='add'>new `sink` after pattern matching</span>
<span class='add'>```python</span>
<span class='add'>UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>more pattern matching with const_folder + transcendental_folding + expander + float4_folding</span>
<span class='add'>then again with&nbsp;&nbsp;&nbsp;&nbsp;const_folder + transcendental_folding + expander + reducer</span>
<span class='add'>which does not change sink in any way</span>
<span class='add'></span>
<span class='add'>does toposort, adds and end for the range, removes SINK, which just indicated MetaOps.KERNEL</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>UOPGraph._uops = [</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ENDRANGE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>]</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>![](attachments/net.0.uops.svg)</span>
<span class='add'>UOp graph from `GRAPHUOPS=1` context variable</span>
<span class='add'>TODO: colors mean anything? -> `codegen/uopgraph.py` -> `UOPGraph.graph`</span>
<span class='add'></span>
<span class='add'>kernel gets a name (`codegen/kernel.py` -> `Kernel.name`)</span>
<span class='add'>r if any reduceops</span>
<span class='add'>C if only BufferOps</span>
<span class='add'>else E</span>
<span class='add'>+</span>
<span class='add'>optional len(Kernel.ast.src) if > 1 else nothing</span>
<span class='add'>+</span>
<span class='add'>`_`</span>
<span class='add'>+</span>
<span class='add'>numbers for shapes in different colors (?) joined by black `_`</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>Render:</span>
<span class='add'></span>
<span class='add'>iterates through uops, globals and const are stored as their name and value and inserted when needed</span>
<span class='add'>otherwise translates 1:1 UOps to valid C code.</span>
<span class='add'>```c</span>
<span class='add'>void E_3(int* restrict data0, const int* restrict data1) &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for (int ridx0 = 0; ridx0 &lt; 3; ridx0++) &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int val0 = data1[ridx0];</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data0[ridx0] = (val0+2);</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>appears to cache the kernel as bytes in an sqlite3 database?</span>
<span class='add'>writes bytes to a temp file</span>
<span class='add'>loads the kernel function from the temp file as a function using `ctypes.CDLL(path)[fname]`</span>
<span class='add'></span>
<span class='add'>`method_cache` stores the kernel as a `CompiledRunner`</span>
<span class='add'></span>
<span class='add'>returns the second `ExecItem`:</span>
<span class='add'>```python</span>
<span class='add'>ExecItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=tinygrad.engine.realize.CompiledRunner &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clprg = &lt;tinygrad.runtime.ops_clang.ClangProgram object at 0x76d56a0fb4f0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = &lt;tinygrad.runtime.ops_clang.ClangDevice object at 0x76d56a0a2b30>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = 'E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = "CLANG",</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first_run = True,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lsd_estimate = 24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lib = &#123;horrible bytemess},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate = 24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op_estimate = 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p = Program(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src='\nvoid E_3(int* restrict data0, const int* restrict data1) &#123;\n&nbsp;&nbsp;&nbsp;&nbsp;for (int ridx0 = 0; ridx0 &lt; 3; ridx0++) &#123;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int val0 = data1[ridx0];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data0[ridx0] = (val0+2);\n&nbsp;&nbsp;&nbsp;&nbsp;}\n}',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname='CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uops=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ENDRANGE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate=24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;global_size=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;local_size=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vars=[],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globals=[0, 1],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outs=[0],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_ran_post_init=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:True device:CLANG size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__]</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>allocates buffers</span>
<span class='add'>calls the loaded c function, measuring time it takes to execute.</span>
<span class='add'></span>
<span class='add'>gets output buffer (moves it to a new buffer if not allow_zero_copy?)</span>
<span class='add'>memoryview is cast to Tensor.dtype.fmt and Tensor.shape (?)</span>
<span class='add'>memoryview.tolist() returns the final output.</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='date' id='t2024-08-11-11:15'>2024 08 11 11:15</span><div class='indent'>
<span>blog post.md</span>
<div class='indent'>
<span class='hdg'>&#123;&#123;title}}</span>
<div class='indent'>
<span class='add'>[TOC]</span>
<span class='rem'>- [[#Direction]]</span>
<span class='rem'>- [[#More refined]]</span>
<span class='rem'>- [[#Less refined]]</span>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'>read</span>
<span class='add'>- tensor.tolist() CLANG introduced</span>
<span class='add'>- tensor.tolist() CUDA introduced</span>
<span class='add'>- gpt2 CUDA introduced</span>
<br>
<span class='add'>consider more abstract layers all the way to the mission., connect to code</span>
<span class='rem'>trace execution of a tinygrad script</span>
<span class='rem'>- steps:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.tensor import Tensor`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3])`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3]) + 2`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `(Tensor([1,2,3]) + 2).tolist()</span>
<span class='rem'>read tensor.py</span>
<span class='rem'>explore anything unfamiliar</span>
<span class='rem'>condense any writing</span>
<span class='rem'>create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</span>
<span class='rem'></span>
<span class='rem'>python inliner for tinygrad?</span>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>tinygrad inliner</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>from reading its not obvious what happens</span>
<span class='rem'>could miss parts</span>
<span class='rem'>can't keep track of values</span>
<span class='rem'>reading too much that is irrelevant</span>
<span class='rem'></span>
<span class='rem'>inlined code would be a practical story through a structure of relationships</span>
<span class='rem'>tinygrad code lays out the structure directly</span>
<span class='rem'></span>
<span class='rem'>- write an executable python file that fulfills the same function as the traced script without calling functions</span>
<span class='rem'>- classes are maintained (Tensor, LazyBuffer(?))</span>
<span class='rem'>- function variables are renamed to be unique</span>
<span class='rem'>- function calls = indented comment = function name, source file, line number</span>
<span class='rem'>- return = indented comment</span>
<span class='rem'>- for loops and comprehensions are only shown once</span>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='rem'>Importing Tensor</span>
<span class='add'>Notes on introduction of tensor.tolist() on CLANG</span>
<div class='indent'>
<span class='rem'>```python</span>
<span class='rem'>from tinygrad.tensor import Tensor</span>
<span class='rem'>```</span>
<br>
<span class='add'>TODO: WINO context var to enable winograd optimization?</span>
<span class='add'>`BinaryOps`:</span>
<span class='add'>- `IDIV` = integer division</span>
<span class='add'>- `CMPLT` = &lt;</span>
<span class='add'>- `SHL` = shift left</span>
<span class='add'>- `THREEFRY` = rng related</span>
<span class='add'>`TernaryOps`:</span>
<span class='add'>- `WHERE` = Multiplexer</span>
<span class='add'>- `MULACC`: `x*y+z</span>
<span class='add'>`ReduceOps`:</span>
<span class='add'>- `WMMA` = wave matrix multiply accumulate: hardware accelerated matrix multiplication</span>
<span class='rem'>sets the stage with 3749 lines of tinygrad code as determined through `sys.settrace` (2024-07-08 17:27)</span>
<span class='rem'>![](attachments/tinygrad_import_tensor.png)</span>
<span class='rem'>Mostly [imports](https://docs.python.org/3/reference/import.html) and the construction of the `PatternMatcher` in `tinygrad/codegen/uops.py` (marked with cyan left border)</span>
<span class='rem'>13: `helpery.py` </span>
<span class='rem'>- makes `U` and `T` `TypeVar`s</span>
<span class='rem'>- determines if the computer runs OSX to set the location of tinygrads cache</span>
<span class='rem'>- sets and caches environment variables as `ContextVar` objects.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- DEBUG, IMAGE, BEAM, NOOPT, JIT</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- WINO, THREEFRY, CAPTURING</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- MULTIOUTPUT, PROFILE</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- this does not cover all environment variables relevant to tinygrad, not even those mentioned in the docs as [global variables](https://docs.tinygrad.org/env_vars/#global-variables)</span>
<span class='rem'>- Global Counters: `global_ops`, `global_mem`, `time_sum_s`, `kernel_count`, `mem_used`</span>
<span class='rem'>- ProfileLogger (?)</span>
<span class='rem'>- sets up cache db path, cachelevel and version (?)</span>
<span class='rem'>206: `dtype.py`</span>
<span class='rem'>- `ConstType = Union[float, int, bool]`</span>
<span class='rem'>- declares dtypes as DType Objects and some aliases:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, float16, bfloat16, float32, float64</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- half = float16; float = float32; double = float64 </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- char = int8; short = int16; int = int32; long = int64</span>
<span class='rem'>- sets default float by environment variable else `float32` and default int `int32`</span>
<span class='rem'>- `promo_lattice` that defines how different dtypes get promoted, presumably when different dtypes meet in an operation.</span>
<span class='rem'>- `DTYPES_DICT` and `INVERSE_DTYPES_DICT` to translate between tinygrad dtypes and their names like "bool": dtypes.bool</span>
<span class='rem'>367: `shape/symbolic.py`</span>
<span class='rem'>- `sint = Union[int, Variable, MulNode, SumNode]`</span>
<span class='rem'>- `render_python: Dict[Type, Callable[..., str]]`&nbsp;&nbsp;&nbsp;&nbsp;where the callables return a string representing the Object in `Type`.</span>
<span class='rem'>581: `ops.py`</span>
<span class='rem'>- tinygrads ops are defined:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `UnaryOps(Enum)`: `EXP2`, `LOG2`, `CAST`, `BITCAST`, `SIN`, `SQRT`, `NEG`, `RECIP`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BinaryOps(Enum)`: `ADD`, `MUL`, `IDIV`, `MAX`, `MOD`, `CMPLT`, `CMPNE`, `XOR`, `SHL`, `SHR`, `OR`, `AND`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `TernaryOps(Enum)`: `WHERE`, `MULACC`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `ReduceOps(Enum)`: `SUM`, `MAX`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BufferOps(Enum)`: `LOAD`, `CONST`, `STORE`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `LoadOps(Enum)`: `EMPTY`, `CONST`, `COPY`, `CONTIGUOUS`, `CUSTOM`, `ASSIGN`, `VIEW`</span>
<span class='rem'>- `Op = Union[UnaryOps, BinaryOps, ReduceOps, LoadOps, TernaryOps, BufferOps]`</span>
<span class='rem'>- `UNSAFE_PAD_OPS = &#123;UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}`</span>
<span class='rem'>- `InterpretedFlopCounter: Dict[Op, Callable]` which generates `FlopCounter` objects with shape, flops and memory for various lazyops except `LoadOps`, `TernaryOps.MULACC`</span>
<span class='rem'>- `python_alu` implements lazyops using python and its math module. covers `UnaryOps` except `CAST` and `BITCAST`, `BinaryOps` and `TernaryOps.WHERE`.</span>
<span class='rem'>- `truncate: Dict[DType, Callable]` providing functions to truncate any number to the desired dtype.</span>
<span class='rem'>754: `codegen/uops.py` (Note: quick reserach says UOps are really $\mu$ (micro) operations, UPat presumably is $\mu$ pattern)</span>
<span class='rem'>- The `UOps(Enum)` class variables:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `SINK`,`VAR`,`DEFINE_GLOBAL`,`DEFINE_VAR`,`DEFINE_LOCAL`,`DEFINE_ACC`,`CONST`,`SPECIAL`,`NOOP`,`UNMUL`,`GEP`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `CAST`,`BITCAST`,`VECTORIZE`,`ALU`,`WMMA`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `LOAD`,`STORE`,`PHI`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BARRIER`,`IF`,`RANGE`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `ENDRANGE`,`ENDIF`</span>
<span class='rem'>- `TypeVar` `T`</span>
<span class='rem'>- `constant_folder` which constructs a `PatternMatcher` singleton with a `patterns:List[Tuple[Union[UPat, UOp], Callable]]` (~500 lines)</span>
<span class='rem'>- `PatternMatcher`'s initialization takes ~1300 more lines as it constructs `UPat` objects and runs their `compile` function.</span>
<span class='rem'>2694: `device.py`</span>
<span class='rem'>- `Device = _Device()` singleton, which populates `Device._devices` with strings of devices for which there is a `runtime/uops_&#123;device}.py` file</span>
<span class='rem'>- sets defaults in `BufferOptions` class: `image = None`,`uncached`,`cpu_access`,`host`,`nolru` are all `False`</span>
<span class='rem'>- `MallocAllocator = _MallocAllocator()` singleton (no `__init__`)</span>
<span class='rem'>2816: `lazy.py`</span>
<span class='rem'>- `lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()`</span>
<span class='rem'>- `view_supported_devices = &#123;"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}`</span>
<span class='rem'>2920: `codegen/kernel.py`</span>
<span class='rem'>- `OptOps(Enum)`: `TC`,`UPCAST`,`UPCASTMID`,`UNROLL`,`LOCAL`,`GROUP`,`GROUPTOP`,`NOLOCALS`,`PADTO`</span>
<span class='rem'>- `LocalBuffer` dataclass with `name`,`size`,`dtype=dtypes.float32`,`realized=None`</span>
<span class='rem'>3007: `codegen/linearizer.py`</span>
<span class='rem'>- `render_ops: Dict[Type, Callable[..., UOp]]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- for `NumNode`,`Variable`,`MulNode`,`DivNode`,`ModNode`,`LtNode`,`SumNode`,`AndNode</span>
<span class='rem'>~3100: `engine/schedule.py`</span>
<span class='rem'>- `SCHEDULES: List = []`</span>
<span class='rem'>3299: `tensor.py`</span>
<span class='rem'>- `Tensor` class with:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `__slots__ = "lazydata", "requires_grad", "grad", "_ctx"`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `__deletable__ = ('_ctx',)`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `training`, `no_grad` are `False`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `_seed = int(time.time())`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `_rng_counter = None`</span>
<span class='rem'>- produces methods on `Tensor`class for each device in `Device._devices` like `Tensor.cuda()` as aliases for `Tensor.to("cuda")`</span>
<span class='rem'>- if `IMAGE` from environment variables `>0`, creates more aliases for `Tensor.image_conv2d` and `Tensor.image_dot` by introducing `Tensor.conv2d` and `Tensor.dot` respectively.</span>
<span class='rem'>3646: `nn/state.py`</span>
<span class='rem'>- `safe_dtypes`and `inverse_safe_dtype` dictionaries for translating between some naming (?) to tinygrad dtypes and back (inverse)</span>
<span class='rem'>3728: `engine/jit.py`</span>
<span class='rem'>- `ReturnType = TypeVar("ReturnType")`</span>
</div>
<span class='rem'>Creating a Tensor</span>
<div class='indent'>
<span class='rem'></span>
<br>
<span class='add'>class UOps(Enum):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;SINK = auto(); EXPAND = auto(); CONTRACT = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:15</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:16</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;CONST = auto(); SPECIAL = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# codegen/uops.py:17</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;NOOP = auto(); GEP = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:18</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:20</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ALU = auto(); REDUCE = auto(); WMMA = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# codegen/uops.py:21</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:23</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:25</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ENDRANGE = auto(); ENDIF = auto() # noqa: E702 </span>
<span class='rem'>Tensor(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data: Union[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConstType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;List,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tuple,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ndarray,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MultiLazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Variable,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: Optional[Union[str, tuple, list]] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: Optional[DType] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;requires_grad: Optional[bool] = None,</span>
<span class='rem'>)</span>
<br>
<span class='add'>`PatternMatcher`?</span>
<span class='rem'>```python</span>
<span class='rem'>from tinygrad.tensor import Tensor</span>
<span class='rem'>Tensor([1,2,3])</span>
<span class='rem'>```</span>
<br>
<span class='rem'>![](attachments/tinygrad_construct_tensor.png)</span>
<span class='rem'>9656 lines (the linearizer-lowerer commit ([#4957](https://github.com/tinygrad/tinygrad/commit/6972a2569f5a848b101f4c9310d5de373328dbfb)) changed this, documentation is paused as this might be cleaned up), the cyan line marks the border between previous import code and new tensor construction code. most new code comes from `runtime/autogen/cuda.py`(magenta left border) because in this case, cuda is the device it finds for the Tensor.</span>
<br>
<span class='rem'>determine device for the Tensor using `Device.canonicalize()`, which merely formats `device` if it's not `None`, but since it is, responsibility is handed to `Device.DEFAULT` to find one.</span>
<span class='rem'>- it looks for `&#123;DEVICE}=1` in environment variables</span>
<span class='rem'>- `Device[&#123;device}]` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM`, -> `Device.__get_canonicalized_item` -> eventually tries `&#123;device}Device.__init__(&#123;device})` (like `CUDADevice`) in their respective `runtime/ops_&#123;device}.py` until it finds one that returns no errors.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `METAL` fails within 3 lines when it tries to import the `Metal` library.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `AMD` imports the `AMDRenderer` from `renderer/cstyle.py` (runs ~300 lines of importing and classvariable definitions), then imports from `runtime/driver/hip_comgr.py` which tries `runtime/autogen/comgr.py` and fails within 15 lines.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `CUDA` should fail within ~30 lines when it tries to get `libcuda.so` but in this case cuda is installed, so it imports from `runtime/ops_cuda.py`, `runtime/autogen/cuda.py` (4000+ lines of mysterious code) and `runtime/autogen/nvrtc.py`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.cstyle import CUDARenderer` which is already available from the AMD attempt earlier.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.assembly import PTXRenderer`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTXRenderer` has lots of class variables:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `device="CUDA"`, `suffix="PTX"`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `global_max = (2147483647, 65535, 65535)`, `local_max = (1024, 1024, 64)`, `shared_max = 49152`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `tensor_cores: List[TensorCore]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `kernel_prefix`, `barrier`, `gid`, `gdim`, `lid`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `asm_for_op:Dict[Op, Callable]` by all appearances functions for op->assembly translation</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `supports_half: List[Op]` with a small selection of ops</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `types: Dict[DType, str]` and `men_types: Dict[DType, str]` (almost identical, except for 3 types(?)) to translate between tinygrad dtypes and apparently some other convention</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `const_requires_mov: List[DType] = [dtypes.half, dtypes.bool]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `ptx_matcher` is another `PatternMatcher`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTX = getenv("PTX")`, 0 if not given.</span>
<span class='rem'>`CUDADevice.__init__` gets itself `device_id`, `cu_device`, `context`, `arch`, `pending_copyin`, checking that the interactions with cuda (`libcuda.so`) return no errors on multiple occasions.</span>
<span class='rem'>`CUDADevice.devices.append(self)`</span>
<span class='rem'>9406: `from tinygrad.runtime.graph.cuda import CUDAGraph`</span>
<span class='rem'>calls</span>
<br>
<span class='add'>a = (Tensor([1,2,3], device="CLANG") + 2)</span>
<span class='rem'>Compiled.__init__(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;CUDAAllocator,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;PTXRenderer(self.arch) if PTX else CUDARenderer(self.arch)`, # PTX=0 (default)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;PTXCompiler(self.arch) if PTX else CUDACompiler(self.arch),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;functools.partial(CUDAProgram, self),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph=CUDAGraph</span>
<span class='rem'>)</span>
<br>
<span class='rem'>which is the superclass of `CUDADevice`, where `dname`(device name), `allocator`, `renderer`, `compiler`, `runtime`, `graph`&nbsp;&nbsp;&nbsp;&nbsp;come together and are stored in `self` (ultimately in `CUDADevice` as it inherits these instance variables from its parent classes.</span>
<span class='rem'>- `CUDAAllocator` inherits from `LRUAllocator`, calls `super().__init__()` which only runs `self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)`(sidenote: `LRUAllocator` itself also inherits from `Allocator`).</span>
<span class='rem'>- `CUDARenderer` initialization in this case stores `[]` in `self.tensor_cores`</span>
<span class='rem'>- `CUDACompiler` (child of `Compiler`) gets itself `self.arch`, `self.compile_options` and `super().__init__(f"compile_cuda_&#123;self.arch}")` which sets `self.cachekey` unless explicitly preventes through env variable `DISABLE_COMPILER_CACHE`</span>
<span class='rem'>- `CUDAGraph`, notably is not initialized, the imported class is just passed on.</span>
<br>
<span class='rem'>in `Compiler.__init__()` if `compiler` was `None` it would be replaced by the generic `Compiler()` and `renderer` by `Renderer()`.</span>
<span class='add'>- data -> numpy ndarray</span>
<span class='add'>- LazyBuffer with shapetracker from ndarray.shape</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- lazycache disabled, but `LAZYCACHE` context variable is 1 by default</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `_METADATA.get()` is `None` by default</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `LazyBuffer` gets a `Buffer` that does not allocate anything yet.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- gets allocator from `Device`, gets `NpyDevice` from&nbsp;&nbsp;&nbsp;&nbsp;`ops_npy.py`, which is unnecessary</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- puts the ndarray into `Buffer._buf`, deletes `LazyBuffer.srcs` (was empty tuple), so that `LazyBuffer.realized` returns `True`</span>
<span class='add'>- new `LazyBuffer` now on `CLANG`, same shapetracker, `MetaOps.COPY`, `arg` = `Buffer.size`, `srcs` = tuple with previous (npy) lazybuffer</span>
<br>
<span class='rem'>`CudaDevice` returned to `Device.__get_canonicalized_item` and cached (`@functools.lru_cache(maxsize=None)` decorator):</span>
<span class='add'>before addition:</span>
<br>
<span class='add'>Tensor.lazydata &#123;</span>
<span class='rem'>CUDADevice &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'cu_device': c_int(0),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'context': &lt;tinygrad.runtime.autogen.cuda.LP_struct_CUctx_st at 0x7f7a12c49a40>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'arch': 'sm_61',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'pending_copyin': [],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'dname': 'CUDA',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'allocator': &lt;tinygrad.runtime.ops_cuda.CUDAAllocator at 0x7f7a12dad9f0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'compiler': &lt;tinygrad.runtime.ops_cuda.CUDACompiler at 0x7f7a12cd7b20>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'runtime': functools.partial(&lt;class 'tinygrad.runtime.ops_cuda.CUDAProgram'>, &lt;tinygrad.runtime.ops_cuda.CUDADevice object at 0x7f7a12daeb60>),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'graph': tinygrad.runtime.graph.cuda.CUDAGraph,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'renderer': &lt;tinygrad.renderer.cstyle.CUDARenderer at 0x7f7a12c244f0></span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'>also this `CUDADevice` is stored in classvariable `CUDADevice.devices:List[CUDADevice]`</span>
<span class='rem'>if `DEBUG>=1`, a message will inform that the device was opened.</span>
<span class='rem'></span>
<span class='rem'>for now, the returned `CUDADevice` only demonstrates that `CUDA` can be used as a device for the new Tensor. environmentvariable `CUDA` is set to `1` to save this work in the future.</span>
<span class='rem'></span>
<span class='rem'>In Tensor construction, depending on type of data input, `_loadop()`, `_fromnp` or `_frompy` create the tensors `LazyBuffer`.</span>
<span class='rem'>The example Tensor construction determines dtype (`dtypes.default_int`), then</span>
<span class='rem'>`data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))`</span>
<span class='rem'>(numpy as a dependency is phased out, so this probably changes soon)</span>
<span class='rem'>`_from_np_dtype` uses a dictionary from `dtype.py` to translate the numpy dtype to a tinygrad `DType`</span>
<span class='rem'>-> `LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@staticmethod</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def loadop(op, shape:Tuple[sint,...], dtype:DType, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert isinstance(src, tuple)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)</span>
<span class='rem'>```</span>
<span class='rem'>`op` was given as `LoadOps.EMPTY`</span>
<span class='rem'>```python</span>
<span class='rem'>ShapeTracker.from_shape(shape:Tuple[sint, ...]): return ShapeTracker((View.create(shape),))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class View:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;shape: Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;strides: Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;offset: sint</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;mask: Optional[Tuple[Tuple[sint, sint], ...]]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;contiguous: bool</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ShapeTracker:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;views: Tuple[View, ...]</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>create_lazybuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Any = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enable_cache = bool(getenv("LAZYCACHE", 1))</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>in `create_lazybuffer` the `lazycache` is interacted with, which stores lazybuffers. a `cache_key` is generated from the lazybuffers parameters. If the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor, where it will pass `metadata=_METADATA.get()` as `metadata` ([#5271](https://github.com/tinygrad/tinygrad/commit/9150a6be7a30bbd17f0b84f3352fac7af0c68b73)):</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>LazyBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata:Optional[Metadata]=None</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`st` is the `ShapeTracker` just created</span>
<span class='rem'></span>
<span class='rem'>In the lazybuffer's initialization, it finds that `base` is `None` and decides that an assignment to `self.buffer` is in order.</span>
<span class='rem'>Given the op `LoadOps.EMPTY`, it makes a `Buffer` (a class imported from `tinygrad.device`) through `Buffer(device, self.size, dtype)`. But creating it like that in this case does nothing except store the instance.</span>
<span class='rem'>the buffer's `_lb_refcount` property is incremented by 1</span>
<span class='rem'>the `contiguous_child` property (didn't exist before) is set to `None`</span>
<span class='rem'>and `forced_realize` to `False`</span>
<span class='rem'>the meaning of all 3 escapes me right now.</span>
<span class='rem'></span>
<span class='rem'>The `LazyBuffer` is done and returning to `_fromnp()` into the variable `ret` where:</span>
<span class='rem'>`ret.buffer.allocate(x)` (x is a numpy array) causes the buffer to find itself an `Allocator`:</span>
<span class='rem'>`self.allocator = Device[self.device].allocator`. Indexing into `Device` returns a `NpyDevice` (same as earlier when it was about finding an available device, but this time with `NPY`. This device is very minimal, has the default `Compiler` and `Renderer` and a mostly empty `NpyAllocator`)</span>
<span class='rem'></span>
<span class='rem'>on `buffer.allocate(x)` where `x` is the `np.ndarray`, `x` is just assigned to `buffer._buf`, without calling `Buffer.alloc` which is not implemented for this device.</span>
<span class='rem'>completing what is commented "fake realize" in `_fromnpy`, `del ret.srcs` (which was `()`) makes sure that `LazyBuffer.realized` will return `True`.</span>
<span class='rem'>Also adds the buffer's size to `GlobalCounters.mem_used`</span>
<span class='rem'></span>
<span class='rem'>In the final step of `Tensor` initialization, the mismatching devices, one being the discovered one (`CUDA` in this case) and one being `NPY` are detected and `self.lazydata = data.copy_to_device(device)` takes care of it, `data` being the created `LazyBuffer` and `device` being the discovered device from the start.</span>
<span class='rem'>`LazyBuffer.copy_to_device(device)` in this case leads to `self.base._copy(device)._view(self.st)`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
</div>
</div>
</div>
<span class='rem'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='rem'>return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, LoadOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>assign a&nbsp;&nbsp;&nbsp;&nbsp;`Buffer` to the `LazyBuffer`, because `base` is `None` again (the npy lazybuffer is stored in `srcs`).</span>
<span class='rem'></span>
<span class='rem'>the `._view(self.st)` that follows `._copy(device)`, does nothing here, because the new shapetracker has the same shape and is contiguous.</span>
<span class='rem'></span>
<span class='rem'>The final object looks like this:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata': &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,), </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;MetaOps.COPY: 3>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (&lt;LB NPY (3,) int (&lt;MetaOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>addition calls `_metadata_wrapper`, sets `_METADATA` to `__add__`, the function being called.</span>
<span class='add'>to add, 2 turns into a Tensor where the lazydata has `op=MetaOps.CONST, shape=(), arg=2`</span>
<span class='add'>to add, shapes must match, the "2 Tensor" is broadcasted from shape `()` to `(3,)`</span>
<span class='add'>`()`-> reshape -> `(1,)` -> expand -> `(3,)`</span>
<span class='add'></span>
<span class='add'>by default `MERGE_VIEW=1`, so the latest `View` in the `ShapeTracker` is replaced by the new one.</span>
<span class='add'>new `LazyBuffer` with `base` being the previous `LazyBuffer`</span>
<span class='add'>`expand` always replaces latest view in shapetracker regardless of `MERGE_ViEW`</span>
<span class='add'>again new `LazyBuffer` with the base of the previous one. The "intermediate" lazybuffer from reshape is garbage collected.</span>
<span class='add'></span>
<span class='add'>Tensor(2) after broadcasting:</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Tensor.lazydata &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': expand,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': LazyBuffer &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 1,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.COPY: 3>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;MetaOps.CONST: 2>,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': LazyBuffer &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'NPY',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.EMPTY: 1>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CLANG size:1 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='rem'>Adding to a Tensor</span>
<div class='indent'>
<span class='add'>addition creates new LazyBuffer;</span>
<span class='rem'>```python</span>
<span class='rem'>t = Tensor([1,2,3]) + 2</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>goes to `Tensor.add(self, x, reverse=False)`</span>
<span class='rem'>-> `return F.Add.apply(*self._broadcasted(x, reverse))`</span>
<span class='rem'></span>
<span class='rem'>`self._broadcasted` determines dtype then creates Tensor from `y` (2) using:</span>
<span class='rem'>`Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)`</span>
<span class='rem'>where `dtypes.as_const()` casts the input using one of pythons `int`, `float`, `bool` functions. Reason still escapes me.</span>
<span class='rem'></span>
<span class='rem'>bypassing the whole numpy story because data is integer and not array this time, so lazybuffer comes more directly from `_loadop(LoadOps.CONST, tuple(), dtype, device, data)` where `data` eventually ends up as the lazybuffers `arg` property.</span>
<span class='rem'></span>
<span class='rem'>The `ShapeTracker` will be empty, because the provided shape is `tuple()`. (its a 0D Tensor)</span>
<span class='rem'></span>
<span class='rem'>Because `op` is `LoadOps.CONST` and dtype is `int` the data once again runs through `dtypes.as_const()` and `enable_cache` (-> `lazycache`)&nbsp;&nbsp;&nbsp;&nbsp;is enabled.</span>
<span class='rem'></span>
<span class='rem'>the returned `Tensor.lazydata`:</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 1,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': __add__,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;BinaryOps.ADD: 1>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>```</span>
<br>
<span class='rem'>back in `_broadcasted`, dtypes of x and y are matched</span>
<span class='rem'>`_broadcast_shape(x.shape, y.shape)` determines a target shape</span>
<span class='rem'>and broadcast `x` and `y` to that shape (x is already that shape so nothing happens)</span>
<span class='rem'></span>
<span class='rem'>`padded = _pad_left(y.shape, shape)` where `shape` is the target shape transforms `()` to `(1,)`, ready to be expanded through `F.Expand.apply(self.reshape(padded), shape=shape)`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='rem'>all `Function` "children", in their `apply`function, return a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if&nbsp;&nbsp;&nbsp;&nbsp;applicable. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='rem'>`Function.apply()` calls the functions `forward` method on the `Tensor.lazydata`</span>
<span class='rem'></span>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape(newShape))` in `lazy.py`.</span>
<span class='rem'>In `st.reshape(newShape)` (`shapetracker.py`), by default, the new returned `ShapeTracker` will have its most recent view in `views` replaced by a new one, through `View.reshape(newShape)`.</span>
<span class='rem'>Environment variable `MERGE_VIEWS=0` changes this behaviour to including all previous views with the new one appended in the new shapetracker.</span>
<span class='rem'></span>
<span class='rem'>`View.reshape(newShape)` in this case simply returns a new View from `View.create(newShape)`</span>
<span class='rem'>strides for the new shape&nbsp;&nbsp;&nbsp;&nbsp;are determined (`strides_for_shape(shape)` -> `(1,)`) and canonicalized -> `(0,)`.</span>
<span class='rem'>finally:</span>
<span class='rem'>```python</span>
<span class='rem'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='rem'>return View(shape, strides, offset, mask, contiguous)</span>
<br>
<span class='add'>>The `LazyBuffer` graph specifies the compute in terms of low level tinygrad ops. Not all LazyBuffers will actually become realized. There's two types of LazyBuffers, base and view. base contains compute into a contiguous buffer, and view is a view (specified by a ShapeTracker). Inputs to a base can be either base or view, inputs to a view can only be a single base.</span>
<span class='add'>>- [tinygrad docs](https://docs.tinygrad.org/developer/developer/#tinygrad.lazy.LazyBuffer)</span>
<span class='rem'>back at `_view(newShapetracker)` in `lazy.py` a new lazybuffer comes from `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`.</span>
<span class='rem'>notably, `self.base` is just `self` because `self._base` is `None`</span>
<span class='rem'>```python</span>
<span class='rem'>@property</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def base(self) -> LazyBuffer: return self._base if self._base is not None else self</span>
<span class='rem'>```</span>
<br>
<span class='rem'>next from `F.Expand.apply(self.reshape(padded), shape=(3,))`, where `self.reshape(padded)` has now returned the new Tensor. Expand similarly returns a new Tensor with a new LazyBuffer from&nbsp;&nbsp;&nbsp;&nbsp;`LazyBuffer.expand` -> `ShapeTracker.expand` -> `View.expand` -> `View.create(new_shape, self.strides, self.offset, mask)` -> `View` -> `ShapeTracker` -> `LazyBuffer._view` -> `createLazyBuffer` -> `LazyBuffer`</span>
<span class='add'>Realize</span>
<div class='indent'>
<span class='rem'>notably, `View.create` does not change strides and since no mask was given it also remains `None`. These lines:</span>
<br>
<span class='add'>a.tolist()</span>
<span class='rem'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='rem'>return View(shape, strides, offset, mask, contiguous)</span>
<br>
<span class='rem'>cause `contiguous` to be `False` because the unchaged stride is `(0,)`, but the appropriate stride for the new shape would be `(1,)`</span>
<span class='rem'>Notably, `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)` takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer. So in the final Tensor, there remains no reference to the reshape lazybuffer:</span>
<br>
<span class='rem'>```python</span>
<span class='rem'>Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad' : None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata':</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st' : ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous':False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': LazyBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous'=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>Finally, `F.Add.apply` is called on the input tensor and the created Tensor.</span>
<span class='rem'>new tensor lazydata = `return x.e(BinaryOps.ADD, y)` where `BinaryOps.ADD`, like `LoadOps.CONST` is an entry in `class BinaryOps(Enum)`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def e(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;self, </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op:Union[LoadOps, UnaryOps, BinaryOps, TernaryOps],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;*in_srcs:LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Optional[Any] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;) -> LazyBuffer</span>
<span class='rem'>```</span>
<span class='rem'>gets `out_dtype` from input</span>
<span class='rem'>tries shortcuts if one of the operants is effectively 0</span>
<span class='rem'>```python</span>
<span class='rem'>create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides = (1,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset = 0</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous = True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;BinaryOps.ADD: 1></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>, # previously created lazybuffer [1,2,3] copied from NPY </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))> # new lazybuffer from 2</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through `srcs` (in ADD in this case) or `_base` (in shape changes) and so form a graph.</span>
<span class='rem'>```bash</span>
<span class='rem'>DEBUG=4 CUDA=1 python -c "from tinygrad.tensor import Tensor; (Tensor([1,2,3]) + 2).tolist()"</span>
<span class='rem'>```</span>
<span class='rem'>displays a graph that seem to echo this, though shape changes are apparently left out</span>
<span class='rem'>```</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;0  BufferOps.STORE MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp; BinaryOps.ADD None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BufferOps.LOAD MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BufferOps.CONST ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)))</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Realizing a Tensor</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>(Tensor([1,2,3]) + 2).tolist()</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`Tensor.tolist()` = `Tensor.data().tolist()` = `Tensor._data().cast(self.dtype.fmt, self.shape).tolist()`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def _data(self) -> memoryview:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if 0 in self.shape: return memoryview(bytearray(0))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this realizes on the object from as_buffer being a Python object</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()</span>
<br>
<span class='add'>-> `cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()`</span>
<br>
<span class='add'>cast does nothing because its already the right dtype</span>
<span class='add'>contiguous does nothing because its already contiguous</span>
<span class='add'>to does nothing, its already on CLANG. Note: this initially looks like it would realize on CLANG regardless of device but really, it just adds a CLANG lazybuffer to the end of the graph, to move the data to the cpu</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.device != "CLANG": buf.options = BufferOptions(nolru=True)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)</span>
<span class='rem'>```</span>
<br>
<span class='rem'>`Tensor.cast(self.dtype.scalar())` does nothing because `self.dtype == self.dtype.scalar()` in this case.</span>
<span class='rem'>`Tensor.contiguous()` -> `lazydata.base.forced_realize = True`, otherwise nothing in this case, because not needed.</span>
<span class='rem'>`Tensor.to("CLANG")`. if it is not already on CLANG, it makes a new Tensor with the same lazydata, but `device="CLANG"`, so it add a `LoadOps.COPY` Lazybuffer to the graph.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>Schedule</span>
<div class='indent'>
<span class='add'>Schedule</span>
<div class='indent'>
<span class='add'>>The scheduler converts the graph of LazyBuffers into a list of ScheduleItem. One ScheduleItem is one kernel on the GPU, and the scheduler is responsible for breaking the large compute graph into subgraphs that can fit in a kernel. ast specifies what compute to run, and bufs specifies what buffers to run it on.</span>
<span class='add'>>- [tinygrad docs](https://docs.tinygrad.org/developer/developer/#scheduling)</span>
<span class='add'></span>
<span class='rem'>```python </span>
<br>
<span class='add'>```python</span>
<span class='rem'>def schedule_with_vars(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;self,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;*lst:Tensor,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='rem'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# left out some lines that aren't executed</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return memory_planner(schedule), var_vals</span>
<span class='rem'>```</span>
<span class='rem'>where `flatten` in this case returns a list with one entry: the "BinaryOps.ADD-lazybuffer" </span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>SCHEDULES: List = []</span>
<span class='rem'>def create_schedule_with_vars(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='rem'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if seen is None: seen = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph, in_degree, prescheduled = _graph_schedule(outs, seen)</span>
<span class='rem'></span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def _graph_schedule(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Set[LazyBuffer]</span>
<span class='rem'>) -> Tuple[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;DefaultDict[LazyBuffer, List[LazyBuffer]], DefaultDict[LazyBuffer, int],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;Dict[LazyBuffer, _LBScheduleItem]</span>
<span class='rem'>]:</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""create a graph for realizing the outputs"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# start by just realizing the buffers passed in</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;realizes: Dict[LazyBuffer, None] = &#123;x.base:None for x in outs if x.base.realized is None}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs: Dict[LazyBuffer, None] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;simple_pads: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;children: DefaultDict[LazyBuffer, Dict[LazyBuffer, None]] = defaultdict(dict)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for out in outs: _recurse_lb(out.base, realizes, allbufs, simple_pads, children, scheduled=True)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;```</span>
<span class='rem'>strange that it uses `out.base` it means if the latest lazybuffer was already on clang and a reshape, it would be ignored for now.</span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def _recurse_lb(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;buf:LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;realizes:Dict[LazyBuffer, None],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs:Dict[LazyBuffer, None],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads:Set[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled=False</span>
<span class='rem'>):</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""recursively search the entire graph for all LazyBuffers, insert realizes after expands"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in allbufs or buf.base.realized is not None: return</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH: log_lazybuffer(buf, scheduled)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# view</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.base != buf:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fuse some pads</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads.add(buf.base)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize all expands</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif prod(buf.base.st.shape) &lt; prod(buf.st.shape):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op in ReduceOps and buf.base.srcs[0].base.op is LoadOps.CONST:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize reduceops on const (unless base is forced_realize)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize image to image casts. this is part of a larger problem</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# check all other pads for safe fusion</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif any(v.mask is not None for v in buf.st.views): simple_pads.add(buf.base)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# base</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs[buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.forced_realize: realizes[buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in LoadOps: realizes[buf.base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.COPY:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.srcs[0].base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.VIEW: realizes[buf.srcs[0].base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for x in buf.srcs:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if x.base.realized is None: children[x.base][buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recurse_lb(x, realizes, allbufs, simple_pads, children)</span>
<span class='rem'></span>
<span class='rem'>def _is_padding_okay(buf:LazyBuffer, realizes:Dict[LazyBuffer, None]) -> bool:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in realizes or buf.realized is not None: return True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this broke to_image_idx and coder with JIT</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in UNSAFE_PAD_OPS: return False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return all(_is_padding_okay(x.base, realizes) for x in buf.srcs)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`realizes` = lbs with `self.forced_realize` or that are `LoadOps` or source of `LoadOps.COPY` and base of view lbs if the lb was expanded compared to its base, unless exceptions.</span>
<span class='rem'>```python</span>
<span class='rem'>realizes = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None # source of copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)> = None # src of copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None # base of view lb</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`allbufs` = base lbs (no view lazybuffers).</span>
<span class='rem'>the NPY LoadOps.EMPTY lazybuffer isn't included because for it `self.realized` returns true which returns from `_recurse_lb` before it could be added to `allbufs`.</span>
<span class='rem'>```python</span>
<span class='rem'>allbufs = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`simple_pads` = lb base if there is a mask&nbsp;&nbsp;&nbsp;&nbsp;= `&#123;}`</span>
<span class='rem'></span>
<span class='rem'>`children` = unrealized lbs in `srcs`.</span>
<span class='rem'>```python</span>
<span class='rem'>children = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>back in `_graph_schedule`:</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in realizes if x.op is LoadOps.ASSIGN and x not in seen and x.realized is None}</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# check if we have to realize pads</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for p in simple_pads:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not _is_padding_okay(p, realizes):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[p] = None</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# find all reduces, and pair them to a elementwise op. if they can't be cleanly paired, force realize the reduce (or a contig child)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op: Dict[LazyBuffer, LazyBuffer] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for r in allbufs:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if r.op not in ReduceOps or r in realizes: continue</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recursive_group(r, r.st, r, children, realizes, reduce_for_op, group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;can_chase = all(tr not in reduce_for_op for tr in group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# TODO: forced_realize exists because the scheduler is incapable of checking for self-contained DAGs</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = r in group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and len(group) > 1:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a multi output kernel if the LazyBufferss can cleanly group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_parents, rc_children = deque(group), deque(group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_parents and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=rc_parents.pop()).op in ReduceOps: forced_realize = True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: rc_parents.extend(x.base for x in p.srcs if x.base.realized is None and x.base is not r)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# search descendants of the reduceop that can cleanly group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_children and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (c:=rc_children.pop()).op in ReduceOps or not c.st.contiguous or c.st.size != r.st.size or c in reduce_for_op:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants.clear()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if c in realizes and c not in group: realized_descendants.add(c)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_children.extend(x for x in children[c] if x.realized is None and x.device == r.device)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group.update(realized_descendants)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can only fuse assign if no other assign_target is used in the kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and any(x.op is LoadOps.ASSIGN for x in group):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents = deque((r, *group))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while parents and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=parents.pop().base).realized or p in realizes:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if p in assign_targets and assign_targets[p] not in group: forced_realize, can_chase = True, False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents.extend(p.srcs)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = r</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if can_chase:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can chase this down to contiguous children</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = tr.st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while len(children[tr]) == 1:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr_next = next(iter(children[tr]))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st_childs = dedup(s for s in tr_next.srcs if s.base is tr)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(st_childs) > 1: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if st.size != st_childs[0].st.size: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = st + st_childs[0].st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not st.contiguous or tr_next.op in ReduceOps: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr_next</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# don't cast to higher size before store (tr cannot be realized if forced_realize)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if tr.op is UnaryOps.CAST and tr.arg.itemsize > tr.srcs[0].dtype.itemsize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr.srcs[0].base</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op[tr] = r</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[tr] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: reduce_for_op.update((tr, r) for tr in group)</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;output_groups: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in realizes:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.realized is not None or buf.op is LoadOps.CONST or buf in seen: continue</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups[reduce_for_op[buf] if buf in reduce_for_op and MULTIOUTPUT else buf].append(buf)</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# make things that can't be images not images</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if isinstance(buf.dtype, ImageDType) and (prod(buf.shape) != prod(buf.dtype.shape) or</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not any(buf.shape[x]%4 == 0 for x in buf.st.unit_stride_axes())):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2: print(f"forcing image &#123;buf.dtype} with shape &#123;buf.shape} to float32")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.dtype = dtypes.float32</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# hack the underlying buffer too</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base is buf:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert not hasattr(buf.buffer, '_buf'), "can't fixup allocated buffer"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.dtype = dtypes.float32</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.options = None</span>
<span class='rem'> # preschedule all buffers in realizes</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prescheduled = &#123;group[0]:(group, *_lower_lazybuffer(group, realizes, reduce_for_op)) for group in output_groups.values()}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>(current value): </span>
<span class='rem'>```python</span>
<span class='rem'>output_groups = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [same buffer]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None], reduce_for_op:Dict[LazyBuffer, LazyBuffer]):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""describe the computation for a LazyBuffer with LazyOp + inputs + var_vals"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if (out:=outs[0]).op is LoadOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rd = LazyOp(BufferOps.LOAD, (), MemBuffer(1, dtypes.uint8, st:=ShapeTracker.from_shape((out.arg,))))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return (LazyOp(BufferOps.STORE, (rd,), MemBuffer(0, dtypes.uint8, st)), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if out.op in &#123;LoadOps.CUSTOM, LoadOps.COPY, LoadOps.EMPTY, LoadOps.VIEW}: return (LazyOp(out.op, (), out.arg), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = merge_dicts([out.st.var_vals.copy() for out in outs])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in outs if x.op is LoadOps.ASSIGN}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;cache: Dict[Tuple[LazyBuffer, ShapeTracker], LazyOp] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ast: List[LazyOp] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for i, out in enumerate(outs):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_st = ShapeTracker.from_shape(reduce_for_op[out].shape if out in reduce_for_op else out.shape)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view = out.arg[0] if out.op is LoadOps.ASSIGN and out.arg else output_st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lop = _recursive_lazyop(out, inputs, tuple(outs), var_vals, output_st, realizes, assign_targets, cache=cache)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view, vv = output_view.simplify().unbind()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if vv: var_vals.update(vv)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return tuple(ast), inputs, var_vals, dedup([x[0].metadata for x in cache if x[0].metadata and x[0] not in inputs])</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True, eq=False)</span>
<span class='rem'>class LazyOp:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op: Op</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;src: Tuple[LazyOp, ...] = ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None</span>
<span class='rem'></span>
<br>
<span class='add'>class ScheduleItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ast: LazyOp</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: Tuple[Buffer, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>class MemBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;idx: int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<span class='rem'></span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ConstBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;val: ConstType | Variable</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<br>
<span class='add'>`children:Dict[LazyBuffer:Dict[Lazybuffer : None]]` is in direction of execution, so a child is an lb that depends on the current one. stores only if the parent is unrealized</span>
<span class='add'>`realizes:Dict[LazyBuffer : None]` stores unrealized bases and ones with `MetaOps`, so it includes the realized npy lb.</span>
<span class='add'>`output_groups` stores lb that are unrealized and not `MetaOps.CONST`</span>
<span class='add'>lots of complexity is skipped because there are no ReduceOps in the lb graph.</span>
<span class='add'></span>
<span class='add'>lop recursively gets a graph of LazyOp from the LazyBuffer graph.</span>
<span class='add'>MetaOps.CONST - lbs turn into a LazyOp with BufferOps.CONST, no sources and a ConstBuffer</span>
<span class='add'>The realized npy lb turns into a LazyOps with BufferOps.LOAD, no sources and a MemBuffer. adds the buffer to inputs, like other lbs would that are either realized or in realizes but not in the current output_group</span>
<span class='add'>ast gets a LazyOp with BufferOps.STORE, lops as sources and a MemBuffer</span>
<span class='add'></span>
<span class='rem'>`_lower_lazybuffer` returns:</span>
<br>
<span class='add'>`_lower_lazybuffer` returns</span>
<span class='rem'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` for `CLANG` copy </span>
<span class='rem'>- enters `_recursive_lazyop` when processing `output_groups[1]`</span>
<span class='rem'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` again for the `CUDA` copy </span>
<span class='add'>- `LazyOp(MetaOps.KERNEL, tuple(ast))`</span>
<span class='add'>- `list(input)`</span>
<span class='add'>- var_vals</span>
<span class='add'>- some metadata</span>
<br>
<span class='add'>the next group, the lb with MetaOps.COPY returns different: returns a LazyOp with MetaOps.COPY and same sources and arg from the lb.</span>
<span class='add'> </span>
<span class='rem'>`_recursive_lazyop` returns `LazyOp` for the two copy lbs and the add lb in `output_groups`.</span>
<span class='rem'>in the add lb it recurses trough its sources:</span>
<span class='rem'>- `&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- simplify and unbind shapetracker (simplify does nothing here because the shapetracker has only one view. Unbind seems to act on variables, of which there aren't any here).</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- append the lb to `inputs`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.LOAD, (), MemBuffer(len(outputs)+inputs.index(buf), buf.dtype, unbound_st)))</span>
<span class='rem'>- `&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>)`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- switch to its base `&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.CONST, (), ConstBuffer(val, buf.dtype, unbound_st))` where `val` is `arg`, which is 2.</span>
<span class='rem'>`ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))'</span>
<span class='rem'>`return tuple(ast), inputs, var_vals,` + metadata stuff, ignored for now. `var_vals` is `&#123;}` because nothing symbolic in this case.</span>
<span class='rem'></span>
<br>
<span class='rem'>prescheduled:List[Tuple[]] &#123;</span>
<span class='add'>prescheduled:Dict[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;group[0]:LazyBuffer (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"group[0]":LazyBuffer : Tuple[</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: List[LazyBuffer],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"group": List[LazyBuffer]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;abstract syntax tree (ast): Tuple[LazyOp],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"inputs": List[LazyBuffer],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;variable values: Dict[Variable, int],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"var_vals": Dict[],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata: List[?]</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata": List[]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;]</span>
<span class='add'>]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;BinaryOps.ADD: 1>, None)>: (</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(MetaOps.KERNEL, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.STORE, arg=MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BinaryOps.ADD, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.LOAD, arg=MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.CONST, arg=ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[__add__]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>: (</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(MetaOps.COPY, arg=12, src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[__add__ - __main__:3::&lt;module>]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;MetaOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<br>
<span class='add'>... skipping some ...</span>
<span class='rem'>back in `_graph_schedule`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule_targets = &#123;out:ps for ps in prescheduled.values() for out in ps.outputs}</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;in_degree: DefaultDict[LazyBuffer, int] = defaultdict(int)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for key, lsi in prescheduled.items():</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if key not in in_degree: in_degree[key] = 0</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs after all parents are realized</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled_parents = set(schedule_targets[x].outputs[0] for x in lsi.inputs if x in schedule_targets)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for x in scheduled_parents:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[x].append(key)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[key] += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs before a parent is assigned to</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents_assigns = set(schedule_targets[assign_targets[x]].outputs[0] for x in lsi.inputs if x in assign_targets)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for assign in parents_assigns:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[key].append(assign)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[assign] += 1</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return graph, in_degree, prescheduled</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`lsi` = LazyScheduleItem?</span>
<span class='rem'>`schedule_targets` makes an entry for every item in every group and assigns it the tuple in `prescheduled` that it is part of.</span>
<span class='rem'></span>
<span class='rem'>`scheduled_parents = set(schedule_targets[x][0][0] for x in lsi[2] if x in schedule_targets)`</span>
<span class='rem'>`lsi[2]` is inputs, so if an input is one of the entries in a lazybuffer group, add the tuple with its info.</span>
<span class='rem'>this returns an empty set for the third group, because its input (the `NPY` lazybuffer) is not in any group (= not in `output_groups` because it is already realized)</span>
<span class='rem'></span>
<span class='rem'>the input group's `group[0]` as a key in `graph` and append the current prescheduled `key`</span>
<span class='rem'>some detailed explanation: The `ADD` lb is the first key in `graph` because it is the input of the first group (where the key is the `COPY` lb) in `prescheduled` that is also part of group itself. The value it gets assigned is the first item in the group that it was an input of, so, the `COPY` lb.</span>
<span class='rem'>this way, graph "points" from the inputs to the groups that depend on them.</span>
<span class='rem'>every time an input of a group is added to graph this way, the groups key in the `in_degree` dictionary increases by 1.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>graph = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>]</span>
<span class='rem'>}</span>
<span class='rem'></span>
<span class='rem'>in_degree = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: 1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: 1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: 0</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>back in `create_schedule_with_vars`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;queue = deque(si for key, si in prescheduled.items() if in_degree[key] == 0)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule: List[ScheduleItem] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;kernel_number = GlobalCounters.kernel_count</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;while queue:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps = queue.popleft()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in ps.outputs: seen.add(buf)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_number += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: realized_lazybuffer(out, kernel_number)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals = merge_dicts([var_vals, ps.var_vals])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: del out.srcs&nbsp;&nbsp;&nbsp;&nbsp;# can only schedule once</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule.append(si:=ScheduleItem(ps.ast, tuple(x.buffer for x in (ps.outputs+ps.inputs) if x.size != 0)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if logops and si.ast[0].op not in LoadOps and not any(i.device.startswith("DISK:") for i in si.inputs): logops.write(str(si.ast)+"\n")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for x in graph[ps.outputs[0]]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[x] -= 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if in_degree[x] == 0: queue.append(prescheduled[x])</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if SAVE_SCHEDULE:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def _save():</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"saving &#123;len(SCHEDULES)} schedule graphs to", fp:=getenv("SAVE_SCHEDULE_PATH", "schedule.pkl"))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with open(fp, "wb") as f: pickle.dump(SCHEDULES, f)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if len(SCHEDULES) == 0: atexit.register(_save)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;SCHEDULES.extend((ps.ast for ps in prescheduled.values()) if getenv("CAPTURE_AST") else [(graph, prescheduled)])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# confirm everything was scheduled correctly</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if not all(degree == 0 for degree in in_degree.values()) or len(prescheduled) != len(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"cycle detected in graph, prescheduled &#123;len(prescheduled)} but only scheduled &#123;len(schedule)}")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 1 and len(schedule) >= 10: print(f"scheduled &#123;len(schedule)} kernels")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return schedule, var_vals</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>queue = deque([</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp; (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>])</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>adds any buffers of the group to `seen`.</span>
<span class='rem'>deletes `srcs` of lazybuffers in the group</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ScheduleItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ast: Tuple[LazyOp, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: Tuple[Buffer, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>schedule.append(si:=ScheduleItem(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps[1],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;tuple(x.buffer for x in ps[0]+ps[2] if x.size != 0),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps[4]</span>
<span class='rem'>))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>then finds the lb it just made a `ScheduleItem` from in `graph`, which returns the `group[0]` item of the groups that depend on the just processed one.</span>
<span class='rem'>Add the group tuple from `prescheduled` to `queue`.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=LazyOp(MetaOps.COPY, arg=12, src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=LazyOp(MetaOps.KERNEL, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.STORE, arg=MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BinaryOps.ADD, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.LOAD, arg=MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.CONST, arg=ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))), src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0, </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__ - __main__:3::&lt;module>]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__]</span>
<br>
<span class='rem'>with `GRAPH=1`, tinygrad produces output that reflects this schedule:</span>
<br>
<span class='add'>with `GRAPH=1`, tinygrad produces an svg that reflects this schedule:</span>
<br>
<span class='add'>`_internal_memory_planner` does nothing here</span>
<span class='rem'>back in `tensor.py` -> `schedule_with_vars`</span>
<span class='rem'>```python</span>
<span class='rem'>return memory_planner(schedule), var_vals</span>
<span class='rem'>```</span>
</div>
<span class='add'>lower schedule</span>
<div class='indent'>
<span class='rem'>-> `schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# Exclude buffers involved in load ops (e.g transfers) to preserve parallelism in graphs.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assigned = _internal_memory_planner(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[si.bufs for si in schedule],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;noopt_buffers=&#123;b for si in schedule if si.ast.op is not MetaOps.SINK for b in si.bufs}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]</span>
<span class='rem'>```</span>
<br>
<span class='add'>in `lower_schedule_item`, trying to get "transfer" attribute from the allocator, ops_clang are imported</span>
<span class='rem'>`_internal_memory_planner` is optional (`NO_MEMORY_PLANNER=1` skips it and it still works).</span>
<span class='rem'>actually, it skips all buffers where `buf.lb_refcount > 0` anyway , which applies to all of the current buffers since they all come from the `LazyBuffer` constructor, where they automatically get a reference. `assigned` will be `&#123;}`.</span>
<br>
<span class='add'>first ExecItem: </span>
<span class='rem'>`memory_planner` returns the `schedule` exactly as it was.</span>
<span class='rem'></span>
<span class='rem'>then back in `Tensor.realize`</span>
<br>
<span class='rem'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Run</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for ei in lower_schedule(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(capturing) and CAPTURING: capturing[0].add(ei)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ei.run(var_vals, do_update_stats=do_update_stats)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;while len(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;si = schedule.pop(0)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try: yield lower_schedule_item(si)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"error lowering &#123;si.ast.op}")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("tensor operations:")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pprint.pprint(si.metadata, indent=2)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise e</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def lower_schedule_item(si:ScheduleItem) -> ExecItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assert len(set(x.device for x in si.bufs)) == 1 or si.ast.op is MetaOps.COPY or getenv("USE_COPY_KERNEL")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.SINK:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runner = get_runner(si.outputs[0].device, si.ast)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(runner, [si.bufs[x[0]] for x in runner.p.globals], si.metadata)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;out = si.outputs[0]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.COPY:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferCopy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferXfer</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.CUSTOM: return ExecItem(CustomOp(si.ast.arg), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.EMPTY: return ExecItem(EmptyOp(out), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.VIEW: return ExecItem(ViewOp(out), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"don't know how to lower &#123;si.ast}")</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>TODO: `LoadOps` was recently renamed to `MetaOps` and `MetaOps.SINK` was added.</span>
<span class='rem'>on the first `ScheduleItem` which copies from `NPY` to `CUDA`</span>
<span class='rem'>determines `kernel_type=BufferCopy` which is a class.</span>
<span class='rem'>`return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))` initiates `BufferCopy`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>class BufferCopy(Runner):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, total_sz, dest_device, src_device):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if total_sz >= 1e6: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz/1e6:7.2f}M, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz:8d}, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def copy(self, dest, src):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.allocator.copy_from_disk(dest._buf, src._buf, src.nbytes)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fast(ish) path, uses readinto in diskbuffers</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src.allocator.copyout(dest.allocator.as_buffer(dest._buf), src._buf)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.copyin(src.as_buffer(allow_zero_copy=True))&nbsp;&nbsp;&nbsp;&nbsp;# may allocate a CPU buffer depending on allow_zero_copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest, src = rawbufs[0:2]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, &#123;dest.size} != &#123;src.size}, &#123;dest.dtype} != &#123;src.dtype}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = time.perf_counter()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.copy(dest, src)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if wait:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Device[dest.device].synchronize()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return time.perf_counter() - st</span>
<span class='rem'></span>
<span class='rem'>class Runner:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate = True, display_name, dname, op_estimate, mem_estimate</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;@property</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def device(self): return Device[self.dname]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def exec(self, rawbufs:List[Buffer], var_vals:Optional[Dict[Variable, int]]=None) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self(rawbufs, &#123;} if var_vals is None else var_vals)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise NotImplementedError("override this")</span>
<span class='rem'></span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ExecItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prg: Runner</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: List[Optional[Buffer]]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if do_update_stats:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.kernel_count += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_ops += (op_estimate:=sym_infer(self.prg.op_estimate, var_vals))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_mem += (mem_estimate:=sym_infer(self.prg.mem_estimate, var_vals))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if et is not None: GlobalCounters.time_sum_s += et</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ptm = (colored(f"&#123;et*1e3:9.2f}ms", "yellow") if et > 0.01 else f"&#123;et*1e6:9.2f}us") if et is not None else ""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"&#123;colored(f'*** &#123;self.prg.dname[:7]:7s} &#123;GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} &#123;self.prg.display_name+' '*(38-ansilen(self.prg.display_name))} arg &#123;len(self.bufs):3d} mem &#123;GlobalCounters.mem_used/1e9:5.2f} GB " +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(str() if et is None else f"tm &#123;ptm}/&#123;GlobalCounters.time_sum_s*1e3:9.2f}ms (&#123;op_estimate/((et or 1e-20)*1e9):8.2f} GFLOPS, &#123;mem_estimate/((et or 1e-20)*1e9):7.2f} GB/s)" +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f" &#123;[repr(m) if DEBUG >= 3 else str(m) for m in self.metadata] if self.metadata else ''}"))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prg.first_run = False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return et</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`name='copy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'`</span>
<span class='rem'>`colored` from `helpers.py` for ANSI color coding the string</span>
<span class='rem'>after `super(init)` and creating an instance of `ExecItem`, the instance looks like this:</span>
<span class='rem'>```python</span>
<span class='rem'>ExecItem(</span>
<br>
<span class='add'>ExecItem &#123;</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prg=BufferCopy(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=tinygrad.engine.realize.BufferCopy &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = &lt;tinygrad.runtime.ops_clang.ClangDevice object at 0x7147ec6d3be0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp; CLANG &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = "CLANG",</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lds_estimate = 0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;},</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>)</span>
<span class='add'>}</span>
<br>
<span class='add'>`_MallocAllocator` always allocates with `ctypes.c_uint8 * size`</span>
<span class='rem'>after constructing `ExecItem` it is yielded to `run_schedule`</span>
<span class='rem'>-> `ExecItem.run(var_vals=&#123;}, do_update_stats=True)</span>
<span class='rem'>allocates the buffers</span>
<span class='rem'>first the cuda buffer, which through some ugly back and forth calls `CUDAAllocator._alloc`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def _alloc(self, size, options:BufferOptions):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;check(cuda.cuCtxSetCurrent(self.device.context))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if options.host: return init_c_var(ctypes.c_void_p(), lambda x: check(cuda.cuMemHostAlloc(ctypes.byref(x), size, 0x01)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return init_c_var(cuda.CUdeviceptr(), lambda x: check(cuda.cuMemAlloc_v2(ctypes.byref(x), size)))</span>
<span class='rem'>```</span>
<br>
<span class='rem'>`init_c_var` returns the variable after calling the supplied function with the variable.</span>
<span class='rem'>using `cuda` library, which escaped me for now.</span>
<span class='add'>if DEBUG >= 2, it will print the kernel (prg) name and kernel number</span>
<span class='add'>- in magenta if jit</span>
<span class='add'>- in green if the kernel is run the first time</span>
<br>
<span class='add'>"methods" (MetaOps.KERNEL) are cached in the method_cache if they repeat in the schedule</span>
<span class='rem'>`NPY` buffer already has `buf._buf`, which is a `numpy.ndarray` with `[1,2,3]` in it.</span>
<span class='rem'></span>
<span class='rem'>`et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)`</span>
<span class='rem'>-> `BufferCopy(bufs, &#123;}, False)</span>
<span class='rem'>-> `dest.copyin(src.as_buffer(allow_zero_copy=True))` where `src` is `bufs[1]` and `dest` is `bufs[0]`</span>
<span class='rem'></span>
<span class='rem'>`src.as_buffer` -> `return self.copyout(memoryview(bytearray(self.nbytes)))`</span>
<span class='rem'>eventually calls `NPYAllocator.copyout(mv:memoryview, self._buf:np.ndarray)`</span>
<span class='rem'>which mostly does numpy stuff to ensure "C-contiguous array", returns a new memoryview to the new array.</span>
<span class='rem'></span>
<span class='rem'>`dest.copyin` produces `host_mem` on the device through cuda library and more weird cuda things.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<br>
<span class='rem'>watch out for garbo below</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>creating tensors through methods</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- Tensor.empty - no new ops</span>
<span class='rem'>- Tensor.zeros - `full(shape, 0, ...)`</span>
<span class='rem'>- Tensor.ones - `full(shape, 1, ...)`</span>
<span class='rem'>- `full(shape, fill_value)`:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>- Tensor.arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='rem'>- Tensor.eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='rem'>- Tensor.full_like - `full`</span>
<span class='rem'>- Tensor.zeros_like `full_like`</span>
<span class='rem'>- Tensor.ones_like `full_like`</span>
<span class='rem'></span>
<span class='rem'>all Tensor constructors that aren't random build on the `Tensor.full(shape, fill_value)` function, which first *reshapes* the Tensor with 1 element (fill_value) to the target number of dimensions.</span>
<span class='rem'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='rem'></span>
<span class='rem'>all `Function` "children", in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if `requires_grad` is True. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='rem'></span>
<span class='rem'>the `forward` method for `F.Reshape()` is called on the `lazydata`.</span>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`.</span>
<span class='rem'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS=0`, the new view is appended to `views` instead.</span>
<span class='rem'>In the current case, the previous View with shape `(1,)` is directly replaced by the new one `(1,)*len(new_shape)`.</span>
<span class='rem'>finally, the tensor gets a new `LazyBuffer` from&nbsp;&nbsp;&nbsp;&nbsp;`create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`</span>
<span class='rem'></span>
<span class='rem'>after the reshape, the dimension use `Tensor.expand(new_shape)` to get the now correct number of dimensions to the final shape.</span>
<span class='rem'>```python</span>
<span class='rem'>self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))</span>
<span class='rem'>```</span>
<span class='rem'>`argfix` ensures the function works even if the shape was not input as a tuple but through multiple arguments like `reshape(2,2,2)`.</span>
<span class='rem'>`_pad_left` gets inputs to the same number of dimensions.</span>
<span class='rem'>`*` unpacks the tuple with both shapes that `_pad_left` returns</span>
<span class='rem'></span>
<span class='rem'>`Tensor._broadcast_to(self, shape)` runs `_pad_left` again</span>
<span class='rem'>runs `self.reshape` again to the "padded" shape</span>
<span class='rem'>then `F.Expand.apply()` -> `lazybuffer.expand()` -> `shapetracker.expand()` -> `View.expand()` which producees&nbsp;&nbsp;&nbsp;&nbsp;a new `View` with the new shape and everything else being equal. returns a new `ShapeTracker`, returns a new `LazyBuffer`, returns a new `Tensor`</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>Tensor.arange offers new stuff, calling `Tensor._cumsum()`, using Tensor-Int addition and casting the Tensor.</span>
<span class='rem'>from `Tensor._cumsum()`:</span>
<span class='rem'>```python</span>
<span class='rem'>self.transpose(axis,-1).pad2d((pl_sz,-int(_first_zero)))._pool((self.shape[axis],)).sum(-1).transpose(axis,-1)</span>
<span class='rem'>```</span>
<span class='rem'>where `axis` is 0 and `pl_sz` will in this case be `self.shape[0] - 1`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.transpose(0, -1)`, which translates to `Tensor.permute(order)` where in the order dim 0 and the last dim were swapped. `permute` resolves orders with negative dim indices, error checks and runs `F.Permute.apply(self, order=resolve_order)` -> `lazybuffer.permute(order)` -> `ShapeTracker.permute(order)` -> `View.permute(axis=order)` -> `View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)`</span>
<span class='rem'>returns a new `View`in a new `ShapeTracker` in a new `lazybuffer` in a new `Tensor`</span>
<span class='rem'>this transpose changes nothing because the input was a 1D Tensor.</span>
<span class='rem'></span>
<span class='rem'>`Tensor.pad2d(self.shape[0] - 1, 0)` adds `self.shape[0] - 1` 0s to the left on the lowest dimension. Using `pad2d()` seems crazy here, it goes through `Tensor._slice()`, which eventually calls `Tensor.pad((self.shape[0] - 1, 0))` which is even crazier, which calls `F.Pad.apply(...)` which goes on the tour again.</span>
<span class='rem'>`LazyBuffer.pad()` -> `ShapeTracker.pad()` -> `View.pad()`</span>
<span class='rem'>where `(self.shape[0] - 1, 0)` turns into&nbsp;&nbsp;&nbsp;&nbsp;`(-self.shape[0] - 1, self.shape)`, which was already calculated in `Tensor.pad2d` for some reason.</span>
<span class='rem'>A mask is created: `((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))`</span>
<span class='rem'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)` where a new `View` is created with the extended `shape` (`self.shape[0] + self.shape[0] - 1`), `offset` of `-self.shape[0] - 1` and the `mask` as it was created. `contiguous` turns `False` whatever that means.</span>
<span class='rem'></span>
<span class='rem'>To see how mask, offset and maybe contiguous are interpreted, a detour to `Tensor.__getitem__()` follows. Or not, because `__getitem__` only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from `Tensor.tolist()`</span>
<span class='rem'>To return to later: rest of `Tensor.arange`, other Tensor construction methods and random construction methods:</span>
<span class='rem'>- Tensor.manual_seed</span>
<span class='rem'>- Tensor.rand</span>
<span class='rem'>- Tensor.randn</span>
<span class='rem'>- Tensor.randint</span>
<span class='rem'>- Tensor.normal</span>
<span class='rem'>- Tensor.uniform</span>
<span class='rem'>- Tensor.scaled_uniform</span>
<span class='rem'>- Tensor.glorot_uniform</span>
<span class='rem'>- Tensor.kaiming_uniform</span>
<span class='rem'>- Tensor.kaiming_normal</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
</div>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='rem'>Some environment variables are stored in `ContexVar._cache` and as `ContextVar` instances and can be imported from `tinygrad.helpers` but others are determined dynamically through `getenv` which is also imported from `tinygrad.helpers` and used like `getenv("LAZYCACHE", 1)`. Not obvious why this added complexity.</span>
<span class='rem'></span>
<span class='rem'>`tensor.py` too big, methods more around imitating style than being nicely categorized? Remove stuff like `Tensor.ones` or duplication of `Tensor.transpose` and `Tensor.T`</span>
<span class='rem'></span>
<br>
<span class='rem'>`Tensor(1).lazydata.contiguous_child` is a tuple of weakref to some lazybuffer and its own ShapeTracker ??</span>
<br>
<span class='add'>`Tensor(1).lazydata.contiguous_child` is a tuple of weakref to some lazybuffer and its own ShapeTracker?</span>
<br>
<span class='add'>context vars don't add themselves to actual context</span>
<span class='add'>some context vars are acceseed via import from helpers, some through getenv</span>
<span class='rem'>trying the AMD device takes a lot of lines, importing from `renderer/cstyle.py`. can be solved by switching lines in import</span>
<span class='rem'></span>
<span class='rem'>can create a Tensor on a device that does not actually work and will only cause an error when realized (not when realized even, but tolist does not work. where do they fail, how much work is wasted on it?</span>
<span class='rem'></span>
<span class='rem'>if `CUDA`, `ptx_matcher:PatternMatcher` might replace the other pattern matcher that was laboriously created when importing tensor?</span>
<span class='rem'></span>
<span class='rem'>how good is tinygrad introspection? feel need for an inliner to be rooted in base reality.</span>
<span class='rem'></span>
<span class='rem'>context vars set in helpers.py return incorrect value through getenv?</span>
<span class='rem'>try `from tinygrad.helpers import CAPTURING; bool(CAPTURING)`</span>
<br>
<span class='add'>try `from tinygrad.helpers import JIT; bool(JIT)` -> True</span>
<br>
<span class='rem'>and `from tinygrad.helpers import getenv; getenv("CAPTURING")`</span>
<br>
<span class='add'>and `from tinygrad.helpers import getenv; bool(getenv("JIT"))` -> False</span>
<span class='add'></span>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-16-15:35'>2024 07 16 15:35</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-07-14 20:23</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>there are bridges to be built. between the ideas, nature, work, the spirits. Curiosity leads to testing new ways to link them.</span>
<span class='add'>building a product means actualizing the spirits, making the product beautiful requires facing and approaching beauty myself. The nerd-unattractiveness comes from an incomplete person if at all, is not inherent in nerdbeing. requires negotiating between the spirits and curageously producing something complete. at least the most complete I can manage.</span>
<span class='add'>the product, say tinygrad, in greatness, is not a mere tool. it developed the elegance to reflect the truth in itself. to shine with the greens of growth, potential and mysteriousness, the mischivousness of a great troll, the danger and exhiliaration of its varied use, power and darkness. It reflects so stongly, it might just show the way. It should not miss but contain challenging sexiness, doors to transformation, destructive determination, exposition, spontaneousness.</span>
<span class='add'>The tool is dead if I talk to it and its reflection does not answer.</span>
<span class='add'>These properties do not lead to a singular product, they are the consequence of a refined product that reflects in truth, which can be the future of many products, though not all. The stupidity of some might be so near infinite, their abolishion might be the best thing to happen to them. Like projects clinging to past technology, unwilling to die gracefully.</span>
<span class='add'>The awfully draining, painful, torturous creation of something beautiful looks deep into the creator. Any of my tendencies to overplan, to clean obsessively, to autodestruct and turn evil in despair will become concentrated and obvious in the naked product, subject to the open world, inevitably failing due to its inadequacy and stupidity. I should not fear destruction, for I can try to fail gracefully.</span>
<span class='add'></span>
<span class='add'>The optimized organisms in nature reflect its truths. Nature does not leave alone, it knows me. The production of highly optimized, open systems that explore more of it, the continuation and expression of the spirits, is what they ask of me.</span>
<span class='add'></span>
<span class='add'>These are but empty words if they don't become actualized in a product.</span>
<span class='add'></span>
<span class='add'>There isn't anything but the present. The symbols of the past and future are superficial. it does not matter if I become terminally ill, am tortured, amount material wealth, receive social approval. there is only a naked person, the spirits and opportunity.</span>
<span class='add'></span>
<span class='add'>tinygrad should not be exclusively about implementing the latest techniques to become an acceptable deep learning framework.</span>
<span class='add'>Elegance through open selection, trial of characters through risk, competition. exploration of the depths through implementing increasingly complete capacity. If I am not scared of tinygrad, what is even the purpose of dealing with it?</span>
<span class='add'>Integration into silicon is the next frontier.</span>
<span class='add'>Uncover the poetry inherent in computing?</span>
<span class='add'></span>
<span class='add'>I don't know how to translate from these words to action, there are bridges to be built.</span>
<span class='add'></span>
<span class='add'>much that I read in tinygrad is ugly, does not present its role openly, merely produces more objects and variables. It isn't obvious why `LazyOp` even exists, why translating `LazyBuffer` to `ScheduleItem` requires hundreds of lines of code. Why `metadata` needs to creep in everywhere, why `Tensor` has so many weak methods that could be boiled down.</span>
<span class='add'>Could try and go all the way down the `realize` to at least see what the outcome is, then understand the structure above it.</span>
<span class='add'>Find archetypes, describe them. Maybe explain tinygrad, produce a vision in the meanwhile.</span>
</div>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='rem'></span>
<span class='add'>`dict.get(self, key, default=None, /)` Return the value for key if key is in the dictionary, else default.</span>
<span class='add'>format specifiers. `int:8` means the int takes up 8 spaces when printing. same as `int:8d` d for digit. can do alignment with `int:>8d` for right alignment.</span>
</div>
</div>
</div>
<span class='hdg'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='hdg'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='hdg'>Realizing a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Schedule</span>
<div class='indent'>
<span class='rem'>back in `schedule_with_vars`</span>
<span class='add'>back in `tensor.py` -> `schedule_with_vars`</span>
<br>
<span class='add'>-> `schedule.py`</span>
<span class='add'>```python</span>
<span class='add'>def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# Exclude buffers involved in load ops (e.g transfers) to preserve parallelism in graphs.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assigned = _internal_memory_planner(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[si.bufs for si in schedule],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;noopt_buffers=&#123;b for si in schedule if si.ast.op is not MetaOps.SINK for b in si.bufs}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]</span>
<span class='add'>```</span>
<br>
<span class='add'>`_internal_memory_planner` is optional (`NO_MEMORY_PLANNER=1` skips it and it still works).</span>
<span class='add'>actually, it skips all buffers where `buf.lb_refcount > 0` anyway , which applies to all of the current buffers since they all come from the `LazyBuffer` constructor, where they automatically get a reference. `assigned` will be `&#123;}`.</span>
<span class='add'></span>
<span class='add'>`memory_planner` returns the `schedule` exactly as it was.</span>
<span class='add'></span>
</div>
<span class='add'>Run</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for ei in lower_schedule(schedule):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(capturing) and CAPTURING: capturing[0].add(ei)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ei.run(var_vals, do_update_stats=do_update_stats)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;while len(schedule):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;si = schedule.pop(0)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try: yield lower_schedule_item(si)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"error lowering &#123;si.ast.op}")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("tensor operations:")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pprint.pprint(si.metadata, indent=2)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise e</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def lower_schedule_item(si:ScheduleItem) -> ExecItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assert len(set(x.device for x in si.bufs)) == 1 or si.ast.op is MetaOps.COPY or getenv("USE_COPY_KERNEL")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.SINK:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runner = get_runner(si.outputs[0].device, si.ast)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(runner, [si.bufs[x[0]] for x in runner.p.globals], si.metadata)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;out = si.outputs[0]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.COPY:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferCopy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferXfer</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.CUSTOM: return ExecItem(CustomOp(si.ast.arg), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.EMPTY: return ExecItem(EmptyOp(out), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.VIEW: return ExecItem(ViewOp(out), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"don't know how to lower &#123;si.ast}")</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>TODO: `LoadOps` was recently renamed to `MetaOps` and `MetaOps.SINK` was added.</span>
<span class='add'>on the first `ScheduleItem` which copies from `NPY` to `CUDA`</span>
<span class='add'>determines `kernel_type=BufferCopy` which is a class.</span>
<span class='add'>`return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))` initiates `BufferCopy`</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>class BufferCopy(Runner):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, total_sz, dest_device, src_device):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if total_sz >= 1e6: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz/1e6:7.2f}M, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz:8d}, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def copy(self, dest, src):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.allocator.copy_from_disk(dest._buf, src._buf, src.nbytes)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fast(ish) path, uses readinto in diskbuffers</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src.allocator.copyout(dest.allocator.as_buffer(dest._buf), src._buf)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.copyin(src.as_buffer(allow_zero_copy=True))&nbsp;&nbsp;&nbsp;&nbsp;# may allocate a CPU buffer depending on allow_zero_copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest, src = rawbufs[0:2]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, &#123;dest.size} != &#123;src.size}, &#123;dest.dtype} != &#123;src.dtype}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = time.perf_counter()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.copy(dest, src)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if wait:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Device[dest.device].synchronize()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return time.perf_counter() - st</span>
<span class='add'></span>
<span class='add'>class Runner:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate = True, display_name, dname, op_estimate, mem_estimate</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;@property</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def device(self): return Device[self.dname]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def exec(self, rawbufs:List[Buffer], var_vals:Optional[Dict[Variable, int]]=None) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self(rawbufs, &#123;} if var_vals is None else var_vals)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise NotImplementedError("override this")</span>
<span class='add'></span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class ExecItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg: Runner</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: List[Optional[Buffer]]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if do_update_stats:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.kernel_count += 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_ops += (op_estimate:=sym_infer(self.prg.op_estimate, var_vals))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_mem += (mem_estimate:=sym_infer(self.prg.mem_estimate, var_vals))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if et is not None: GlobalCounters.time_sum_s += et</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ptm = (colored(f"&#123;et*1e3:9.2f}ms", "yellow") if et > 0.01 else f"&#123;et*1e6:9.2f}us") if et is not None else ""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"&#123;colored(f'*** &#123;self.prg.dname[:7]:7s} &#123;GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} &#123;self.prg.display_name+' '*(38-ansilen(self.prg.display_name))} arg &#123;len(self.bufs):3d} mem &#123;GlobalCounters.mem_used/1e9:5.2f} GB " +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(str() if et is None else f"tm &#123;ptm}/&#123;GlobalCounters.time_sum_s*1e3:9.2f}ms (&#123;op_estimate/((et or 1e-20)*1e9):8.2f} GFLOPS, &#123;mem_estimate/((et or 1e-20)*1e9):7.2f} GB/s)" +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f" &#123;[repr(m) if DEBUG >= 3 else str(m) for m in self.metadata] if self.metadata else ''}"))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prg.first_run = False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return et</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`name='copy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'`</span>
<span class='add'>`colored` from `helpers.py` for ANSI color coding the string</span>
<span class='add'>after `super(init)` and creating an instance of `ExecItem`, the instance looks like this:</span>
<span class='add'>```python</span>
<span class='add'>ExecItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=BufferCopy(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first_run = True,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate = 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op_estimate = 0</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata=None</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>after constructing `ExecItem` it is yielded to `run_schedule`</span>
<span class='add'>-> `ExecItem.run(var_vals=&#123;}, do_update_stats=True)</span>
<span class='add'>allocates the buffers</span>
<span class='add'>first the cuda buffer, which through some ugly back and forth calls `CUDAAllocator._alloc`</span>
<span class='add'>```python</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def _alloc(self, size, options:BufferOptions):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;check(cuda.cuCtxSetCurrent(self.device.context))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if options.host: return init_c_var(ctypes.c_void_p(), lambda x: check(cuda.cuMemHostAlloc(ctypes.byref(x), size, 0x01)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return init_c_var(cuda.CUdeviceptr(), lambda x: check(cuda.cuMemAlloc_v2(ctypes.byref(x), size)))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`init_c_var` returns the variable after calling the supplied function with the variable.</span>
<span class='add'>using `cuda` library, which escaped me for now.</span>
<span class='add'></span>
<span class='add'>`NPY` buffer already has `buf._buf`, which is a `numpy.ndarray` with `[1,2,3]` in it.</span>
<span class='add'></span>
<span class='add'>`et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)`</span>
<span class='add'>-> `BufferCopy(bufs, &#123;}, False)</span>
<span class='add'>-> `dest.copyin(src.as_buffer(allow_zero_copy=True))` where `src` is `bufs[1]` and `dest` is `bufs[0]`</span>
<span class='add'></span>
<span class='add'>`src.as_buffer` -> `return self.copyout(memoryview(bytearray(self.nbytes)))`</span>
<span class='add'>eventually calls `NPYAllocator.copyout(mv:memoryview, self._buf:np.ndarray)`</span>
<span class='add'>which mostly does numpy stuff to ensure "C-contiguous array", returns a new memoryview to the new array.</span>
<span class='add'></span>
<span class='add'>`dest.copyin` produces `host_mem` on the device through cuda library and more weird cuda things.</span>
<span class='add'></span>
<span class='add'></span>
</div>
</div>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>context vars set in helpers.py return incorrect value through getenv?</span>
<span class='add'>try `from tinygrad.helpers import CAPTURING; bool(CAPTURING)`</span>
<span class='add'>and `from tinygrad.helpers import getenv; getenv("CAPTURING")`</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-14-11:53'>2024 07 14 11:53</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'></span>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `(Tensor([1,2,3]) + 2).tolist()`</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `(Tensor([1,2,3]) + 2).tolist()</span>
<span class='rem'>- visualize what parts of the script do</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- diff for each step</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- divide by file the lines come from</span>
<br>
<span class='add'>python inliner for tinygrad?</span>
<span class='add'></span>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>tinygrad inliner</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>from reading its not obvious what happens</span>
<span class='add'>could miss parts</span>
<span class='add'>can't keep track of values</span>
<span class='add'>reading too much that is irrelevant</span>
<span class='add'></span>
<span class='add'>inlined code would be a practical story through a structure of relationships</span>
<span class='add'>tinygrad code lays out the structure directly</span>
<span class='add'></span>
<span class='add'>- write an executable python file that fulfills the same function as the traced script without calling functions</span>
<span class='add'>- classes are maintained (Tensor, LazyBuffer(?))</span>
<span class='add'>- function variables are renamed to be unique</span>
<span class='add'>- function calls = indented comment = function name, source file, line number</span>
<span class='add'>- return = indented comment</span>
<span class='add'>- for loops and comprehensions are only shown once</span>
</div>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='rem'></span>
<span class='add'>`deque` from `collections` = data structure for efficient insertion and deletion from two ends of a list.</span>
</div>
<span class='hdg'>Creating a Tensor</span>
<div class='indent'>
<span class='rem'>9656 lines, the cyan line marks the border between previous import code and new tensor construction code. most new code comes from `runtime/autogen/cuda.py`(magenta left border) because in this case, cuda is the device it finds for the Tensor.</span>
<span class='add'>9656 lines (the linearizer-lowerer commit ([#4957](https://github.com/tinygrad/tinygrad/commit/6972a2569f5a848b101f4c9310d5de373328dbfb)) changed this, documentation is paused as this might be cleaned up), the cyan line marks the border between previous import code and new tensor construction code. most new code comes from `runtime/autogen/cuda.py`(magenta left border) because in this case, cuda is the device it finds for the Tensor.</span>
<br>
<span class='rem'>in `create_lazybuffer` the `lazycache` is interacted with, which stores lazybuffers. a `cache_key` is generated from the lazybuffers parameters. If the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor:</span>
<span class='add'>in `create_lazybuffer` the `lazycache` is interacted with, which stores lazybuffers. a `cache_key` is generated from the lazybuffers parameters. If the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor, where it will pass `metadata=_METADATA.get()` as `metadata` ([#5271](https://github.com/tinygrad/tinygrad/commit/9150a6be7a30bbd17f0b84f3352fac7af0c68b73)):</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata:Optional[Metadata]=None</span>
</div>
</div>
</div>
<span class='hdg'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,), </span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,), </span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='hdg'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='hdg'>Adding to a Tensor</span>
<div class='indent'>
<span class='rem'>The `ShapeTracker` will be empty, because the provided shape is `tuple()`. (its a 0D Tensor - one number)</span>
<br>
<span class='add'>The `ShapeTracker` will be empty, because the provided shape is `tuple()`. (its a 0D Tensor)</span>
<br>
<span class='rem'>the returned `Tensor` looks like this:</span>
<span class='add'>the returned `Tensor.lazydata`:</span>
<br>
<span class='rem'>&#123;</span>
<span class='add'>Tensor.lazydata &#123;</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<br>
<span class='rem'>Tensor after reshape:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (1,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = LazyBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;LoadOps.CONST: 2></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = 2</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child: None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='rem'>```</span>
<span class='rem'></span>
<br>
<span class='rem'>cause `contiguous` to be `False` because the unchaged stride is `(0,)`, but the the appropriate stride for the new shape would be `(1,)`</span>
<br>
<span class='add'>cause `contiguous` to be `False` because the unchaged stride is `(0,)`, but the appropriate stride for the new shape would be `(1,)`</span>
<span class='rem'>Notably, `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)` takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer.</span>
<span class='add'>Notably, `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)` takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer. So in the final Tensor, there remains no reference to the reshape lazybuffer:</span>
<br>
<span class='rem'>So the finally returned Tensor is:</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad' : None</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata':</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st' : ShapeTracker(views=(View(</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(3,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(0,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous':False</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (3,)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 3</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = LazyBuffer:</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': LazyBuffer:</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous'=True</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = ()</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': ()</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 1</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;LoadOps.CONST: 2></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = 2</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = ()</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': ()</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child: None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'></span>
<span class='rem'>While the expand used the "reshape lazybuffer", there remains no reference to that lazybuffer in the final Tensor.</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>, # whole lazybuffer, not writing it out here</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>, # previously created lazybuffer [1,2,3] copied from NPY </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))> # whole lazybuffer, not writing it out here</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))> # new lazybuffer from 2</span>
<br>
<span class='rem'>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through `srcs` (in ADD) or `_base` (in shape changes) and so form a graph.</span>
<span class='add'>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through `srcs` (in ADD in this case) or `_base` (in shape changes) and so form a graph.</span>
</div>
<span class='hdg'>Realizing a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>`Tensor.cast(self.dtype.scalar())` does nothing because `self.dtype == self.dtype.scalar()` in this case.</span>
<span class='add'>`Tensor.contiguous()` -> `lazydata.base.forced_realize = True`, otherwise nothing in this case, because not needed.</span>
<span class='rem'>`Tensor.cast(self.dtype.scalar())` applies `F.Cast(dtype)`</span>
<span class='rem'>`Tensor.contiguous()` applies `F.Contiguous()`</span>
<span class='rem'> - `LazyBuffer.contigous()`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.e(LoadOps.CONTIGUOUS)` in the current case</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - makes sure dtypes and shapes(?) of all lazybuffers and their bases match</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - "const folding"(?), which in the current case does nothing</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - returns a new `LazyBuffer` with all sources (self and bases, in this case only self) in the `srcs` attribute</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- stores a reference and something in self.base.contiguous_child (?)</span>
<span class='rem'>`Tensor.to("CLANG")`</span>
<span class='rem'>- if it is not already on CLANG, it makes a new Tensor with the same lazydata, but `device="CLANG"`, so it makes a copy.</span>
<br>
<span class='add'>`Tensor.to("CLANG")`. if it is not already on CLANG, it makes a new Tensor with the same lazydata, but `device="CLANG"`, so it add a `LoadOps.COPY` Lazybuffer to the graph.</span>
<span class='rem'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# left out some lines that aren't executed</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;</span>
<br>
<span class='rem'>strange that it uses `out.base` it means if the latest lazybuffer were a reshape, it would be ignored for now.</span>
<br>
<span class='add'>strange that it uses `out.base` it means if the latest lazybuffer was already on clang and a reshape, it would be ignored for now.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op in ReduceOps and buf.base.srcs[0].base.op is LoadOps.CONST:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize reduceops on const (unless base is forced_realize)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;children[x.base][buf] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if x.base.realized is None: children[x.base][buf] = None</span>
<span class='add'></span>
<span class='add'>def _is_padding_okay(buf:LazyBuffer, realizes:Dict[LazyBuffer, None]) -> bool:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in realizes or buf.realized is not None: return True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this broke to_image_idx and coder with JIT</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in UNSAFE_PAD_OPS: return False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return all(_is_padding_okay(x.base, realizes) for x in buf.srcs)</span>
<br>
<span class='rem'>puts lazybuffers in `allbuffs` dictionary</span>
<span class='rem'>and loadops into `realizes`</span>
<span class='add'></span>
<span class='add'>`realizes` = lbs with `self.forced_realize` or that are `LoadOps` or source of `LoadOps.COPY` and base of view lbs if the lb was expanded compared to its base, unless exceptions.</span>
<span class='add'>```python</span>
<span class='add'>realizes = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None # source of copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)> = None # src of copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None # base of view lb</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`allbufs` = base lbs (no view lazybuffers).</span>
<span class='add'>the NPY LoadOps.EMPTY lazybuffer isn't included because for it `self.realized` returns true which returns from `_recurse_lb` before it could be added to `allbufs`.</span>
<span class='add'>```python</span>
<span class='add'>allbufs = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`simple_pads` = lb base if there is a mask&nbsp;&nbsp;&nbsp;&nbsp;= `&#123;}`</span>
<span class='add'></span>
<span class='add'>`children` = unrealized lbs in `srcs`.</span>
<span class='add'>```python</span>
<span class='add'>children = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# preschedule all buffers in realizes</span>
<br>
<span class='add'> # preschedule all buffers in realizes</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prescheduled = &#123;group[0]:_schedule_group(tuple(group), realizes, reduce_for_op) for group in output_groups.values()}</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prescheduled = &#123;group[0]:(group, *_lower_lazybuffer(group, realizes, reduce_for_op)) for group in output_groups.values()}</span>
<br>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>(current value): </span>
<span class='add'>```python</span>
<span class='add'>output_groups = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [same buffer]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None], reduce_for_op:Dict[LazyBuffer, LazyBuffer]):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""describe the computation for a LazyBuffer with LazyOp + inputs + var_vals"""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if (out:=outs[0]).op is LoadOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rd = LazyOp(BufferOps.LOAD, (), MemBuffer(1, dtypes.uint8, st:=ShapeTracker.from_shape((out.arg,))))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return (LazyOp(BufferOps.STORE, (rd,), MemBuffer(0, dtypes.uint8, st)), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if out.op in &#123;LoadOps.CUSTOM, LoadOps.COPY, LoadOps.EMPTY, LoadOps.VIEW}: return (LazyOp(out.op, (), out.arg), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = merge_dicts([out.st.var_vals.copy() for out in outs])</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in outs if x.op is LoadOps.ASSIGN}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;cache: Dict[Tuple[LazyBuffer, ShapeTracker], LazyOp] = &#123;}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ast: List[LazyOp] = []</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer] = []</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for i, out in enumerate(outs):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_st = ShapeTracker.from_shape(reduce_for_op[out].shape if out in reduce_for_op else out.shape)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view = out.arg[0] if out.op is LoadOps.ASSIGN and out.arg else output_st</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lop = _recursive_lazyop(out, inputs, tuple(outs), var_vals, output_st, realizes, assign_targets, cache=cache)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view, vv = output_view.simplify().unbind()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if vv: var_vals.update(vv)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return tuple(ast), inputs, var_vals, dedup([x[0].metadata for x in cache if x[0].metadata and x[0] not in inputs])</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>@dataclass(frozen=True, eq=False)</span>
<span class='add'>class LazyOp:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;op: Op</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;src: Tuple[LazyOp, ...] = ()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None</span>
<span class='add'></span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class MemBuffer:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;idx: int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<span class='add'></span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class ConstBuffer:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;val: ConstType | Variable</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`_lower_lazybuffer` returns:</span>
<span class='add'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` for `CLANG` copy </span>
<span class='add'>- enters `_recursive_lazyop` when processing `output_groups[1]`</span>
<span class='add'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` again for the `CUDA` copy </span>
<span class='add'></span>
<span class='add'>`_recursive_lazyop` returns `LazyOp` for the two copy lbs and the add lb in `output_groups`.</span>
<span class='add'>in the add lb it recurses trough its sources:</span>
<span class='add'>- `&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- simplify and unbind shapetracker (simplify does nothing here because the shapetracker has only one view. Unbind seems to act on variables, of which there aren't any here).</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- append the lb to `inputs`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.LOAD, (), MemBuffer(len(outputs)+inputs.index(buf), buf.dtype, unbound_st)))</span>
<span class='add'>- `&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>)`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- switch to its base `&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.CONST, (), ConstBuffer(val, buf.dtype, unbound_st))` where `val` is `arg`, which is 2.</span>
<span class='add'>`ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))'</span>
<span class='add'>`return tuple(ast), inputs, var_vals,` + metadata stuff, ignored for now. `var_vals` is `&#123;}` because nothing symbolic in this case.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>"""</span>
<span class='add'>prescheduled:List[Tuple[]] &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;group[0]:LazyBuffer (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: List[LazyBuffer],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;abstract syntax tree (ast): Tuple[LazyOp],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;variable values: Dict[Variable, int],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata: List[?]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>}</span>
<span class='add'>"""</span>
<span class='add'>prescheduled = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[__add__ - __main__:3::&lt;module>]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>back in `_graph_schedule`</span>
<span class='add'>```python</span>
<br>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`lsi` = LazyScheduleItem?</span>
<span class='add'>`schedule_targets` makes an entry for every item in every group and assigns it the tuple in `prescheduled` that it is part of.</span>
<span class='add'></span>
<span class='add'>`scheduled_parents = set(schedule_targets[x][0][0] for x in lsi[2] if x in schedule_targets)`</span>
<span class='add'>`lsi[2]` is inputs, so if an input is one of the entries in a lazybuffer group, add the tuple with its info.</span>
<span class='add'>this returns an empty set for the third group, because its input (the `NPY` lazybuffer) is not in any group (= not in `output_groups` because it is already realized)</span>
<span class='add'></span>
<span class='add'>the input group's `group[0]` as a key in `graph` and append the current prescheduled `key`</span>
<span class='add'>some detailed explanation: The `ADD` lb is the first key in `graph` because it is the input of the first group (where the key is the `COPY` lb) in `prescheduled` that is also part of group itself. The value it gets assigned is the first item in the group that it was an input of, so, the `COPY` lb.</span>
<span class='add'>this way, graph "points" from the inputs to the groups that depend on them.</span>
<span class='add'>every time an input of a group is added to graph this way, the groups key in the `in_degree` dictionary increases by 1.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>graph = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>]</span>
<span class='add'>}</span>
<span class='add'></span>
<span class='add'>in_degree = &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: 1,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: 1,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: 0</span>
<span class='add'>}</span>
<br>
<span class='add'>```python</span>
<span class='add'>queue = deque([</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>])</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>adds any buffers of the group to `seen`.</span>
<span class='add'>deletes `srcs` of lazybuffers in the group</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class ScheduleItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ast: Tuple[LazyOp, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: Tuple[Buffer, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>schedule.append(si:=ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ps[1],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;tuple(x.buffer for x in ps[0]+ps[2] if x.size != 0),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ps[4]</span>
<span class='add'>))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>then finds the lb it just made a `ScheduleItem` from in `graph`, which returns the `group[0]` item of the groups that depend on the just processed one.</span>
<span class='add'>Add the group tuple from `prescheduled` to `queue`.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>schedule = [</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0, </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__ - __main__:3::&lt;module>]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>]</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>with `GRAPH=1`, tinygrad produces output that reflects this schedule:</span>
<span class='add'>![200](attachments/net_1.svg)</span>
<span class='add'></span>
<span class='add'>back in `schedule_with_vars`</span>
<span class='add'>```python</span>
<span class='add'>return memory_planner(schedule), var_vals</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>then back in `Tensor.realize`</span>
<span class='add'>```python</span>
<span class='add'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>how good is tinygrad introspection? feel need for an inliner to be rooted in base reality.</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-10-10:47'>2024 07 10 10:47</span><div class='indent'>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Importing Tensor</span>
<div class='indent'>
<span class='add'>- tinygrads ops are defined:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `UnaryOps(Enum)`: `EXP2`, `LOG2`, `CAST`, `BITCAST`, `SIN`, `SQRT`, `NEG`, `RECIP`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `BinaryOps(Enum)`: `ADD`, `MUL`, `IDIV`, `MAX`, `MOD`, `CMPLT`, `CMPNE`, `XOR`, `SHL`, `SHR`, `OR`, `AND`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `TernaryOps(Enum)`: `WHERE`, `MULACC`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `ReduceOps(Enum)`: `SUM`, `MAX`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `BufferOps(Enum)`: `LOAD`, `CONST`, `STORE`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `LoadOps(Enum)`: `EMPTY`, `CONST`, `COPY`, `CONTIGUOUS`, `CUSTOM`, `ASSIGN`, `VIEW`</span>
</div>
<span class='hdg'>Creating a Tensor</span>
<div class='indent'>
<span class='rem'>from tinygrad.tensor import Tensor</span>
<span class='rem'>t = Tensor([1,2,3])</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`from tinygrad.tensor import Tensor` triggers creation of the `Device` singleton, as `tensor.py` imports its, which is useful when creating Tensors.</span>
<span class='rem'>`Device._devices` stores uppercase strings for devices available in tinygrad as determined by collecting all `tinygrad/runtime/ops_&#123;device}.py` files.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<br>
<span class='rem'>*Tensor creation from [tinygrad docs](https://docs.tinygrad.org/tensor/):</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>from tinygrad.tensor import Tensor</span>
<span class='add'>Tensor([1,2,3])</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>![](attachments/tinygrad_construct_tensor.png)</span>
<span class='add'>9656 lines, the cyan line marks the border between previous import code and new tensor construction code. most new code comes from `runtime/autogen/cuda.py`(magenta left border) because in this case, cuda is the device it finds for the Tensor.</span>
<br>
<span class='rem'>- if it finds none `&#123;device}Device.__init__(&#123;device})` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM` in their respective `runtime/ops_&#123;device}.py`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- which eventually returns a `Compiled` device, which is cached for later use, but here it is only used to check if the device works and no more.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- if a device causes no problems, `Device.DEFAULT` returns its string and sets it to 1 as an environment variable</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp;&nbsp;if `DEBUG` > 1, a message is printed informing which device was initialized. `DEBUG` is a `ContextVar` defined in `helpers.py`. There are a few such variables and are initalized when importing from `helpers.py`. They store environment variables, are are shorthand. But not all environment variables relevant to tinygrad are initialized. Which makes this look useless.</span>
<span class='rem'></span>
<span class='rem'>depending on type of data, some local helper functions `_loadop()`, `_fromnp` or `_frompy`.</span>
<span class='rem'>The example Tensor construction above triggers this handling:</span>
<span class='add'>- `Device[&#123;device}]` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM`, -> `Device.__get_canonicalized_item` -> eventually tries `&#123;device}Device.__init__(&#123;device})` (like `CUDADevice`) in their respective `runtime/ops_&#123;device}.py` until it finds one that returns no errors.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `METAL` fails within 3 lines when it tries to import the `Metal` library.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `AMD` imports the `AMDRenderer` from `renderer/cstyle.py` (runs ~300 lines of importing and classvariable definitions), then imports from `runtime/driver/hip_comgr.py` which tries `runtime/autogen/comgr.py` and fails within 15 lines.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `CUDA` should fail within ~30 lines when it tries to get `libcuda.so` but in this case cuda is installed, so it imports from `runtime/ops_cuda.py`, `runtime/autogen/cuda.py` (4000+ lines of mysterious code) and `runtime/autogen/nvrtc.py`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.cstyle import CUDARenderer` which is already available from the AMD attempt earlier.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.assembly import PTXRenderer`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTXRenderer` has lots of class variables:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `device="CUDA"`, `suffix="PTX"`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `global_max = (2147483647, 65535, 65535)`, `local_max = (1024, 1024, 64)`, `shared_max = 49152`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `tensor_cores: List[TensorCore]`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `kernel_prefix`, `barrier`, `gid`, `gdim`, `lid`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `asm_for_op:Dict[Op, Callable]` by all appearances functions for op->assembly translation</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `supports_half: List[Op]` with a small selection of ops</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `types: Dict[DType, str]` and `men_types: Dict[DType, str]` (almost identical, except for 3 types(?)) to translate between tinygrad dtypes and apparently some other convention</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `const_requires_mov: List[DType] = [dtypes.half, dtypes.bool]`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `ptx_matcher` is another `PatternMatcher`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTX = getenv("PTX")`, 0 if not given.</span>
<span class='add'>`CUDADevice.__init__` gets itself `device_id`, `cu_device`, `context`, `arch`, `pending_copyin`, checking that the interactions with cuda (`libcuda.so`) return no errors on multiple occasions.</span>
<span class='add'>`CUDADevice.devices.append(self)`</span>
<span class='add'>9406: `from tinygrad.runtime.graph.cuda import CUDAGraph`</span>
<span class='add'>calls</span>
<br>
<span class='rem'>elif isinstance(data, (list, tuple)):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if dtype is None:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata</span>
<span class='add'>Compiled.__init__(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;CUDAAllocator,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;PTXRenderer(self.arch) if PTX else CUDARenderer(self.arch)`, # PTX=0 (default)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;PTXCompiler(self.arch) if PTX else CUDACompiler(self.arch),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;functools.partial(CUDAProgram, self),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;graph=CUDAGraph</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'>which is the superclass of `CUDADevice`, where `dname`(device name), `allocator`, `renderer`, `compiler`, `runtime`, `graph`&nbsp;&nbsp;&nbsp;&nbsp;come together and are stored in `self` (ultimately in `CUDADevice` as it inherits these instance variables from its parent classes.</span>
<span class='add'>- `CUDAAllocator` inherits from `LRUAllocator`, calls `super().__init__()` which only runs `self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)`(sidenote: `LRUAllocator` itself also inherits from `Allocator`).</span>
<span class='add'>- `CUDARenderer` initialization in this case stores `[]` in `self.tensor_cores`</span>
<span class='add'>- `CUDACompiler` (child of `Compiler`) gets itself `self.arch`, `self.compile_options` and `super().__init__(f"compile_cuda_&#123;self.arch}")` which sets `self.cachekey` unless explicitly preventes through env variable `DISABLE_COMPILER_CACHE`</span>
<span class='add'>- `CUDAGraph`, notably is not initialized, the imported class is just passed on.</span>
<span class='add'></span>
<span class='add'>in `Compiler.__init__()` if `compiler` was `None` it would be replaced by the generic `Compiler()` and `renderer` by `Renderer()`.</span>
<span class='add'></span>
<span class='add'>`CudaDevice` returned to `Device.__get_canonicalized_item` and cached (`@functools.lru_cache(maxsize=None)` decorator):</span>
<span class='add'>```python</span>
<span class='add'>CUDADevice &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'cu_device': c_int(0),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'context': &lt;tinygrad.runtime.autogen.cuda.LP_struct_CUctx_st at 0x7f7a12c49a40>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arch': 'sm_61',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'pending_copyin': [],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dname': 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'allocator': &lt;tinygrad.runtime.ops_cuda.CUDAAllocator at 0x7f7a12dad9f0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'compiler': &lt;tinygrad.runtime.ops_cuda.CUDACompiler at 0x7f7a12cd7b20>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'runtime': functools.partial(&lt;class 'tinygrad.runtime.ops_cuda.CUDAProgram'>, &lt;tinygrad.runtime.ops_cuda.CUDADevice object at 0x7f7a12daeb60>),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'graph': tinygrad.runtime.graph.cuda.CUDAGraph,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'renderer': &lt;tinygrad.renderer.cstyle.CUDARenderer at 0x7f7a12c244f0></span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'>also this `CUDADevice` is stored in classvariable `CUDADevice.devices:List[CUDADevice]`</span>
<span class='add'>if `DEBUG>=1`, a message will inform that the device was opened.</span>
<span class='add'></span>
<span class='add'>for now, the returned `CUDADevice` only demonstrates that `CUDA` can be used as a device for the new Tensor. environmentvariable `CUDA` is set to `1` to save this work in the future.</span>
<span class='add'></span>
<span class='add'>In Tensor construction, depending on type of data input, `_loadop()`, `_fromnp` or `_frompy` create the tensors `LazyBuffer`.</span>
<span class='add'>The example Tensor construction determines dtype (`dtypes.default_int`), then</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))</span>
<br>
<span class='add'>`data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))`</span>
<br>
<span class='add'>(numpy as a dependency is phased out, so this probably changes soon)</span>
<span class='add'>`_from_np_dtype` uses a dictionary from `dtype.py` to translate the numpy dtype to a tinygrad `DType`</span>
<span class='add'>-> `LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")`</span>
<span class='rem'>```</span>
<span class='rem'>which infers the dtype and then uses numpy to create and cast an array. finally calling `_fromnp`: (numpy as a dependency is phased out, so this probably changes soon)</span>
<span class='rem'>- a `LazyBuffer` is created using `LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")` where `x.shape` is numpys function to return array shape.</span>
<span class='rem'>- `_from_np_dtype` looks up the numpy dtype name in a dictionary from `dtype.py` to get a tinygrad `DType`</span>
<br>
<span class='add'>@staticmethod</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def loadop(op, shape:Tuple[sint,...], dtype:DType, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert isinstance(src, tuple)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)</span>
<span class='rem'>def loadop(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;shape: Tuple[sint,...],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;src: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;enable_cache = False</span>
<span class='rem'>): ...</span>
<br>
<span class='add'>`op` was given as `LoadOps.EMPTY`</span>
<span class='rem'></span>
<span class='rem'>`LazyBuffer.loadop` otherwise does one errorcheck on the `srcs` argument (not supplied here) and produces a `ShapeTracker` through `ShapeTracker.from_shape(shape)` before passing arguments on to a helper function `create_lazybuffer` (further below) which also receives the argument `enable_cache` (`False` by default - if it were `True`, the lazybuffer would be stored in `lazycache`after creation).</span>
<span class='rem'></span>
<span class='rem'>`op` was given as `LoadOps.EMPTY`, which ist just a number in&nbsp;&nbsp;&nbsp;&nbsp;`class LoadOps(Enum)`, 0 in this case.</span>
<span class='rem'></span>
<br>
<span class='rem'>`ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments</span>
<br>
<span class='add'>`ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class View:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;shape: Tuple[sint, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;strides: Tuple[sint, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;offset: sint</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;mask: Optional[Tuple[Tuple[sint, sint], ...]]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;contiguous: bool</span>
<span class='add'>```</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;views: Tuple[View, ...]</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;views: Tuple[View, ...]</span>
<br>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class View:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;shape:Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;strides:Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;offset:sint</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;mask:Optional[Tuple[Tuple[sint, sint], ...]]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;contiguous:bool</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>in `create_lazybuffer` the `lazycache` is interacted with, a `WeakValueDictionary` storing lazybuffers. a `cache_key` is generated from the lazybuffers parameters and if the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor:</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>LazyBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'>*from [tinygrad docs](https://docs.tinygrad.org/developer/)*</span>
<span class='rem'></span>
<span class='rem'>notably `device` here given to be `"NPY"`, which comes from how the Tensor was initialized. This is different from the device determined at the beginning through `Device.DEFAULT`. Reason for this may become clearer?</span>
<span class='rem'>`st` is the `ShapeTracker` just created</span>
<span class='rem'></span>
<span class='rem'>In the lazybuffer's initialization, it finds that `base` is `None` and decides that an assignment to `self.buffer` is in order.</span>
<span class='rem'>Given the op `LoadOps.EMPTY`, it makes a `Buffer` (a class imported from `tinygrad.device`) through `Buffer(device, self.size, dtype)`. But creating it like that does nothing except store the instance.</span>
<span class='rem'>the buffers `_lb_refcount` property is incremented by 1</span>
<span class='rem'>the `contiguous_child` property (didn't exist before) is set to `None`</span>
<span class='rem'>and `forced_realize` to `False`</span>
<span class='rem'>the meaning of all 3 escapes me right now.</span>
<span class='rem'></span>
<span class='rem'>The `LazyBuffer` is done and returning to `_fromnp()` into the variable `ret` where:</span>
<span class='rem'>`ret.buffer.allocate(x)` (x is a numpy array) causes the buffer to find itself an `Allocator`:</span>
<span class='rem'>`self.allocator = Device[self.device].allocator`. Indexing into `Device` returns a `Compiled` Device (same as earlier when it was about finding an available device, but this time with "NPY")</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>Compiled (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allocator: Allocator,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;renderer: Optional[Renderer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;compiler: Optional[Compiler],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;runtime,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph = None</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>on `buffer.allocate(x)` where `x` is the np.ndarray, which is just assigned to `buffer._buf`.</span>
<span class='rem'>`del ret.srcs` (which is cruicial for `LazyBuffer.realized` to return `True`) completes what is commented "fake realize".</span>
<span class='rem'></span>
<span class='rem'>In the final step of `Tensor` initialization, the mismatching devices, one being the discovered one and one being "NPY" are detected and `self.lazydata = data.copy_to_device(device)` takes care of it, `data` being the created `LazyBuffer` and `device` being the discovered device from the start.</span>
<span class='rem'>`LazyBuffer.copy_to_device(device)` in this case leads to `self.base._copy(device)._view(self.st)`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
</div>
</div>
</div>
<span class='rem'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='rem'>return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, LoadOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)</span>
<br>
<span class='add'>in `create_lazybuffer` the `lazycache` is interacted with, which stores lazybuffers. a `cache_key` is generated from the lazybuffers parameters. If the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor:</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>LazyBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`st` is the `ShapeTracker` just created</span>
<span class='add'></span>
<span class='add'>In the lazybuffer's initialization, it finds that `base` is `None` and decides that an assignment to `self.buffer` is in order.</span>
<span class='add'>Given the op `LoadOps.EMPTY`, it makes a `Buffer` (a class imported from `tinygrad.device`) through `Buffer(device, self.size, dtype)`. But creating it like that in this case does nothing except store the instance.</span>
<span class='add'>the buffer's `_lb_refcount` property is incremented by 1</span>
<span class='add'>the `contiguous_child` property (didn't exist before) is set to `None`</span>
<span class='add'>and `forced_realize` to `False`</span>
<span class='add'>the meaning of all 3 escapes me right now.</span>
<span class='add'></span>
<span class='add'>The `LazyBuffer` is done and returning to `_fromnp()` into the variable `ret` where:</span>
<span class='add'>`ret.buffer.allocate(x)` (x is a numpy array) causes the buffer to find itself an `Allocator`:</span>
<span class='add'>`self.allocator = Device[self.device].allocator`. Indexing into `Device` returns a `NpyDevice` (same as earlier when it was about finding an available device, but this time with `NPY`. This device is very minimal, has the default `Compiler` and `Renderer` and a mostly empty `NpyAllocator`)</span>
<span class='add'></span>
<span class='add'>on `buffer.allocate(x)` where `x` is the `np.ndarray`, `x` is just assigned to `buffer._buf`, without calling `Buffer.alloc` which is not implemented for this device.</span>
<span class='add'>completing what is commented "fake realize" in `_fromnpy`, `del ret.srcs` (which was `()`) makes sure that `LazyBuffer.realized` will return `True`.</span>
<span class='add'>Also adds the buffer's size to `GlobalCounters.mem_used`</span>
<span class='add'></span>
<span class='add'>In the final step of `Tensor` initialization, the mismatching devices, one being the discovered one (`CUDA` in this case) and one being `NPY` are detected and `self.lazydata = data.copy_to_device(device)` takes care of it, `data` being the created `LazyBuffer` and `device` being the discovered device from the start.</span>
<span class='add'>`LazyBuffer.copy_to_device(device)` in this case leads to `self.base._copy(device)._view(self.st)`</span>
<span class='add'></span>
<span class='add'>```python</span>
</div>
<span class='add'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='add'>return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, LoadOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>assign a&nbsp;&nbsp;&nbsp;&nbsp;`Buffer` to the `LazyBuffer`, because `base` is `None` again (the npy lazybuffer is stored in `srcs`).</span>
<span class='add'></span>
<span class='rem'>`._view(self.st)` does nothing here, because the new shapetracker has the same shape and is contiguous.</span>
<span class='add'>the `._view(self.st)` that follows `._copy(device)`, does nothing here, because the new shapetracker has the same shape and is contiguous.</span>
<br>
<span class='rem'>TODO: Not true, Tensor has more attributes than lazydata!</span>
<br>
<span class='rem'>Tensor.lazydata &#123;</span>
<span class='add'>Tensor &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata': &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,), </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.COPY: 3>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': LazyBuffer &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'NPY',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,), </span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.COPY: 3>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': LazyBuffer &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'NPY',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.EMPTY: 1>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.EMPTY: 1>,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'></span>
<span class='add'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>trying the AMD device takes a lot of lines, importing from `renderer/cstyle.py`. can be solved by switching lines in import</span>
<span class='add'></span>
<span class='add'>can create a Tensor on a device that does not actually work and will only cause an error when realized (not when realized even, but tolist does not work. where do they fail, how much work is wasted on it?</span>
<span class='add'></span>
<span class='add'>if `CUDA`, `ptx_matcher:PatternMatcher` might replace the other pattern matcher that was laboriously created when importing tensor?</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-09-08:25'>2024 07 09 08:25</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='add'>Space to develop ideas until they have grown up to earn their own space. Mortality is high though.</span>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'>preliminary expression. I wonder if cyberspace exist, what it looks like. Maybe the dissolution of the body goes unnoticed. Maybe its dissolution is forgetting the real dimensions, as attention shifts to new worlds.</span>
<span class='add'>cyberspace</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Reaching cyberspace is an interface problem. If the computer would respond to my will more closely, my mind would enter it more completely. Sometimes I fear that it is mostly here already and it just turned out underwhelming.</span>
<span class='add'>Invention of telephones is followed by people who do nothing but phonecalls. Sucks. Tech can merge with nature if it allows to contain it: Make the telephone usable, automatable by anyone.</span>
<span class='add'></span>
<span class='add'>preliminary expression. I wonder if cyberspace exist, what it looks like.</span>
<br>
<span class='rem'>May the spirits remain informed by mortality and wander far.</span>
<br>
<span class='add'>May the spirits remain informed by mortality and journey far.</span>
</div>
<span class='hdg'>2024-05-15 08:28 Proto spirit stream</span>
<div class='indent'>
<span class='rem'>thumbnails of proximate spheres that act as buttons to the projects make them more distinct.</span>
<br>
<span class='add'>thumbnails of proximate spheres that act as buttons to the projects and make them more recognizable.</span>
<br>
<span class='rem'>example for connections: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me: they all give a more direct image of my mind.</span>
<span class='rem'></span>
<br>
<span class='add'>The dynamically generated content was undiscovered by crawlers, required javascript and was inefficient to use and to add to.</span>
<span class='add'></span>
</div>
<span class='add'>decentralization</span>
<div class='indent'>
<span class='rem'>- No search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<span class='rem'>- requires javascript</span>
<span class='rem'>- inefficient to use, relies on curiosity of visitor, needs to turn the blob to discover everything</span>
<span class='rem'>- information structure too weak to support large number of projects at various levels of resolution (density, thought hours) and versions</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- low benefit from connections at high cost of visual complexity. imagine dozens of project bubbles</span>
<span class='hdg'>2024-05-11 08:15 [Vitalik Buterin - Coordination, Good and bad](https://vitalik.eth.limo/general/2020/09/11/coordination.html)</span>
<div class='indent'>
<span class='rem'>> In _[Liars and Outliers](https://books.google.com.sg/books/about/Liars_and_Outliers.html?id=lPsbhIUexo0C&amp;redir_esc=y)_, Bruce Schneier reminds us that many "security systems" (locks on doors, warning signs reminding people of punishments...) also serve a moral function, reminding potential misbehavers that they are about to conduct a serious transgression and if they want to be a good person they should not do that.</span>
<span class='rem'></span>
<span class='rem'>(decentralization contains a warning that any attempt of centralization is unwelcome? However, consider the Ministry of Love)</span>
<span class='rem'></span>
<br>
<span class='add'>- reward to whistleblowers</span>
<span class='rem'>- reward to whistleblowers, encouraging defection in the collusion</span>
<span class='rem'>- "Moral barriers" (?)</span>
<br>
<span class='rem'>(much of this thought seems unnecessary to me. The structure of reality will reveal itself when the tools allow it. If collusion is the optimal strategy, then so be it. but collusion should not succeed because an alternative was discouraged not technically possible. The job of a social system is to allow anything and everything, then have people build structure on top as they wish: filters, undoing privacy, subspaces. The best system is no system, the best part is no part. Reduce complexity and let people build. if the structure can contain reality it will, and then everyone gets what they deserve which effectively maximizes progress)</span>
<span class='add'>(much of this thought seems unnecessary to me. The structure of reality will reveal itself when the tools allow it. If collusion is the optimal strategy, then so be it. Not sure what the purpose of a social system is. If anything were allowed and people build structure on it like filters, undoing privacy, subspaces, would it not be desirable? The best system is no system? If the structure can contain reality it will, and reality, as death, will be the appropriate judge?</span>
<span class='add'></span>
</div>
<span class='hdg'>2024-05-10 12:47 [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>>[...], I realized that I do not even need to argue any of this. Regardless of whether our lives as a whole are finite or infinite, every single beautiful thing in our lives is finite.</span>
<span class='rem'></span>
<span class='rem'>(= more lifetime is almost only nice)</span>
<span class='rem'></span>
<br>
<span class='rem'>>[...]the most important variables that make the difference between existing flawed systems succeeding or failing in practice (often, the degree of coordination between subgroups of participants, but also other things that we often black-box as ["culture"](https://www.writingruxandrabio.com/p/ideas-matter-how-i-stopped-being)</span>
<span class='rem'></span>
<span class='rem'>(success requires vision, which is embedded in culture)</span>
<span class='rem'></span>
<span class='rem'>>These two events, as different as they are in the type and the scale of their tragedy, both burned into my mind a similar lesson: that I actually have responsibilities in this world, and I need to be intentional about how I operate. Doing nothing, or living on autopilot and letting myself simply become part of the plans of others, is not an automatically safe, or even blameless, course of action.</span>
<span class='rem'></span>
<span class='rem'>(To me, maybe similar effect from playing Witcher 3, Cyberpunk 2077 or similar because they force decision making and show the results)</span>
<span class='rem'> </span>
<br>
<span class='add'>???</span>
<span class='add'></span>
<br>
<span class='rem'>![](attachments/Pasted%20image%2020240510130851.png)</span>
<span class='rem'></span>
<span class='rem'>>Paul Graham has written about how every city [sends a message](https://paulgraham.com/cities.html): in New York, "you should make more money". In Boston, "You really should get around to reading all those books". In Silicon Valley, "you should be more powerful". When I visit Taipei, the message that comes to my mind is "you should rediscover your inner high school student".</span>
<span class='rem'></span>
</div>
<span class='rem'>2024-04-25 13:44</span>
</div>
<span class='add'>2024-04-25 13:44</span>
<div class='indent'>
<span class='rem'>2024-04-14 14:04</span>
</div>
<span class='add'>2024-04-14 14:04</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>2024-04-03 19:11</span>
</div>
<span class='add'>2024-04-03 19:11</span>
</div>
</div>
</div>
<span class='date' id='t2024-07-08-18:39'>2024 07 08 18:39</span><div class='indent'>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>tinycorp mission</span>
<div class='indent'>
<span class='rem'>improve soft-hardware interface for AI computesoftware first</span>
<br>
<span class='add'>improve soft-hardware interface for tensor compute first</span>
<br>
<span class='rem'>factory -> soft (tinygrad), hard (tinybox, tinychip)</span>
<span class='add'>factory -> soft (tinygrad), hard (tinybox, tinychip?)</span>
<br>
<span class='rem'>product -> compiled models</span>
<span class='add'>product -> compiled models?</span>
<br>
<span class='rem'>AI compute = tensors = multidimensional lists of floats</span>
<span class='rem'></span>
</div>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='rem'>`all()` is True of all arguments evaluate to True</span>
<span class='add'>`all()` and `any()` for evaluating multiple bools.</span>
<br>
<span class='rem'>if there is an argument in a function definition like `*`,&nbsp;&nbsp;&nbsp;&nbsp;it becomes optional and returns an empty tuple (or list?) if not given</span>
<span class='add'>if there is an argument in a function definition like `*atuple`,&nbsp;&nbsp;&nbsp;&nbsp;it becomes optional and returns an empty tuple (or list?) if not given</span>
<span class='add'></span>
</div>
<span class='add'>Importing Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>from tinygrad.tensor import Tensor</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>sets the stage with 3749 lines of tinygrad code as determined through `sys.settrace` (2024-07-08 17:27)</span>
<span class='add'>![](attachments/tinygrad_import_tensor.png)</span>
<span class='add'>Mostly [imports](https://docs.python.org/3/reference/import.html) and the construction of the `PatternMatcher` in `tinygrad/codegen/uops.py` (marked with cyan left border)</span>
<span class='add'>13: `helpery.py` </span>
<span class='add'>- makes `U` and `T` `TypeVar`s</span>
<span class='add'>- determines if the computer runs OSX to set the location of tinygrads cache</span>
<span class='add'>- sets and caches environment variables as `ContextVar` objects.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- DEBUG, IMAGE, BEAM, NOOPT, JIT</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- WINO, THREEFRY, CAPTURING</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- MULTIOUTPUT, PROFILE</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- this does not cover all environment variables relevant to tinygrad, not even those mentioned in the docs as [global variables](https://docs.tinygrad.org/env_vars/#global-variables)</span>
<span class='add'>- Global Counters: `global_ops`, `global_mem`, `time_sum_s`, `kernel_count`, `mem_used`</span>
<span class='add'>- ProfileLogger (?)</span>
<span class='add'>- sets up cache db path, cachelevel and version (?)</span>
<span class='add'>206: `dtype.py`</span>
<span class='add'>- `ConstType = Union[float, int, bool]`</span>
<span class='add'>- declares dtypes as DType Objects and some aliases:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, float16, bfloat16, float32, float64</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- half = float16; float = float32; double = float64 </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- char = int8; short = int16; int = int32; long = int64</span>
<span class='add'>- sets default float by environment variable else `float32` and default int `int32`</span>
<span class='add'>- `promo_lattice` that defines how different dtypes get promoted, presumably when different dtypes meet in an operation.</span>
<span class='add'>- `DTYPES_DICT` and `INVERSE_DTYPES_DICT` to translate between tinygrad dtypes and their names like "bool": dtypes.bool</span>
<span class='add'>367: `shape/symbolic.py`</span>
<span class='add'>- `sint = Union[int, Variable, MulNode, SumNode]`</span>
<span class='add'>- `render_python: Dict[Type, Callable[..., str]]`&nbsp;&nbsp;&nbsp;&nbsp;where the callables return a string representing the Object in `Type`.</span>
<span class='add'>581: `ops.py`</span>
<span class='add'>- `Op = Union[UnaryOps, BinaryOps, ReduceOps, LoadOps, TernaryOps, BufferOps]`</span>
<span class='add'>- `UNSAFE_PAD_OPS = &#123;UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}`</span>
<span class='add'>- `InterpretedFlopCounter: Dict[Op, Callable]` which generates `FlopCounter` objects with shape, flops and memory for various lazyops except `LoadOps`, `TernaryOps.MULACC`</span>
<span class='add'>- `python_alu` implements lazyops using python and its math module. covers `UnaryOps` except `CAST` and `BITCAST`, `BinaryOps` and `TernaryOps.WHERE`.</span>
<span class='add'>- `truncate: Dict[DType, Callable]` providing functions to truncate any number to the desired dtype.</span>
<span class='add'>754: `codegen/uops.py` (Note: quick reserach says UOps are really $\mu$ (micro) operations, UPat presumably is $\mu$ pattern)</span>
<span class='add'>- The `UOps(Enum)` class variables:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `SINK`,`VAR`,`DEFINE_GLOBAL`,`DEFINE_VAR`,`DEFINE_LOCAL`,`DEFINE_ACC`,`CONST`,`SPECIAL`,`NOOP`,`UNMUL`,`GEP`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `CAST`,`BITCAST`,`VECTORIZE`,`ALU`,`WMMA`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `LOAD`,`STORE`,`PHI`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `BARRIER`,`IF`,`RANGE`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `ENDRANGE`,`ENDIF`</span>
<span class='add'>- `TypeVar` `T`</span>
<span class='add'>- `constant_folder` which constructs a `PatternMatcher` singleton with a `patterns:List[Tuple[Union[UPat, UOp], Callable]]` (~500 lines)</span>
<span class='add'>- `PatternMatcher`'s initialization takes ~1300 more lines as it constructs `UPat` objects and runs their `compile` function.</span>
<span class='add'>2694: `device.py`</span>
<span class='add'>- `Device = _Device()` singleton, which populates `Device._devices` with strings of devices for which there is a `runtime/uops_&#123;device}.py` file</span>
<span class='add'>- sets defaults in `BufferOptions` class: `image = None`,`uncached`,`cpu_access`,`host`,`nolru` are all `False`</span>
<span class='add'>- `MallocAllocator = _MallocAllocator()` singleton (no `__init__`)</span>
<span class='add'>2816: `lazy.py`</span>
<span class='add'>- `lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()`</span>
<span class='add'>- `view_supported_devices = &#123;"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}`</span>
<span class='add'>2920: `codegen/kernel.py`</span>
<span class='add'>- `OptOps(Enum)`: `TC`,`UPCAST`,`UPCASTMID`,`UNROLL`,`LOCAL`,`GROUP`,`GROUPTOP`,`NOLOCALS`,`PADTO`</span>
<span class='add'>- `LocalBuffer` dataclass with `name`,`size`,`dtype=dtypes.float32`,`realized=None`</span>
<span class='add'>3007: `codegen/linearizer.py`</span>
<span class='add'>- `render_ops: Dict[Type, Callable[..., UOp]]`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- for `NumNode`,`Variable`,`MulNode`,`DivNode`,`ModNode`,`LtNode`,`SumNode`,`AndNode</span>
<span class='add'>~3100: `engine/schedule.py`</span>
<span class='add'>- `SCHEDULES: List = []`</span>
<span class='add'>3299: `tensor.py`</span>
<span class='add'>- `Tensor` class with:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `__slots__ = "lazydata", "requires_grad", "grad", "_ctx"`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `__deletable__ = ('_ctx',)`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `training`, `no_grad` are `False`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `_seed = int(time.time())`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `_rng_counter = None`</span>
<span class='add'>- produces methods on `Tensor`class for each device in `Device._devices` like `Tensor.cuda()` as aliases for `Tensor.to("cuda")`</span>
<span class='add'>- if `IMAGE` from environment variables `>0`, creates more aliases for `Tensor.image_conv2d` and `Tensor.image_dot` by introducing `Tensor.conv2d` and `Tensor.dot` respectively.</span>
<span class='add'>3646: `nn/state.py`</span>
<span class='add'>- `safe_dtypes`and `inverse_safe_dtype` dictionaries for translating between some naming (?) to tinygrad dtypes and back (inverse)</span>
<span class='add'>3728: `engine/jit.py`</span>
<span class='add'>- `ReturnType = TypeVar("ReturnType")`</span>
<span class='add'></span>
</div>
<span class='rem'>creating a Tensor</span>
<span class='add'>Creating a Tensor</span>
<div class='indent'>
<span class='rem'>*Tensor creation from [tinygrad docs](https://docs.tinygrad.org/tensor/:*</span>
<br>
<span class='add'>*Tensor creation from [tinygrad docs](https://docs.tinygrad.org/tensor/):</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-07-20:07'>2024 07 07 20:07</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>preliminary expression. I wonder if cyberspace exist, what it looks like. Maybe the dissolution of the body goes unnoticed. Maybe its dissolution is forgetting the real dimensions, as attention shifts to new worlds.</span>
<span class='add'>It seems, any ideological battle, any spiritual one, would be fought in virtual spaces because they adapt faster, can be more expressive. How good are todays virtual spaces? what would the high quality ones look like?</span>
<span class='add'>Visiting the endless worlds. Building a bridge to reality is an impossible translation, the work to break myself at.</span>
<span class='add'></span>
<span class='add'>![](attachments/creative-destruction-detail-3.jpg)</span>
<span class='add'>May the spirits remain informed by mortality and wander far.</span>
<span class='add'>![](attachments/creative-destruction-detail.jpg)</span>
</div>
</div>
</div>
<span class='date' id='t2024-07-07-13:58'>2024 07 07 13:58</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'>2024-05-15 08:28 Proto spirit stream ////</span>
<span class='add'>2024-05-15 08:28 Proto spirit stream</span>
</div>
</div>
<span>Towards insanely great AI.md</span>
<span>Towards spirit stream.md</span>
<span>index.md</span>
<span>tinygrad dev exploration.md</span>
</div>
<span class='date' id='t2024-07-07-13:47'>2024 07 07 13:47</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'>2024-05-15 08:28 Proto spirit stream</span>
<span class='add'>2024-05-15 08:28 Proto spirit stream ////</span>
<div class='indent'>
<span class='hdg'>2024-04-25 13:44</span>
<div class='indent'>
<span class='rem'>With some luck, the virtual clone ([2024-02-03-towards-insanely-great-ai](2024-02-03-towards-insanely-great-ai.md)) makes negotiation so cheap that many such rules can be thrown out.</span>
<br>
<span class='add'>With some luck, the virtual clone ([Towards insanely great AI](Towards%20insanely%20great%20AI.md)) makes negotiation so cheap that many such rules can be thrown out.</span>
</div>
</div>
</div>
</div>
<span>index.md</span>
<div class='indent'>
<span class='hdg'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='rem'>See note &lt;a href="_posts/changes.html">changes&lt;/a> for current thought stream and history.</span>
<span class='add'>See note &#123;&#123;changes}} for current thought stream and history.</span>
</div>
</div>
</div>
<span class='date' id='t2024-07-07-12:25'>2024 07 07 12:25</span><div class='indent'>
<span class='rem'>index.md</span>
<div class='indent'>
<span class='rem'>&lt;section> &lt;!-- introduction --></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;p></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A work in progress to stream the spirits to the collective mind.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The spirits can be expected to be disagreeable, conscientious, open, reasonably stable and slightly introverted.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/p></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;p></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See note &lt;a href="changes.html">changes&lt;/a> for current thought stream and history.&lt;br></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/p></span>
<span class='rem'>&lt;/section></span>
<span class='rem'></span>
<span class='rem'>&lt;section></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;ul class="posts"></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="date">&#123;&#123;- post.updated | date: "%Y %m %d" -}}&lt;/span></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/li></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;% endfor %}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/ul></span>
<span class='rem'>&lt;/section></span>
</div>
</div>
<span class='date' id='t2024-07-07-12:22'>2024 07 07 12:22</span><div class='indent'>
<span class='rem'>2024-06-22-tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='rem'>tinygrad dev exploration</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- [Direction](#Direction)</span>
<span class='rem'>- [More refined](#More%20refined)</span>
<span class='rem'>- [Less refined](#Less%20refined)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [tinycorp mission](#tinycorp%20mission)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [encountered python](#encountered%20python)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [creating a Tensor](#creating%20a%20Tensor)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [creating tensors with constructors](#creating%20tensors%20with%20constructors)</span>
<span class='rem'></span>
<span class='rem'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>read tensor.py</span>
<span class='rem'>explore anything unfamiliar</span>
<span class='rem'>condense any writing</span>
<span class='rem'>create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</span>
<span class='rem'></span>
</div>
<span class='rem'>More refined</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>tinycorp mission</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>accelerate, commoditize the petaflop</span>
<span class='rem'>improve soft-hardware interface for AI computesoftware first</span>
<span class='rem'>funded by love and tinyboxes</span>
<span class='rem'></span>
<span class='rem'>factory -> soft (tinygrad), hard (tinybox, tinychip)</span>
<span class='rem'>product -> compiled models</span>
<span class='rem'></span>
<span class='rem'>*tinygrad model --> friendly C --> standalone would be (is?) nice*</span>
<span class='rem'></span>
<span class='rem'>AI compute = tensors = multidimensional lists of floats</span>
<span class='rem'></span>
</div>
<span class='rem'>encountered python</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>`__slots__` lists the expected class attributes for fast access and memory savings [more](https://stackoverflow.com/questions/472000/usage-of-slots)</span>
<span class='rem'>`all()` is True of all arguments evaluate to True</span>
<span class='rem'>`WeakValueDictionary` for accessing values that can be garbage collected like the reference isn't there</span>
<span class='rem'>if there is an argument in a function definition like `*`</span>
<span class='rem'></span>
</div>
<span class='rem'>creating a Tensor</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>in `tensor.py`</span>
<span class='rem'></span>
<span class='rem'>`Tensor(data, device=None, dtype=None, requires_grad=None)`</span>
<span class='rem'></span>
<span class='rem'>determine device for the Tensor using `Device.canonicalize()`</span>
<span class='rem'>- eligible devices are those for which exists a `runtime/ops_&#123;device}.py`</span>
<span class='rem'>- if `device` is `None` and so cannot be canonicalized, it is set to the returned string from `Device.DEFAULT`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- returns the device that is set to 1 as an environment variable</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- if it finds none `&#123;device}Device.__init__(&#123;device})` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM` in their respective `runtime/ops_&#123;device}.py`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- for each device: `Compiled.__init__(device, MallocAllocator or &#123;device allocator}, &#123;Device renderer}, &#123;Device compiler}, &#123;device runtime}, &#123;device graph})`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- the name of the first device this causes no problems with, is returned from `Device.DEFAULT` and set to 1 as an environment variable.</span>
<span class='rem'></span>
<span class='rem'>*TODO: if `DEBUG` > 1, a message is printed informing which device was initialized*</span>
<span class='rem'></span>
<span class='rem'>depending on type of data, sets data to return value of helper functions `_loadop()` or `_frompy`</span>
<span class='rem'>Both return a `LazyBuffer` from calling:</span>
<span class='rem'>`LazyBuffer.loadop(op, shape, dtype, device, arg=None, src=(), enable_cache=False)`</span>
<span class='rem'>- `op` is either `LoadOps.EMPTY` or `LoadOps.CONST`, which are just numbers (0 and 1 respectively) from the LoadOps Enumerator in `ops.py`</span>
<span class='rem'>- `shape` is `tuple()`or `(0,)` if `LoadOps.EMPTY` or the actual shape if the input was `tuple` or `list`</span>
<span class='rem'>- `dtype`is always tinygrad.dtype, either given or determined from data</span>
<span class='rem'>- `device` is what was determined above or `NPY` if the input is a list, tuple or ndarray and so is numpy. Since numpy is removed soon from tinygrad, I ingore this detail.</span>
<span class='rem'>- `arg` is `data` passed to the Tensor</span>
<span class='rem'>`LazyBuffer.loadop()` checks that src is a tuple and gets `ShapeTracker.from_shape(shape)`</span>
<span class='rem'>- `ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments</span>
<span class='rem'>ShapeTracker is passed as an argument to the helper function `create_lazybuffer(device, ShapeTracker, dtype, op, arg, src, enable_cache)`</span>
<span class='rem'>- if `op==LoadOps.EMPTY`, the `size` of the ShapeTracker will be 0 and `op`will turn to `LoadOps.CONST` and unless Tensor data was `Variable`.</span>
<span class='rem'>- For reasons yet unknown, if `LoadOps.CONST`(guaranteed at this point, unless data was `Variable`), then the data (now in `arg`) runs through pythons native `int()`, `float()` or `bool()` functions depending on its dtype.</span>
<span class='rem'>- if the `LazyBuffer` is already `lazycache` and `enable_cache` is True, use that one.</span>
<span class='rem'>- else create one: `LazyBuffer(device, st, dtype, op, arg, srcs, base=base)` (`base` is `None`)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- Creates a `Buffer(device, st.size, dtype)` with additional properties:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.options = None`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.offset = 0`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self._base = None`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.lb_refcount = 1`</span>
<span class='rem'>The created `LazyBuffer` is stored in `Tensor.lazydata` after making sure it is on the right device</span>
<span class='rem'></span>
<span class='rem'>creating tensors through methods</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- Tensor.empty - no new ops</span>
<span class='rem'>- Tensor.zeros - `full(shape, 0, ...)`</span>
<span class='rem'>- Tensor.ones - `full(shape, 1, ...)`</span>
<span class='rem'>- `full(shape, fill_value)`:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>- Tensor.arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='rem'>- Tensor.eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='rem'>- Tensor.full_like - `full`</span>
<span class='rem'>- Tensor.zeros_like `full_like`</span>
<span class='rem'>- Tensor.ones_like `full_like`</span>
<span class='rem'></span>
<span class='rem'>all Tensor constructors that aren't random build on the `Tensor.full(shape, fill_value)` function, which first *reshapes* the Tensor with 1 element (fill_value) to the target number of dimensions.</span>
<span class='rem'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='rem'></span>
<span class='rem'>all `Function` "children", in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if `requires_grad` is True. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='rem'></span>
<span class='rem'>the `forward` method for `F.Reshape()` is called on the `lazydata`.</span>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`.</span>
<span class='rem'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS=0`, the new view is appended to `views` instead.</span>
<span class='rem'>In the current case, the previous View with shape `(1,)` is directly replaced by the new one `(1,)*len(new_shape)`.</span>
<span class='rem'>finally, the tensor gets a new `LazyBuffer` from&nbsp;&nbsp;&nbsp;&nbsp;`create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`</span>
<span class='rem'></span>
<span class='rem'>after the reshape, the dimension use `Tensor.expand(new_shape)` to get the now correct number of dimensions to the final shape.</span>
<span class='rem'>```python</span>
<span class='rem'>self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))</span>
<span class='rem'>```</span>
<span class='rem'>`argfix` ensures the function works even if the shape was not input as a tuple but through multiple arguments like `reshape(2,2,2)`.</span>
<span class='rem'>`_pad_left` gets inputs to the same number of dimensions.</span>
<span class='rem'>`*` unpacks the tuple with both shapes that `_pad_left` returns</span>
<span class='rem'></span>
<span class='rem'>`Tensor._broadcast_to(self, shape)` runs `_pad_left` again</span>
<span class='rem'>runs `self.reshape` again to the "padded" shape</span>
<span class='rem'>then `F.Expand.apply()` -> `lazybuffer.expand()` -> `shapetracker.expand()` -> `View.expand()` which producees&nbsp;&nbsp;&nbsp;&nbsp;a new `View` with the new shape and everything else being equal. returns a new `ShapeTracker`, returns a new `LazyBuffer`, returns a new `Tensor`</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>Tensor.arange offers new stuff, calling `Tensor._cumsum()`, using Tensor-Int addition and casting the Tensor.</span>
<span class='rem'>from `Tensor._cumsum()`:</span>
<span class='rem'>```python</span>
<span class='rem'>self.transpose(axis,-1).pad2d((pl_sz,-int(_first_zero)))._pool((self.shape[axis],)).sum(-1).transpose(axis,-1)</span>
<span class='rem'>```</span>
<span class='rem'>where `axis` is 0 and `pl_sz` will in this case be `self.shape[0] - 1`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.transpose(0, -1)`, which translates to `Tensor.permute(order)` where in the order dim 0 and the last dim were swapped. `permute` resolves orders with negative dim indices, error checks and runs `F.Permute.apply(self, order=resolve_order)` -> `lazybuffer.permute(order)` -> `ShapeTracker.permute(order)` -> `View.permute(axis=order)` -> `View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)`</span>
<span class='rem'>returns a new `View`in a new `ShapeTracker` in a new `lazybuffer` in a new `Tensor`</span>
<span class='rem'>this transpose changes nothing because the input was a 1D Tensor.</span>
<span class='rem'></span>
<span class='rem'>`Tensor.pad2d(self.shape[0] - 1, 0)` adds `self.shape[0] - 1` 0s to the left on the lowest dimension. Using `pad2d()` seems crazy here, it goes through `Tensor._slice()`, which eventually calls `Tensor.pad((self.shape[0] - 1, 0))` which is even crazier, which calls `F.Pad.apply(...)` which goes on the tour again.</span>
<span class='rem'>`LazyBuffer.pad()` -> `ShapeTracker.pad()` -> `View.pad()`</span>
<span class='rem'>where `(self.shape[0] - 1, 0)` turns into&nbsp;&nbsp;&nbsp;&nbsp;`(-self.shape[0] - 1, self.shape)`, which was already calculated in `Tensor.pad2d` for some reason.</span>
<span class='rem'>A mask is created: `((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))`</span>
<span class='rem'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)` where a new `View` is created with the extended `shape` (`self.shape[0] + self.shape[0] - 1`), `offset` of `-self.shape[0] - 1` and the `mask` as it was created. `contiguous` turns `False` whatever that means.</span>
<span class='rem'></span>
<span class='rem'>To see how mask, offset and maybe contiguous are interpreted, a detour to `Tensor.__getitem__()` follows. Or not, because `__getitem__` only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from `Tensor.tolist()`</span>
<span class='rem'>To return to: rest of `Tensor.arange`, other Tensor construction methods and random construction methods:</span>
<span class='rem'>- Tensor.manual_seed</span>
<span class='rem'>- Tensor.rand</span>
<span class='rem'>- Tensor.randn</span>
<span class='rem'>- Tensor.randint</span>
<span class='rem'>- Tensor.normal</span>
<span class='rem'>- Tensor.uniform</span>
<span class='rem'>- Tensor.scaled_uniform</span>
<span class='rem'>- Tensor.glorot_uniform</span>
<span class='rem'>- Tensor.kaiming_uniform</span>
<span class='rem'>- Tensor.kaiming_normal</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Realizing Tensors</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Starting from `Tensor([2,2,2]).pad(((2,0),)).tolist()`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.tolist()` -> `Tensor.data().tolist()` -> `Tensor._data().cast(self.dtype.fmt, self.shape).tolist()`</span>
<span class='rem'></span>
<span class='rem'>`Tensor._data`</span>
<span class='rem'> - `Tensor.cast(self.dtype.scalar())` applies `F.Cast(dtype)`</span>
<span class='rem'> - `Tensor.contiguous()` applies `F.Contiguous()`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.contigous()`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.e(LoadOps.CONTIGUOUS)` in the current case</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - makes sure dtypes and shapes(?) of all lazybuffers and their bases match</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - "const folding"(?), which in the current case does nothing</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - returns a new `LazyBuffer` with all sources (self and bases, in this case only self) in the `srcs` attribute</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- stores a reference and something in self.base.contiguous_child (?)</span>
<span class='rem'>- `Tensor.to("CLANG")`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- if it is not already on CLANG, it makes a new Tensor with the same lazydata, but a different device, so it makes a copy.</span>
<span class='rem'>- `Tensor.realize()`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `run_schedule(*self.schedule_with_vars(), do_update_stats = True)`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.schedule_with_vars()`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `create_schedule_with_vars(flatten(self.lazydata.lbs), seen=None)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `flatten()` (comes from `helpers.py`) does nothing with the Tensors, just makes the lazybuffers not be nested in multiple lists</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `_graph_schedule(outs:List[LazyBuffer], seen=set())`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `realises: Dict[LazyBuffer, None]` holds all unrealized lazybuffers</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `for out in outs: _recurse_lb(out.base, realizes, allbuffs = &#123;}, simple_pads = set(), children = defaultdict, scheduled=True)`</span>
</div>
</div>
</div>
</div>
<span class='rem'>2024-02-09-inbox.md</span><span class='add'>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'></span>
<span class='add'>[TOC]</span>
<span class='hdg'>2024-05-15 08:28 Proto spirit stream</span>
<div class='indent'>
<span class='rem'>![](/assets/pasted-image-20240123193144.png)</span>
<br>
<span class='add'>![](attachments/pasted-image-20240123193144.png)</span>
<br>
<span class='rem'>![](/assets/pasted-image-20240123193439.png)</span>
<br>
<span class='add'>![](attachments/pasted-image-20240123193439.png)</span>
<span class='hdg'>2024-05-11 08:15 [Vitalik Buterin - Coordination, Good and bad](https://vitalik.eth.limo/general/2020/09/11/coordination.html)</span>
<div class='indent'>
<span class='rem'>![](Pasted%20image%2020240511084052.png)</span>
<span class='add'>![](attachments/Pasted%20image%2020240511084052.png)</span>
</div>
<span class='hdg'>2024-05-10 12:47 [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
<div class='indent'>
<span class='rem'>![](Pasted%20image%2020240510130851.png)</span>
<span class='add'>![](attachments/Pasted%20image%2020240510130851.png)</span>
</div>
<span class='hdg'>2024-04-25 13:44</span>
<div class='indent'>
<span class='rem'>![](/assets/abandoned-building.jpg)</span>
<br>
<span class='add'>![](attachments/abandoned-building.jpg)</span>
</div>
</div>
</div>
</div>
<span class='rem'>2024-02-03-towards-insanely-great-ai.md</span><span class='add'>Towards insanely great AI.md</span>
<div class='indent'>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='add'>[TOC]</span>
<span class='rem'>- [Direction](#Direction)</span>
<span class='rem'>- [More refined](#More%20refined)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#Big%20picture%20path)</span>
<span class='rem'>- [Less refined](#Less%20refined)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Reading digits](#Reading%20digits)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#AI%20project%20ideas)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#Learning%20material)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [fastai diffusion from scratch](#fastai%20diffusion%20from%20scratch)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#Math%20of%20Diffusion)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Other](#Other)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tools](#Tools)</span>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Reading digits</span>
<div class='indent'>
<span class='rem'>![](/assets/mnist_example.png)</span>
<br>
<span class='add'>![](attachments/mnist_example.png)</span>
<br>
<span class='rem'>![](untrained-l1.png)</span>
<span class='add'>![](attachments/untrained-l1.png)</span>
<br>
<span class='rem'>![](untrained-neuron-summation.png)</span>
<span class='add'>![](attachments/untrained-neuron-summation.png)</span>
<br>
<span class='rem'>![](/assets/untrained_weighted_sums_output.png)</span>
<br>
<span class='add'>![](attachments/untrained_weighted_sums_output.png)</span>
<br>
<span class='rem'>![](/assets/summing_weighted_input_in_first_output_neuron.png)</span>
<br>
<span class='add'>![](attachments/summing_weighted_input_in_first_output_neuron.png)</span>
<br>
<span class='rem'>![](/assets/untrained_logits.png)</span>
<br>
<span class='add'>![](attachments/untrained_logits.png)</span>
<br>
<span class='rem'>![](/assets/untrained_logits_exp.png)</span>
<br>
<span class='add'>![](attachments/untrained_logits_exp.png)</span>
<br>
<span class='rem'>![](/assets/softmax_untrained_logits.png)</span>
<br>
<span class='add'>![](attachments/softmax_untrained_logits.png)</span>
<br>
<span class='rem'>![](/assets/untrained_log_softmax_and_nll.png)</span>
<br>
<span class='add'>![](attachments/untrained_log_softmax_and_nll.png)</span>
</div>
<span class='hdg'>Other</span>
<div class='indent'>
<span class='rem'>![](/assets/pasted-image-20240203122353.png)</span>
<br>
<span class='add'>![](attachments/pasted-image-20240203122353.png)</span>
</div>
</div>
</div>
</div>
<span class='rem'>2024-01-23-towards-spirit-stream.md</span><span class='add'>Towards spirit stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='add'>[TOC]</span>
<span class='rem'>- [Direction](#Direction)</span>
<span class='rem'>- [More refined](#More%20refined)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#Note%20structure)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure derivation](#Stream%20structure%20derivation)</span>
<span class='rem'>- [Less refined](#Less%20refined)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Specification](#Specification)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Implementation](#Implementation)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Research](#Research)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#Tech)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Text editor](#Text%20editor)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [other](#other)</span>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='add'>2024-06-30 18:06 Static site generator</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [] pages</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- index page</span>
<span class='add'>- [] output</span>
<span class='add'>- [] gen</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [] libraries</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- script</span>
<span class='add'>- settings</span>
<span class='add'>- css</span>
<span class='add'></span>
<span class='add'>features:</span>
<span class='add'>- convert from obsidian to html + table of contents</span>
<span class='add'>- version history with git</span>
<span class='add'>- automatic publishing</span>
<span class='add'></span>
<span class='add'>- include mathjax relevant .js</span>
<span class='add'>- implement empty lines</span>
<span class='add'></span>
<span class='add'>- update frontmatter</span>
<span class='add'>- write changes.md</span>
<span class='add'>- publish</span>
<span class='add'>- minify html and css?</span>
<span class='add'></span>
<span class='add'>to add later:</span>
<span class='add'>- mirror file structure like it is, only pop the index html out to the top if not already there</span>
<span class='add'>- readme file for github</span>
<span class='add'>- alert for attachments that are out of use</span>
<span class='add'>- local translation into some languages: german, chinese, japanese, russian, indian? probably only those that I understand, otherwise client side translation probably better.</span>
<span class='add'>- light mode</span>
<span class='add'></span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>```powershell</span>
<br>
<span class='add'>```shell</span>
</div>
</div>
</div>
</div>
<span class='add'>index.md</span>
<div class='indent'>
<span class='add'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='add'>A work in progress to stream the spirits to the collective mind.</span>
<span class='add'>The spirits can be expected to be disagreeable, conscientious, open, reasonably stable and slightly introverted.</span>
<span class='add'>They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='add'></span>
<span class='add'>See note &lt;a href="_posts/changes.html">changes&lt;/a> for current thought stream and history.</span>
<span class='add'>The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='add'></span>
<span class='add'>&#123;&#123;sitelist}}</span>
</div>
</div>
<span class='add'>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='add'>tinygrad tries to be simple. I like deleting things. See if I can't help delete in tinygrad. Seems to be a new and adventurous world on the other side.</span>
<span class='add'>tinygrad dev exploration</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[TOC]</span>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>trace execution of a tinygrad script</span>
<span class='add'>- steps:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.tensor import Tensor`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3])`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3]) + 2`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `(Tensor([1,2,3]) + 2).tolist()`</span>
<span class='add'>- visualize what parts of the script do</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- diff for each step</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- divide by file the lines come from</span>
<span class='add'>read tensor.py</span>
<span class='add'>explore anything unfamiliar</span>
<span class='add'>condense any writing</span>
<span class='add'>create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</span>
<span class='add'></span>
</div>
<span class='add'>More refined</span>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>tinycorp mission</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>accelerate, commoditize the petaflop</span>
<span class='add'>improve soft-hardware interface for AI computesoftware first</span>
<span class='add'>funded by love and tinyboxes</span>
<span class='add'></span>
<span class='add'>factory -> soft (tinygrad), hard (tinybox, tinychip)</span>
<span class='add'>product -> compiled models</span>
<span class='add'></span>
<span class='add'>*tinygrad model --> friendly C --> standalone would be (is?) nice*</span>
<span class='add'></span>
<span class='add'>AI compute = tensors = multidimensional lists of floats</span>
<span class='add'></span>
</div>
<span class='add'>encountered python</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>`__slots__` lists the expected class attributes for fast access and memory savings [more](https://stackoverflow.com/questions/472000/usage-of-slots)</span>
<span class='add'>`all()` is True of all arguments evaluate to True</span>
<span class='add'>`WeakValueDictionary` for accessing values that can be garbage collected like the reference isn't there</span>
<span class='add'>if there is an argument in a function definition like `*`,&nbsp;&nbsp;&nbsp;&nbsp;it becomes optional and returns an empty tuple (or list?) if not given</span>
<span class='add'></span>
</div>
<span class='add'>creating a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>from tinygrad.tensor import Tensor</span>
<span class='add'>t = Tensor([1,2,3])</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`from tinygrad.tensor import Tensor` triggers creation of the `Device` singleton, as `tensor.py` imports its, which is useful when creating Tensors.</span>
<span class='add'>`Device._devices` stores uppercase strings for devices available in tinygrad as determined by collecting all `tinygrad/runtime/ops_&#123;device}.py` files.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Tensor(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data: Union[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConstType,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;List,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tuple,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyBuffer,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ndarray,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MultiLazyBuffer,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Variable,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: Optional[Union[str, tuple, list]] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: Optional[DType] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;requires_grad: Optional[bool] = None,</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'>*Tensor creation from [tinygrad docs](https://docs.tinygrad.org/tensor/:*</span>
<span class='add'></span>
<span class='add'>determine device for the Tensor using `Device.canonicalize()`, which merely formats `device` if it's not `None`, but since it is, responsibility is handed to `Device.DEFAULT` to find one.</span>
<span class='add'>- it looks for `&#123;DEVICE}=1` in environment variables</span>
<span class='add'>- if it finds none `&#123;device}Device.__init__(&#123;device})` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM` in their respective `runtime/ops_&#123;device}.py`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- which eventually returns a `Compiled` device, which is cached for later use, but here it is only used to check if the device works and no more.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- if a device causes no problems, `Device.DEFAULT` returns its string and sets it to 1 as an environment variable</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp;&nbsp;if `DEBUG` > 1, a message is printed informing which device was initialized. `DEBUG` is a `ContextVar` defined in `helpers.py`. There are a few such variables and are initalized when importing from `helpers.py`. They store environment variables, are are shorthand. But not all environment variables relevant to tinygrad are initialized. Which makes this look useless.</span>
<span class='add'></span>
<span class='add'>depending on type of data, some local helper functions `_loadop()`, `_fromnp` or `_frompy`.</span>
<span class='add'>The example Tensor construction above triggers this handling:</span>
<span class='add'>```python</span>
<span class='add'>elif isinstance(data, (list, tuple)):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if dtype is None:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))</span>
<span class='add'>```</span>
<span class='add'>which infers the dtype and then uses numpy to create and cast an array. finally calling `_fromnp`: (numpy as a dependency is phased out, so this probably changes soon)</span>
<span class='add'>- a `LazyBuffer` is created using `LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")` where `x.shape` is numpys function to return array shape.</span>
<span class='add'>- `_from_np_dtype` looks up the numpy dtype name in a dictionary from `dtype.py` to get a tinygrad `DType`</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def loadop(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;op,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;shape: Tuple[sint,...],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;arg = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;src: Tuple[LazyBuffer, ...] = (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;enable_cache = False</span>
<span class='add'>): ...</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`LazyBuffer.loadop` otherwise does one errorcheck on the `srcs` argument (not supplied here) and produces a `ShapeTracker` through `ShapeTracker.from_shape(shape)` before passing arguments on to a helper function `create_lazybuffer` (further below) which also receives the argument `enable_cache` (`False` by default - if it were `True`, the lazybuffer would be stored in `lazycache`after creation).</span>
<span class='add'></span>
<span class='add'>`op` was given as `LoadOps.EMPTY`, which ist just a number in&nbsp;&nbsp;&nbsp;&nbsp;`class LoadOps(Enum)`, 0 in this case.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>ShapeTracker.from_shape(shape:Tuple[sint, ...]): return ShapeTracker((View.create(shape),))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class ShapeTracker:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;views: Tuple[View, ...]</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class View:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;shape:Tuple[sint, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;strides:Tuple[sint, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;offset:sint</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;mask:Optional[Tuple[Tuple[sint, sint], ...]]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;contiguous:bool</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>in `create_lazybuffer` the `lazycache` is interacted with, a `WeakValueDictionary` storing lazybuffers. a `cache_key` is generated from the lazybuffers parameters and if the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor:</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>LazyBuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'>*from [tinygrad docs](https://docs.tinygrad.org/developer/)*</span>
<span class='add'></span>
<span class='add'>notably `device` here given to be `"NPY"`, which comes from how the Tensor was initialized. This is different from the device determined at the beginning through `Device.DEFAULT`. Reason for this may become clearer?</span>
<span class='add'>`st` is the `ShapeTracker` just created</span>
<span class='add'></span>
<span class='add'>In the lazybuffer's initialization, it finds that `base` is `None` and decides that an assignment to `self.buffer` is in order.</span>
<span class='add'>Given the op `LoadOps.EMPTY`, it makes a `Buffer` (a class imported from `tinygrad.device`) through `Buffer(device, self.size, dtype)`. But creating it like that does nothing except store the instance.</span>
<span class='add'>the buffers `_lb_refcount` property is incremented by 1</span>
<span class='add'>the `contiguous_child` property (didn't exist before) is set to `None`</span>
<span class='add'>and `forced_realize` to `False`</span>
<span class='add'>the meaning of all 3 escapes me right now.</span>
<span class='add'></span>
<span class='add'>The `LazyBuffer` is done and returning to `_fromnp()` into the variable `ret` where:</span>
<span class='add'>`ret.buffer.allocate(x)` (x is a numpy array) causes the buffer to find itself an `Allocator`:</span>
<span class='add'>`self.allocator = Device[self.device].allocator`. Indexing into `Device` returns a `Compiled` Device (same as earlier when it was about finding an available device, but this time with "NPY")</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Compiled (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;allocator: Allocator,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;renderer: Optional[Renderer],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;compiler: Optional[Compiler],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;runtime,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;graph = None</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>on `buffer.allocate(x)` where `x` is the np.ndarray, which is just assigned to `buffer._buf`.</span>
<span class='add'>`del ret.srcs` (which is cruicial for `LazyBuffer.realized` to return `True`) completes what is commented "fake realize".</span>
<span class='add'></span>
<span class='add'>In the final step of `Tensor` initialization, the mismatching devices, one being the discovered one and one being "NPY" are detected and `self.lazydata = data.copy_to_device(device)` takes care of it, `data` being the created `LazyBuffer` and `device` being the discovered device from the start.</span>
<span class='add'>`LazyBuffer.copy_to_device(device)` in this case leads to `self.base._copy(device)._view(self.st)`</span>
<span class='add'></span>
<span class='add'>```python</span>
</div>
</div>
</div>
<span class='add'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='add'>return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, LoadOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>create_lazybuffer(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Any = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enable_cache = bool(getenv("LAZYCACHE", 1))</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`._view(self.st)` does nothing here, because the new shapetracker has the same shape and is contiguous.</span>
<span class='add'></span>
<span class='add'>The final object looks like this:</span>
<span class='add'>TODO: Not true, Tensor has more attributes than lazydata!</span>
<span class='add'>```python</span>
<span class='add'>Tensor.lazydata &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,), </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.COPY: 3>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': LazyBuffer &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'NPY',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.EMPTY: 1>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<div class='indent'>
<span class='add'>Adding to a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>t = Tensor([1,2,3]) + 2</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>goes to `Tensor.add(self, x, reverse=False)`</span>
<span class='add'>-> `return F.Add.apply(*self._broadcasted(x, reverse))`</span>
<span class='add'></span>
<span class='add'>`self._broadcasted` determines dtype then creates Tensor from `y` (2) using:</span>
<span class='add'>`Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)`</span>
<span class='add'>where `dtypes.as_const()` casts the input using one of pythons `int`, `float`, `bool` functions. Reason still escapes me.</span>
<span class='add'></span>
<span class='add'>bypassing the whole numpy story because data is integer and not array this time, so lazybuffer comes more directly from `_loadop(LoadOps.CONST, tuple(), dtype, device, data)` where `data` eventually ends up as the lazybuffers `arg` property.</span>
<span class='add'></span>
<span class='add'>The `ShapeTracker` will be empty, because the provided shape is `tuple()`. (its a 0D Tensor - one number)</span>
<span class='add'></span>
<span class='add'>Because `op` is `LoadOps.CONST` and dtype is `int` the data once again runs through `dtypes.as_const()` and `enable_cache` (-> `lazycache`)&nbsp;&nbsp;&nbsp;&nbsp;is enabled.</span>
<span class='add'></span>
<span class='add'>the returned `Tensor` looks like this:</span>
<span class='add'>```python</span>
<span class='add'>&#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 1,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>back in `_broadcasted`, dtypes of x and y are matched</span>
<span class='add'>`_broadcast_shape(x.shape, y.shape)` determines a target shape</span>
<span class='add'>and broadcast `x` and `y` to that shape (x is already that shape so nothing happens)</span>
<span class='add'></span>
<span class='add'>`padded = _pad_left(y.shape, shape)` where `shape` is the target shape transforms `()` to `(1,)`, ready to be expanded through `F.Expand.apply(self.reshape(padded), shape=shape)`</span>
<span class='add'></span>
<span class='add'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='add'>all `Function` "children", in their `apply`function, return a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if&nbsp;&nbsp;&nbsp;&nbsp;applicable. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='add'>`Function.apply()` calls the functions `forward` method on the `Tensor.lazydata`</span>
<span class='add'></span>
<span class='add'>`lazydata.reshape` turns into `self._view(st.reshape(newShape))` in `lazy.py`.</span>
<span class='add'>In `st.reshape(newShape)` (`shapetracker.py`), by default, the new returned `ShapeTracker` will have its most recent view in `views` replaced by a new one, through `View.reshape(newShape)`.</span>
<span class='add'>Environment variable `MERGE_VIEWS=0` changes this behaviour to including all previous views with the new one appended in the new shapetracker.</span>
<span class='add'></span>
<span class='add'>`View.reshape(newShape)` in this case simply returns a new View from `View.create(newShape)`</span>
<span class='add'>strides for the new shape&nbsp;&nbsp;&nbsp;&nbsp;are determined (`strides_for_shape(shape)` -> `(1,)`) and canonicalized -> `(0,)`.</span>
<span class='add'>finally:</span>
<span class='add'>```python</span>
<span class='add'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='add'>return View(shape, strides, offset, mask, contiguous)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>back at `_view(newShapetracker)` in `lazy.py` a new lazybuffer comes from `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`.</span>
<span class='add'>notably, `self.base` is just `self` because `self._base` is `None`</span>
<span class='add'>```python</span>
<span class='add'>@property</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def base(self) -> LazyBuffer: return self._base if self._base is not None else self</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>Tensor after reshape:</span>
<span class='add'>```python</span>
<span class='add'>Tensor:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(1,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (1,)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = LazyBuffer:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = ()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;LoadOps.CONST: 2></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = 2</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = ()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child: None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>next from `F.Expand.apply(self.reshape(padded), shape=(3,))`, where `self.reshape(padded)` has now returned the new Tensor. Expand similarly returns a new Tensor with a new LazyBuffer from&nbsp;&nbsp;&nbsp;&nbsp;`LazyBuffer.expand` -> `ShapeTracker.expand` -> `View.expand` -> `View.create(new_shape, self.strides, self.offset, mask)` -> `View` -> `ShapeTracker` -> `LazyBuffer._view` -> `createLazyBuffer` -> `LazyBuffer`</span>
<span class='add'></span>
<span class='add'>notably, `View.create` does not change strides and since no mask was given it also remains `None`. These lines:</span>
<span class='add'>```python</span>
<span class='add'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='add'>return View(shape, strides, offset, mask, contiguous)</span>
<span class='add'>```</span>
<span class='add'>cause `contiguous` to be `False` because the unchaged stride is `(0,)`, but the the appropriate stride for the new shape would be `(1,)`</span>
<span class='add'>Notably, `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)` takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer.</span>
<span class='add'>So the finally returned Tensor is:</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Tensor:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (3,)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 3</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = LazyBuffer:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = ()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size = 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;LoadOps.CONST: 2></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = 2</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = ()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child: None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>While the expand used the "reshape lazybuffer", there remains no reference to that lazybuffer in the final Tensor.</span>
<span class='add'></span>
<span class='add'>Finally, `F.Add.apply` is called on the input tensor and the created Tensor.</span>
<span class='add'>new tensor lazydata = `return x.e(BinaryOps.ADD, y)` where `BinaryOps.ADD`, like `LoadOps.CONST` is an entry in `class BinaryOps(Enum)`</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def e(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;self, </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;op:Union[LoadOps, UnaryOps, BinaryOps, TernaryOps],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;*in_srcs:LazyBuffer,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Optional[Any] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;) -> LazyBuffer</span>
<span class='add'>```</span>
<span class='add'>gets `out_dtype` from input</span>
<span class='add'>tries shortcuts if one of the operants is effectively 0</span>
<span class='add'>```python</span>
<span class='add'>create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Tensor:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides = (1,)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset = 0</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous = True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (3,)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;BinaryOps.ADD: 1></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>, # whole lazybuffer, not writing it out here</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))> # whole lazybuffer, not writing it out here</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through `srcs` (in ADD) or `_base` (in shape changes) and so form a graph.</span>
<span class='add'>```bash</span>
<span class='add'>DEBUG=4 CUDA=1 python -c "from tinygrad.tensor import Tensor; (Tensor([1,2,3]) + 2).tolist()"</span>
<span class='add'>```</span>
<span class='add'>displays a graph that seem to echo this, though shape changes are apparently left out</span>
<span class='add'>```</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;0  BufferOps.STORE MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp; BinaryOps.ADD None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BufferOps.LOAD MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BufferOps.CONST ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)))</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
<span class='add'>Realizing a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>(Tensor([1,2,3]) + 2).tolist()</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`Tensor.tolist()` = `Tensor.data().tolist()` = `Tensor._data().cast(self.dtype.fmt, self.shape).tolist()`</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def _data(self) -> memoryview:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if 0 in self.shape: return memoryview(bytearray(0))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this realizes on the object from as_buffer being a Python object</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.device != "CLANG": buf.options = BufferOptions(nolru=True)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)</span>
<span class='add'>```</span>
<span class='add'>`Tensor.cast(self.dtype.scalar())` applies `F.Cast(dtype)`</span>
<span class='add'>`Tensor.contiguous()` applies `F.Contiguous()`</span>
<span class='add'> - `LazyBuffer.contigous()`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.e(LoadOps.CONTIGUOUS)` in the current case</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - makes sure dtypes and shapes(?) of all lazybuffers and their bases match</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - "const folding"(?), which in the current case does nothing</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - returns a new `LazyBuffer` with all sources (self and bases, in this case only self) in the `srcs` attribute</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- stores a reference and something in self.base.contiguous_child (?)</span>
<span class='add'>`Tensor.to("CLANG")`</span>
<span class='add'>- if it is not already on CLANG, it makes a new Tensor with the same lazydata, but `device="CLANG"`, so it makes a copy.</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python </span>
<span class='add'>def schedule_with_vars(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;self,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;*lst:Tensor,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='add'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return memory_planner(schedule), var_vals</span>
<span class='add'>```</span>
<span class='add'>where `flatten` in this case returns a list with one entry: the "BinaryOps.ADD-lazybuffer" </span>
<span class='add'></span>
<span class='add'>from `engine/schedule.py`</span>
<span class='add'>```python</span>
<span class='add'>SCHEDULES: List = []</span>
<span class='add'>def create_schedule_with_vars(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='add'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if seen is None: seen = set()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;graph, in_degree, prescheduled = _graph_schedule(outs, seen)</span>
<span class='add'></span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>from `engine/schedule.py`</span>
<span class='add'>```python</span>
<span class='add'>def _graph_schedule(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Set[LazyBuffer]</span>
<span class='add'>) -> Tuple[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;DefaultDict[LazyBuffer, List[LazyBuffer]], DefaultDict[LazyBuffer, int],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;Dict[LazyBuffer, _LBScheduleItem]</span>
<span class='add'>]:</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""create a graph for realizing the outputs"""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# start by just realizing the buffers passed in</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;realizes: Dict[LazyBuffer, None] = &#123;x.base:None for x in outs if x.base.realized is None}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs: Dict[LazyBuffer, None] = &#123;}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;simple_pads: Set[LazyBuffer] = set()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;children: DefaultDict[LazyBuffer, Dict[LazyBuffer, None]] = defaultdict(dict)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for out in outs: _recurse_lb(out.base, realizes, allbufs, simple_pads, children, scheduled=True)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;```</span>
<span class='add'>strange that it uses `out.base` it means if the latest lazybuffer were a reshape, it would be ignored for now.</span>
<span class='add'></span>
<span class='add'>from `engine/schedule.py`</span>
<span class='add'>```python</span>
<span class='add'>def _recurse_lb(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;buf:LazyBuffer,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;realizes:Dict[LazyBuffer, None],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs:Dict[LazyBuffer, None],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads:Set[LazyBuffer],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled=False</span>
<span class='add'>):</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""recursively search the entire graph for all LazyBuffers, insert realizes after expands"""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in allbufs or buf.base.realized is not None: return</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH: log_lazybuffer(buf, scheduled)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# view</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.base != buf:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fuse some pads</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads.add(buf.base)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize all expands</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif prod(buf.base.st.shape) &lt; prod(buf.st.shape):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize image to image casts. this is part of a larger problem</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.base] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# check all other pads for safe fusion</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif any(v.mask is not None for v in buf.st.views): simple_pads.add(buf.base)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# base</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs[buf] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.forced_realize: realizes[buf] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in LoadOps: realizes[buf.base] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.COPY:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.srcs[0].base] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.VIEW: realizes[buf.srcs[0].base] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for x in buf.srcs:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;children[x.base][buf] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recurse_lb(x, realizes, allbufs, simple_pads, children)</span>
<span class='add'>```</span>
<span class='add'>puts lazybuffers in `allbuffs` dictionary</span>
<span class='add'>and loadops into `realizes`</span>
<span class='add'></span>
<span class='add'>back in `_graph_schedule`:</span>
<span class='add'>```python</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in realizes if x.op is LoadOps.ASSIGN and x not in seen and x.realized is None}</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# check if we have to realize pads</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for p in simple_pads:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not _is_padding_okay(p, realizes):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[p] = None</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# find all reduces, and pair them to a elementwise op. if they can't be cleanly paired, force realize the reduce (or a contig child)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op: Dict[LazyBuffer, LazyBuffer] = &#123;}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for r in allbufs:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if r.op not in ReduceOps or r in realizes: continue</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: Set[LazyBuffer] = set()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recursive_group(r, r.st, r, children, realizes, reduce_for_op, group)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;can_chase = all(tr not in reduce_for_op for tr in group)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# TODO: forced_realize exists because the scheduler is incapable of checking for self-contained DAGs</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = r in group</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and len(group) > 1:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a multi output kernel if the LazyBufferss can cleanly group</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_parents, rc_children = deque(group), deque(group)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_parents and not forced_realize:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=rc_parents.pop()).op in ReduceOps: forced_realize = True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: rc_parents.extend(x.base for x in p.srcs if x.base.realized is None and x.base is not r)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# search descendants of the reduceop that can cleanly group</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants: Set[LazyBuffer] = set()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_children and not forced_realize:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (c:=rc_children.pop()).op in ReduceOps or not c.st.contiguous or c.st.size != r.st.size or c in reduce_for_op:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants.clear()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if c in realizes and c not in group: realized_descendants.add(c)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_children.extend(x for x in children[c] if x.realized is None and x.device == r.device)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group.update(realized_descendants)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can only fuse assign if no other assign_target is used in the kernel</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and any(x.op is LoadOps.ASSIGN for x in group):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents = deque((r, *group))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while parents and not forced_realize:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=parents.pop().base).realized or p in realizes:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if p in assign_targets and assign_targets[p] not in group: forced_realize, can_chase = True, False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents.extend(p.srcs)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if forced_realize:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = r</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if can_chase:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can chase this down to contiguous children</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = tr.st</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while len(children[tr]) == 1:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr_next = next(iter(children[tr]))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st_childs = dedup(s for s in tr_next.srcs if s.base is tr)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(st_childs) > 1: break</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if st.size != st_childs[0].st.size: break</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = st + st_childs[0].st</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not st.contiguous or tr_next.op in ReduceOps: break</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr_next</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# don't cast to higher size before store (tr cannot be realized if forced_realize)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if tr.op is UnaryOps.CAST and tr.arg.itemsize > tr.srcs[0].dtype.itemsize:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr.srcs[0].base</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op[tr] = r</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[tr] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: reduce_for_op.update((tr, r) for tr in group)</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;output_groups: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in realizes:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.realized is not None or buf.op is LoadOps.CONST or buf in seen: continue</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups[reduce_for_op[buf] if buf in reduce_for_op and MULTIOUTPUT else buf].append(buf)</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# make things that can't be images not images</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if isinstance(buf.dtype, ImageDType) and (prod(buf.shape) != prod(buf.dtype.shape) or</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not any(buf.shape[x]%4 == 0 for x in buf.st.unit_stride_axes())):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2: print(f"forcing image &#123;buf.dtype} with shape &#123;buf.shape} to float32")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.dtype = dtypes.float32</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# hack the underlying buffer too</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base is buf:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert not hasattr(buf.buffer, '_buf'), "can't fixup allocated buffer"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.dtype = dtypes.float32</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.options = None</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# preschedule all buffers in realizes</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prescheduled = &#123;group[0]:_schedule_group(tuple(group), realizes, reduce_for_op) for group in output_groups.values()}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;schedule_targets = &#123;out:ps for ps in prescheduled.values() for out in ps.outputs}</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;graph: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;in_degree: DefaultDict[LazyBuffer, int] = defaultdict(int)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for key, lsi in prescheduled.items():</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if key not in in_degree: in_degree[key] = 0</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs after all parents are realized</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled_parents = set(schedule_targets[x].outputs[0] for x in lsi.inputs if x in schedule_targets)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for x in scheduled_parents:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[x].append(key)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[key] += 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs before a parent is assigned to</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents_assigns = set(schedule_targets[assign_targets[x]].outputs[0] for x in lsi.inputs if x in assign_targets)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for assign in parents_assigns:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[key].append(assign)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[assign] += 1</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return graph, in_degree, prescheduled</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>back in `create_schedule_with_vars`</span>
<span class='add'>```python</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;queue = deque(si for key, si in prescheduled.items() if in_degree[key] == 0)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;schedule: List[ScheduleItem] = []</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = &#123;}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;kernel_number = GlobalCounters.kernel_count</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;while queue:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ps = queue.popleft()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in ps.outputs: seen.add(buf)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_number += 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: realized_lazybuffer(out, kernel_number)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals = merge_dicts([var_vals, ps.var_vals])</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: del out.srcs&nbsp;&nbsp;&nbsp;&nbsp;# can only schedule once</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;schedule.append(si:=ScheduleItem(ps.ast, tuple(x.buffer for x in (ps.outputs+ps.inputs) if x.size != 0)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if logops and si.ast[0].op not in LoadOps and not any(i.device.startswith("DISK:") for i in si.inputs): logops.write(str(si.ast)+"\n")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for x in graph[ps.outputs[0]]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[x] -= 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if in_degree[x] == 0: queue.append(prescheduled[x])</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if SAVE_SCHEDULE:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def _save():</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"saving &#123;len(SCHEDULES)} schedule graphs to", fp:=getenv("SAVE_SCHEDULE_PATH", "schedule.pkl"))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with open(fp, "wb") as f: pickle.dump(SCHEDULES, f)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if len(SCHEDULES) == 0: atexit.register(_save)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;SCHEDULES.extend((ps.ast for ps in prescheduled.values()) if getenv("CAPTURE_AST") else [(graph, prescheduled)])</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# confirm everything was scheduled correctly</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if not all(degree == 0 for degree in in_degree.values()) or len(prescheduled) != len(schedule):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"cycle detected in graph, prescheduled &#123;len(prescheduled)} but only scheduled &#123;len(schedule)}")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 1 and len(schedule) >= 10: print(f"scheduled &#123;len(schedule)} kernels")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return schedule, var_vals</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>---</span>
<span class='add'>watch out for garbo below</span>
<span class='add'></span>
</div>
<span class='add'>creating tensors through methods</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Tensor.empty - no new ops</span>
<span class='add'>- Tensor.zeros - `full(shape, 0, ...)`</span>
<span class='add'>- Tensor.ones - `full(shape, 1, ...)`</span>
<span class='add'>- `full(shape, fill_value)`:</span>
<span class='add'>```python</span>
<span class='add'>Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>- Tensor.arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='add'>- Tensor.eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='add'>- Tensor.full_like - `full`</span>
<span class='add'>- Tensor.zeros_like `full_like`</span>
<span class='add'>- Tensor.ones_like `full_like`</span>
<span class='add'></span>
<span class='add'>all Tensor constructors that aren't random build on the `Tensor.full(shape, fill_value)` function, which first *reshapes* the Tensor with 1 element (fill_value) to the target number of dimensions.</span>
<span class='add'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='add'></span>
<span class='add'>all `Function` "children", in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if `requires_grad` is True. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='add'></span>
<span class='add'>the `forward` method for `F.Reshape()` is called on the `lazydata`.</span>
<span class='add'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`.</span>
<span class='add'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS=0`, the new view is appended to `views` instead.</span>
<span class='add'>In the current case, the previous View with shape `(1,)` is directly replaced by the new one `(1,)*len(new_shape)`.</span>
<span class='add'>finally, the tensor gets a new `LazyBuffer` from&nbsp;&nbsp;&nbsp;&nbsp;`create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`</span>
<span class='add'></span>
<span class='add'>after the reshape, the dimension use `Tensor.expand(new_shape)` to get the now correct number of dimensions to the final shape.</span>
<span class='add'>```python</span>
<span class='add'>self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))</span>
<span class='add'>```</span>
<span class='add'>`argfix` ensures the function works even if the shape was not input as a tuple but through multiple arguments like `reshape(2,2,2)`.</span>
<span class='add'>`_pad_left` gets inputs to the same number of dimensions.</span>
<span class='add'>`*` unpacks the tuple with both shapes that `_pad_left` returns</span>
<span class='add'></span>
<span class='add'>`Tensor._broadcast_to(self, shape)` runs `_pad_left` again</span>
<span class='add'>runs `self.reshape` again to the "padded" shape</span>
<span class='add'>then `F.Expand.apply()` -> `lazybuffer.expand()` -> `shapetracker.expand()` -> `View.expand()` which producees&nbsp;&nbsp;&nbsp;&nbsp;a new `View` with the new shape and everything else being equal. returns a new `ShapeTracker`, returns a new `LazyBuffer`, returns a new `Tensor`</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>Tensor.arange offers new stuff, calling `Tensor._cumsum()`, using Tensor-Int addition and casting the Tensor.</span>
<span class='add'>from `Tensor._cumsum()`:</span>
<span class='add'>```python</span>
<span class='add'>self.transpose(axis,-1).pad2d((pl_sz,-int(_first_zero)))._pool((self.shape[axis],)).sum(-1).transpose(axis,-1)</span>
<span class='add'>```</span>
<span class='add'>where `axis` is 0 and `pl_sz` will in this case be `self.shape[0] - 1`</span>
<span class='add'></span>
<span class='add'>`Tensor.transpose(0, -1)`, which translates to `Tensor.permute(order)` where in the order dim 0 and the last dim were swapped. `permute` resolves orders with negative dim indices, error checks and runs `F.Permute.apply(self, order=resolve_order)` -> `lazybuffer.permute(order)` -> `ShapeTracker.permute(order)` -> `View.permute(axis=order)` -> `View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)`</span>
<span class='add'>returns a new `View`in a new `ShapeTracker` in a new `lazybuffer` in a new `Tensor`</span>
<span class='add'>this transpose changes nothing because the input was a 1D Tensor.</span>
<span class='add'></span>
<span class='add'>`Tensor.pad2d(self.shape[0] - 1, 0)` adds `self.shape[0] - 1` 0s to the left on the lowest dimension. Using `pad2d()` seems crazy here, it goes through `Tensor._slice()`, which eventually calls `Tensor.pad((self.shape[0] - 1, 0))` which is even crazier, which calls `F.Pad.apply(...)` which goes on the tour again.</span>
<span class='add'>`LazyBuffer.pad()` -> `ShapeTracker.pad()` -> `View.pad()`</span>
<span class='add'>where `(self.shape[0] - 1, 0)` turns into&nbsp;&nbsp;&nbsp;&nbsp;`(-self.shape[0] - 1, self.shape)`, which was already calculated in `Tensor.pad2d` for some reason.</span>
<span class='add'>A mask is created: `((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))`</span>
<span class='add'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)` where a new `View` is created with the extended `shape` (`self.shape[0] + self.shape[0] - 1`), `offset` of `-self.shape[0] - 1` and the `mask` as it was created. `contiguous` turns `False` whatever that means.</span>
<span class='add'></span>
<span class='add'>To see how mask, offset and maybe contiguous are interpreted, a detour to `Tensor.__getitem__()` follows. Or not, because `__getitem__` only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from `Tensor.tolist()`</span>
<span class='add'>To return to later: rest of `Tensor.arange`, other Tensor construction methods and random construction methods:</span>
<span class='add'>- Tensor.manual_seed</span>
<span class='add'>- Tensor.rand</span>
<span class='add'>- Tensor.randn</span>
<span class='add'>- Tensor.randint</span>
<span class='add'>- Tensor.normal</span>
<span class='add'>- Tensor.uniform</span>
<span class='add'>- Tensor.scaled_uniform</span>
<span class='add'>- Tensor.glorot_uniform</span>
<span class='add'>- Tensor.kaiming_uniform</span>
<span class='add'>- Tensor.kaiming_normal</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='add'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Some environment variables are stored in `ContexVar._cache` and as `ContextVar` instances and can be imported from `tinygrad.helpers` but others are determined dynamically through `getenv` which is also imported from `tinygrad.helpers` and used like `getenv("LAZYCACHE", 1)`. Not obvious why this added complexity.</span>
<span class='add'></span>
<span class='add'>`tensor.py` too big, methods more around imitating style than being nicely categorized? Remove stuff like `Tensor.ones` or duplication of `Tensor.transpose` and `Tensor.T`</span>
<span class='add'></span>
<span class='add'>`Tensor(2).lazydata.contiguous_child` is `None` but</span>
<span class='add'>`Tensor(1).lazydata.contiguous_child` is a tuple of weakref to some lazybuffer and its own ShapeTracker ??</span>
<span class='add'></span>
<span class='add'>beautiful lazy graph and linearized graph in DEBUG=4</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-06-30-18:05'>2024 06 30 18:05</span><div class='indent'>
<span>2024-06-22-tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'>condense any writing</span>
<span class='add'>create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</span>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='add'>if there is an argument in a function definition like `*`</span>
</div>
<span class='hdg'>creating a Tensor</span>
<div class='indent'>
<span class='hdg'>creating tensors through methods</span>
<div class='indent'>
<span class='rem'>A mask is created: `((self.shape[0] - 1, self.shape + self.shape[0] - 1))`</span>
<br>
<span class='add'>A mask is created: `((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))`</span>
<br>
<span class='rem'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)`</span>
<span class='add'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)` where a new `View` is created with the extended `shape` (`self.shape[0] + self.shape[0] - 1`), `offset` of `-self.shape[0] - 1` and the `mask` as it was created. `contiguous` turns `False` whatever that means.</span>
<br>
<span class='add'>To see how mask, offset and maybe contiguous are interpreted, a detour to `Tensor.__getitem__()` follows. Or not, because `__getitem__` only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from `Tensor.tolist()`</span>
<span class='add'>To return to: rest of `Tensor.arange`, other Tensor construction methods and random construction methods:</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='add'></span>
</div>
</div>
<span class='add'>Realizing Tensors</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Starting from `Tensor([2,2,2]).pad(((2,0),)).tolist()`</span>
<span class='add'></span>
<span class='add'>`Tensor.tolist()` -> `Tensor.data().tolist()` -> `Tensor._data().cast(self.dtype.fmt, self.shape).tolist()`</span>
<span class='add'></span>
<span class='add'>`Tensor._data`</span>
<span class='add'> - `Tensor.cast(self.dtype.scalar())` applies `F.Cast(dtype)`</span>
<span class='add'> - `Tensor.contiguous()` applies `F.Contiguous()`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.contigous()`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - `LazyBuffer.e(LoadOps.CONTIGUOUS)` in the current case</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - makes sure dtypes and shapes(?) of all lazybuffers and their bases match</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - "const folding"(?), which in the current case does nothing</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - returns a new `LazyBuffer` with all sources (self and bases, in this case only self) in the `srcs` attribute</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- stores a reference and something in self.base.contiguous_child (?)</span>
<span class='add'>- `Tensor.to("CLANG")`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- if it is not already on CLANG, it makes a new Tensor with the same lazydata, but a different device, so it makes a copy.</span>
<span class='add'>- `Tensor.realize()`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `run_schedule(*self.schedule_with_vars(), do_update_stats = True)`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.schedule_with_vars()`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `create_schedule_with_vars(flatten(self.lazydata.lbs), seen=None)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `flatten()` (comes from `helpers.py`) does nothing with the Tensors, just makes the lazybuffers not be nested in multiple lists</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `_graph_schedule(outs:List[LazyBuffer], seen=set())`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `realises: Dict[LazyBuffer, None]` holds all unrealized lazybuffers</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `for out in outs: _recurse_lb(out.base, realizes, allbuffs = &#123;}, simple_pads = set(), children = defaultdict, scheduled=True)`</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-06-29-19:28'>2024 06 29 19:28</span><div class='indent'>
<span>2024-06-22-tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='rem'>- [Direction](#direction)</span>
<br>
<span class='add'>- [Direction](#Direction)</span>
<br>
<span class='rem'>- [More refined](#more%20refined)</span>
<br>
<span class='add'>- [More refined](#More%20refined)</span>
<br>
<span class='rem'>- [Less refined](#less%20refined)</span>
<br>
<span class='add'>- [Less refined](#Less%20refined)</span>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>creating a Tensor</span>
<div class='indent'>
<span class='rem'>creating tensors with constructors</span>
<span class='add'>creating tensors through methods</span>
<div class='indent'>
<span class='rem'>- empty - no new ops</span>
<span class='add'>- Tensor.empty - no new ops</span>
<br>
<span class='rem'>- zeros - `full(shape, 0, ...)`</span>
<span class='add'>- Tensor.zeros - `full(shape, 0, ...)`</span>
<br>
<span class='rem'>- ones - `full(shape, 1, ...)`</span>
<span class='add'>- Tensor.ones - `full(shape, 1, ...)`</span>
<br>
<span class='rem'>- arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='add'>- Tensor.arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<br>
<span class='rem'>- eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='add'>- Tensor.eye - `ones().pad().flatten().shrink().reshape()`</span>
<br>
<span class='rem'>- full_like - `full`</span>
<span class='add'>- Tensor.full_like - `full`</span>
<br>
<span class='rem'>- zeros_like `full_like`</span>
<span class='add'>- Tensor.zeros_like `full_like`</span>
<br>
<span class='rem'>- ones_like `full_like`</span>
<span class='add'>- Tensor.ones_like `full_like`</span>
<br>
<span class='add'>all `Function` "children", in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if `requires_grad` is True. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='add'></span>
<span class='rem'>the `forward` function is called on the `lazydata`.</span>
<br>
<span class='add'>the `forward` method for `F.Reshape()` is called on the `lazydata`.</span>
<br>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`</span>
<span class='add'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`.</span>
<br>
<span class='rem'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS`is 0, the new view is appended to `views` instead.</span>
<br>
<span class='add'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS=0`, the new view is appended to `views` instead.</span>
<br>
<span class='rem'>In the current case, because the previous shape was `(1,)`, the new one `(1,)*len(new_shape)`, the new View directly replaces the old one.</span>
<span class='add'>In the current case, the previous View with shape `(1,)` is directly replaced by the new one `(1,)*len(new_shape)`.</span>
<span class='rem'></span>
<span class='rem'>all `Function` successors, in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx`.</span>
<br>
<span class='add'>`argfix` ensures the function works even if the shape was not input as a tuple but through multiple arguments like `reshape(2,2,2)`.</span>
<span class='add'>`_pad_left` gets inputs to the same number of dimensions.</span>
<span class='add'>`*` unpacks the tuple with both shapes that `_pad_left` returns</span>
<span class='add'></span>
<span class='add'>`Tensor._broadcast_to(self, shape)` runs `_pad_left` again</span>
<span class='add'>runs `self.reshape` again to the "padded" shape</span>
<span class='add'>then `F.Expand.apply()` -> `lazybuffer.expand()` -> `shapetracker.expand()` -> `View.expand()` which producees&nbsp;&nbsp;&nbsp;&nbsp;a new `View` with the new shape and everything else being equal. returns a new `ShapeTracker`, returns a new `LazyBuffer`, returns a new `Tensor`</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>Tensor.arange offers new stuff, calling `Tensor._cumsum()`, using Tensor-Int addition and casting the Tensor.</span>
<span class='add'>from `Tensor._cumsum()`:</span>
<span class='add'>```python</span>
<span class='add'>self.transpose(axis,-1).pad2d((pl_sz,-int(_first_zero)))._pool((self.shape[axis],)).sum(-1).transpose(axis,-1)</span>
<span class='add'>```</span>
<span class='add'>where `axis` is 0 and `pl_sz` will in this case be `self.shape[0] - 1`</span>
<span class='add'></span>
<span class='add'>`Tensor.transpose(0, -1)`, which translates to `Tensor.permute(order)` where in the order dim 0 and the last dim were swapped. `permute` resolves orders with negative dim indices, error checks and runs `F.Permute.apply(self, order=resolve_order)` -> `lazybuffer.permute(order)` -> `ShapeTracker.permute(order)` -> `View.permute(axis=order)` -> `View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)`</span>
<span class='add'>returns a new `View`in a new `ShapeTracker` in a new `lazybuffer` in a new `Tensor`</span>
<span class='add'>this transpose changes nothing because the input was a 1D Tensor.</span>
<span class='add'></span>
<span class='add'>`Tensor.pad2d(self.shape[0] - 1, 0)` adds `self.shape[0] - 1` 0s to the left on the lowest dimension. Using `pad2d()` seems crazy here, it goes through `Tensor._slice()`, which eventually calls `Tensor.pad((self.shape[0] - 1, 0))` which is even crazier, which calls `F.Pad.apply(...)` which goes on the tour again.</span>
<span class='add'>`LazyBuffer.pad()` -> `ShapeTracker.pad()` -> `View.pad()`</span>
<span class='add'>where `(self.shape[0] - 1, 0)` turns into&nbsp;&nbsp;&nbsp;&nbsp;`(-self.shape[0] - 1, self.shape)`, which was already calculated in `Tensor.pad2d` for some reason.</span>
<span class='add'>A mask is created: `((self.shape[0] - 1, self.shape + self.shape[0] - 1))`</span>
<span class='add'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)`</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='rem'>- manual_seed</span>
<span class='add'>- Tensor.manual_seed</span>
<br>
<span class='rem'>- rand</span>
<span class='rem'>- randn</span>
<span class='rem'>- randint</span>
<span class='rem'>- normal</span>
<span class='rem'>- uniform</span>
<span class='add'>- Tensor.rand</span>
<span class='add'>- Tensor.randn</span>
<span class='add'>- Tensor.randint</span>
<span class='add'>- Tensor.normal</span>
<span class='add'>- Tensor.uniform</span>
<span class='rem'>- scaled_uniform</span>
<span class='add'>- Tensor.scaled_uniform</span>
<br>
<span class='rem'>- glorot_uniform</span>
<span class='add'>- Tensor.glorot_uniform</span>
<br>
<span class='rem'>- kaiming_uniform</span>
<span class='add'>- Tensor.kaiming_uniform</span>
<br>
<span class='rem'>- kaiming_normal</span>
<span class='add'>- Tensor.kaiming_normal</span>
</div>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-06-28-11:45'>2024 06 28 11:45</span><div class='indent'>
<span>2024-06-22-tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>creating a Tensor</span>
<div class='indent'>
<span class='hdg'>creating tensors with constructors</span>
<div class='indent'>
<span class='rem'>```</span>
<span class='add'>```python</span>
<br>
<span class='rem'>```</span>
<span class='add'>```python</span>
</div>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-06-28-10:58'>2024 06 28 10:58</span><div class='indent'>
<span class='rem'>2024-06-22 tinygrad-dev-exploration.md</span><span class='add'>2024-06-22-tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='rem'>2024-06-22 tinygrad-dev-exploration</span>
<span class='add'>tinygrad dev exploration</span>
<div class='indent'>
<span class='rem'>- [[#Direction]]</span>
<span class='rem'>- [[#More refined]]</span>
<span class='rem'>- [[#Less refined]]</span>
<span class='add'>- [Direction](#direction)</span>
<span class='add'>- [More refined](#more%20refined)</span>
<span class='add'>- [Less refined](#less%20refined)</span>
</div>
</div>
</div>
<span class='date' id='t2024-06-28-09:34'>2024 06 28 09:34</span><div class='indent'>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<span class='add'>2024-06-22 tinygrad-dev-exploration.md</span>
<div class='indent'>
<span class='add'>2024-06-22 tinygrad-dev-exploration</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [[#Direction]]</span>
<span class='add'>- [[#More refined]]</span>
<span class='add'>- [[#Less refined]]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [tinycorp mission](#tinycorp%20mission)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [encountered python](#encountered%20python)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [creating a Tensor](#creating%20a%20Tensor)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [creating tensors with constructors](#creating%20tensors%20with%20constructors)</span>
<span class='add'></span>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>read tensor.py</span>
<span class='add'>explore anything unfamiliar</span>
<span class='add'></span>
</div>
<span class='add'>More refined</span>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>tinycorp mission</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>accelerate, commoditize the petaflop</span>
<span class='add'>improve soft-hardware interface for AI computesoftware first</span>
<span class='add'>funded by love and tinyboxes</span>
<span class='add'></span>
<span class='add'>factory -> soft (tinygrad), hard (tinybox, tinychip)</span>
<span class='add'>product -> compiled models</span>
<span class='add'></span>
<span class='add'>*tinygrad model --> friendly C --> standalone would be (is?) nice*</span>
<span class='add'></span>
<span class='add'>AI compute = tensors = multidimensional lists of floats</span>
<span class='add'></span>
</div>
<span class='add'>encountered python</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>`__slots__` lists the expected class attributes for fast access and memory savings [more](https://stackoverflow.com/questions/472000/usage-of-slots)</span>
<span class='add'>`all()` is True of all arguments evaluate to True</span>
<span class='add'>`WeakValueDictionary` for accessing values that can be garbage collected like the reference isn't there</span>
<span class='add'></span>
</div>
<span class='add'>creating a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>in `tensor.py`</span>
<span class='add'></span>
<span class='add'>`Tensor(data, device=None, dtype=None, requires_grad=None)`</span>
<span class='add'></span>
<span class='add'>determine device for the Tensor using `Device.canonicalize()`</span>
<span class='add'>- eligible devices are those for which exists a `runtime/ops_&#123;device}.py`</span>
<span class='add'>- if `device` is `None` and so cannot be canonicalized, it is set to the returned string from `Device.DEFAULT`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- returns the device that is set to 1 as an environment variable</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- if it finds none `&#123;device}Device.__init__(&#123;device})` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM` in their respective `runtime/ops_&#123;device}.py`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- for each device: `Compiled.__init__(device, MallocAllocator or &#123;device allocator}, &#123;Device renderer}, &#123;Device compiler}, &#123;device runtime}, &#123;device graph})`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- the name of the first device this causes no problems with, is returned from `Device.DEFAULT` and set to 1 as an environment variable.</span>
<span class='add'></span>
<span class='add'>*TODO: if `DEBUG` > 1, a message is printed informing which device was initialized*</span>
<span class='add'></span>
<span class='add'>depending on type of data, sets data to return value of helper functions `_loadop()` or `_frompy`</span>
<span class='add'>Both return a `LazyBuffer` from calling:</span>
<span class='add'>`LazyBuffer.loadop(op, shape, dtype, device, arg=None, src=(), enable_cache=False)`</span>
<span class='add'>- `op` is either `LoadOps.EMPTY` or `LoadOps.CONST`, which are just numbers (0 and 1 respectively) from the LoadOps Enumerator in `ops.py`</span>
<span class='add'>- `shape` is `tuple()`or `(0,)` if `LoadOps.EMPTY` or the actual shape if the input was `tuple` or `list`</span>
<span class='add'>- `dtype`is always tinygrad.dtype, either given or determined from data</span>
<span class='add'>- `device` is what was determined above or `NPY` if the input is a list, tuple or ndarray and so is numpy. Since numpy is removed soon from tinygrad, I ingore this detail.</span>
<span class='add'>- `arg` is `data` passed to the Tensor</span>
<span class='add'>`LazyBuffer.loadop()` checks that src is a tuple and gets `ShapeTracker.from_shape(shape)`</span>
<span class='add'>- `ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using the helper function `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments</span>
<span class='add'>ShapeTracker is passed as an argument to the helper function `create_lazybuffer(device, ShapeTracker, dtype, op, arg, src, enable_cache)`</span>
<span class='add'>- if `op==LoadOps.EMPTY`, the `size` of the ShapeTracker will be 0 and `op`will turn to `LoadOps.CONST` and unless Tensor data was `Variable`.</span>
<span class='add'>- For reasons yet unknown, if `LoadOps.CONST`(guaranteed at this point, unless data was `Variable`), then the data (now in `arg`) runs through pythons native `int()`, `float()` or `bool()` functions depending on its dtype.</span>
<span class='add'>- if the `LazyBuffer` is already `lazycache` and `enable_cache` is True, use that one.</span>
<span class='add'>- else create one: `LazyBuffer(device, st, dtype, op, arg, srcs, base=base)` (`base` is `None`)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Creates a `Buffer(device, st.size, dtype)` with additional properties:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.options = None`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.offset = 0`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self._base = None`</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `self.lb_refcount = 1`</span>
<span class='add'>The created `LazyBuffer` is stored in `Tensor.lazydata` after making sure it is on the right device</span>
<span class='add'></span>
<span class='add'>creating tensors with constructors</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- empty - no new ops</span>
<span class='add'>- zeros - `full(shape, 0, ...)`</span>
<span class='add'>- ones - `full(shape, 1, ...)`</span>
<span class='add'>- `full(shape, fill_value)`:</span>
<span class='add'>```</span>
<span class='add'>Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>- arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='add'>- eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='add'>- full_like - `full`</span>
<span class='add'>- zeros_like `full_like`</span>
<span class='add'>- ones_like `full_like`</span>
<span class='add'></span>
<span class='add'>all Tensor constructors that aren't random build on the `Tensor.full(shape, fill_value)` function, which first *reshapes* the Tensor with 1 element (fill_value) to the target number of dimensions.</span>
<span class='add'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='add'></span>
<span class='add'>the `forward` function is called on the `lazydata`.</span>
<span class='add'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`</span>
<span class='add'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS`is 0, the new view is appended to `views` instead.</span>
<span class='add'>In the current case, because the previous shape was `(1,)`, the new one `(1,)*len(new_shape)`, the new View directly replaces the old one.</span>
<span class='add'>finally, the tensor gets a new `LazyBuffer` from&nbsp;&nbsp;&nbsp;&nbsp;`create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`</span>
<span class='add'></span>
<span class='add'>all `Function` successors, in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx`.</span>
<span class='add'></span>
<span class='add'>after the reshape, the dimension use `Tensor.expand(new_shape)` to get the now correct number of dimensions to the final shape.</span>
<span class='add'>```</span>
<span class='add'>self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>- manual_seed</span>
<span class='add'>- rand</span>
<span class='add'>- randn</span>
<span class='add'>- randint</span>
<span class='add'>- normal</span>
<span class='add'>- uniform</span>
<span class='add'>- scaled_uniform</span>
<span class='add'>- glorot_uniform</span>
<span class='add'>- kaiming_uniform</span>
<span class='add'>- kaiming_normal</span>
</div>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-06-06-13:44'>2024 06 06 13:44</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream evolution](#Spirit%20stream%20evolution)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure derivation](#Stream%20structure%20derivation)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Specification](#Specification)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Implementation](#Implementation)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#Stream%20structure%20design)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [1. Generating, viewing](#1.%20Generating,%20viewing)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [2. Sharing](#2.%20Sharing)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [P2P networks](#P2P%20networks)</span>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Nand2Tetris](https://nand2tetris.org/)</span>
</div>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'>Spirit stream evolution</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>![](/assets/pasted-image-20240126212515.png)</span>
<span class='rem'>history page with headings where something was updated</span>
<span class='rem'></span>
<span class='rem'>Version 2024 03 07:</span>
<span class='rem'>![](/assets/20240307-spirit-stream.png)</span>
<span class='rem'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='rem'>- where is the history?</span>
<span class='rem'>- the newest update column provides only garbage information, who cares if I fixed a typo?</span>
<span class='rem'>- who decided to make links suddenly be red?</span>
<span class='rem'>- why can't I tell when I already visited them?</span>
<span class='rem'>- is it an external or internal link?</span>
<span class='rem'>- can't tell the difference between headings</span>
<span class='rem'>- how do I see only what was recently changed?</span>
<span class='rem'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='rem'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
<span class='rem'></span>
<span class='rem'>2024-05-01 20:40:</span>
<span class='rem'>![](Pasted%20image%2020240501203222.png)</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Stream structure design</span>
<span class='add'>Stream structure derivation</span>
<div class='indent'>
<span class='rem'>a concise, stable a path through the blocks - the ultimate test of their consistency - is a true story.</span>
<br>
<span class='add'>a concise, stable path through the blocks - the ultimate test of their consistency - is a true story.</span>
<span class='add'></span>
<br>
<span class='add'>sharing structures/stories, they are further tested. A successful meme directly enteres the recipient, who tests it for consistency in their own mind.</span>
<span class='rem'>sharing stories, I test their consistency</span>
<span class='rem'>a true story is recognized and directly enteres the recipient, who tests it for consistency in their own mind.</span>
<span class='rem'>it has become an effective meme.</span>
<br>
<span class='add'>Assuming the goal is playing a long and interesting game. Making sharing of discoveries, offers, uncertainties, plans, progress and spheres of mind context easy means offering a more synchronized, everybody-on-the-front-line discovery tour of the universe.</span>
<span class='add'>Seems more interesting than remaining in a confined environment and negotiating with its dominant structure.</span>
<span class='rem'>joscha bach streams in non-integrated posts</span>
<span class='rem'>the structure is hidden but accessible</span>
<span class='rem'>what is the preferred interface to the hive mind at different distances into the future?</span>
<br>
<span class='rem'>Importantly, the goal is not story generation, but playing a long and interesting game. Making sharing&nbsp;&nbsp;&nbsp;&nbsp;of discoveries, offers, uncertainties, plans, progress and spheres of mind context easy means offering a more synchronized, everybody-on-the-front-line discovery tour of the universe.</span>
<br>
<span class='rem'>a spirit stream should also platform direct conversations to further consistency.</span>
<span class='add'></span>
<span class='add'>The spirit stream is also an enabler for ultra capitalism, bringing everyday microtransactions closer and making cooperation -> competition more accessible.</span>
<span class='add'>Probably, extreme capitalism is indistinguishable from normal group dynamics - it forwards the rules of reality to the user without middlemen - , but with some communication tools, extendable to an infinitely large group.</span>
<span class='add'></span>
<span class='add'></span>
<br>
<span class='rem'>- enabling abitrary direct exchange (optionally entering the stream)</span>
<span class='rem'>- forms of payment, an emergent "meme market"</span>
<span class='add'>- abitrary direct exchange, private or public</span>
<span class='add'>- forms of payment</span>
<span class='add'>- privacy / lack of intrusion</span>
<br>
<span class='rem'>Tools that help:</span>
<span class='rem'>- version history to keep the present clean without information loss, building a ramp for new readers, transparency and base reality.</span>
<br>
<span class='rem'>if reuse is easy enough, cooperation works without shared files which would require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='add'>if reuse is easy enough, cooperation works without shared files which require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='rem'>conversation can optionally be public and seamlessly enter the spirit stream.</span>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='add'>Specification</span>
<div class='indent'>
<span class='rem'>easily and securely host quotable, changing information, optionally anonymously, without middlemen.</span>
<span class='rem'>a beautiful tool has few, clear components that allow diverse structures.</span>
</div>
<span class='rem'>1. Generating, viewing</span>
<div class='indent'>
<span class='add'>>Thinking about the ideal blogging platform:&lt;br>&lt;br>1. Writing: &lt;br>- in markdown&lt;br>- with full WYSIWYG, not just split view (think: Typora)&lt;br>- super easy to copy paste and add images&lt;br>2. Deploying:&lt;br>- renders into static pages (think: Jekyll)&lt;br>- super simple, super minimal html with no bloat&lt;br>-&amp;mdash; Andrej Karpathy (@karpathy) [January 27, 2024](https://twitter.com/karpathy/status/1751350002281300461)</span>
<br>
<span class='add'>content</span>
<span class='add'>vocabulary</span>
<span class='add'>rendering engine / interface</span>
<span class='add'></span>
<span class='add'>Direct experience in a responsive environment precedes the need for stamps.</span>
<span class='add'>stamps only work if the reader can interpret them.</span>
<span class='add'></span>
<span class='add'>Featureset:</span>
<span class='add'>stamp creation quickly explodes (video, 3d, animation, sound, - to BCI and infinity)</span>
<span class='add'>increasing featureset get diminishing returns. The problem in expression is translating to another medium and a limited featureset forces a more concise translation. Infinite possibility/parameters easily distract.</span>
<span class='add'>Markdown punches up with linked media and good text formatting.</span>
<span class='add'>To honor the continuum between letter stamp and media, images should be easily resizable and inline-placeable. This instantly enables arbitrary positioning.</span>
<span class='add'></span>
<span class='add'>[file over app](https://stephango.com/file-over-app) but the "file" does not exist long term until printed. with programming languages, storage management, compilers, OSs still between me and my data, it can hardly persist forever.</span>
<span class='add'>make the format simple, "general", exportable and printable?</span>
<span class='add'>image formats change as coding languages do?</span>
<span class='add'></span>
<span class='add'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='add'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='add'></span>
<span class='add'>have multiple scopes overlap, eg friend party that is watching.</span>
<span class='add'>like entering a "channel" with dynamic scope of recipients, opt. hierarchy.</span>
<span class='add'>reverse searching for the stream can also find people who are watching.</span>
<span class='add'></span>
<span class='add'>maybe the tool itself should be so minimal that the interface itself is in markdown too. Means markdown links can now link to code?</span>
<span class='add'>Means the text-manipulation tools apply to the software too which avoids duplicating the tools to make a fast interface for messages/filters/including messages.</span>
<span class='add'></span>
<span class='add'>maybe twitter tries to become so good that they build coherence for users, which requires human level+ AI and near complete knowledge of the user.</span>
<span class='add'>"subscribing" = scraping a target, which is expensive. entering on a list on their server is cheaper.</span>
<span class='add'></span>
<span class='add'>if the system becomes corrupted, it should be extremely easy to fork and rebuild it somewhere else.</span>
<span class='add'></span>
<span class='rem'>sight, hearing, touch, smell, taste + emotion, thought, association</span>
<span class='add'>- live streaming and recording (sight, hearing, touch, smell, taste + emotion, thought, association)</span>
<br>
<span class='rem'>until BCI, internal state is expressed indirectly and can be captured with sight and hearing.</span>
<span class='rem'>unobtrusive cam, mic, battery and charging. headband for first person, otherwise smartphone?</span>
<span class='add'>- viewing</span>
<span class='add'>- editing -> making and placing stamps, symbols, tokens at various scales</span>
<span class='add'>- make any file available at any scope</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- convert files to sharing formats. SEO/discoverability, translation, compatibility</span>
<span class='add'>- opt. notify recipients, signaling relevance, which is on a continuum and approximated by both the sender and recipient.</span>
<span class='add'>- API for automation</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- filter messages</span>
<span class='add'>- export content to common formats / other services</span>
<span class='add'>- analytics</span>
</div>
<span class='add'>Implementation</span>
<div class='indent'>
<span class='rem'>it is inherently too much, needs filtering</span>
<span class='rem'>streaming is cheap. maintaining and sorting the past is expensive (articles, movies).</span>
<br>
<span class='rem'>- bci</span>
<br>
<span class='rem'>Only a BCI is a true stream.</span>
<span class='rem'>Language is "stamping" of symbols to represent experience. images / video are high-res one-time stamps.</span>
<span class='rem'>Experience become "stamps" on a continuum.</span>
<span class='rem'>But stamps only work if the reader can interpret them.</span>
<span class='rem'>Direct experience in a responsive environment precedes the need for stamps. Stamps likely can include everything.</span>
<span class='rem'>"stamps" = "tokens". subword or multiword.</span>
<br>
<span class='add'>"lazy payment" - unrealized costs become realized when someone pays transaction costs.</span>
<span class='rem'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='rem'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='rem'></span>
<span class='rem'>content</span>
<span class='rem'>vocabulary</span>
<span class='rem'>rendering engine / interface</span>
<span class='rem'></span>
<span class='rem'>solution should work on scales between small screen / VR desktop/video / BCI</span>
<span class='rem'></span>
</div>
<span class='rem'>2. Sharing</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>on continuum of scope</span>
<span class='rem'>I offer information passively.</span>
<span class='rem'>Then emulate minds of others to send them what they may want and how they may want it.</span>
<span class='rem'></span>
<span class='rem'>convert to sharing-friendly formats (SEO) + using existing rendering engines on client + client oriented formats</span>
<span class='rem'></span>
<span class='rem'>live streaming is high res. can interface with other streamers and through chat, which is a collaborative environment, which instantly gets messy because it is shared. has some moderation.</span>
<span class='rem'>stream in a sense opens up for direct messaging relevant to the current situation.</span>
<span class='rem'>can be a "mode", a different filter for interaction. No tax emails, only in scope for current thing.</span>
<span class='rem'>then negotiate what is shared, what is private. can be specified in message through scope.</span>
<span class='rem'>scope selector.</span>
<span class='rem'>live stream chat is a scope to avoid the scope selector.</span>
<span class='rem'>maybe because live opens up a lot to relevance.</span>
<span class='rem'>live interaction reveals true complexity of live hive mind</span>
<span class='rem'>seems like the zoom-problem, where 20 people are forced to listen to 1 and can't fragment easily.</span>
<span class='rem'></span>
<span class='rem'>find quotes and edits (optional)</span>
<span class='rem'></span>
<span class='rem'>pay for sharing/viewing/donation</span>
<span class='rem'></span>
<span class='rem'>scrape other sites/RSS</span>
<span class='rem'>send advertisement with some compensation</span>
<span class='rem'></span>
<br>
<span class='rem'>smoothe scope selection for sharing. opens messaging, todolists, cloud service, calendar, groups, global sharing</span>
<span class='rem'>host any file type</span>
<span class='rem'>viewer for some files (html, md, images, video, sound)</span>
<span class='rem'>analytics</span>
</div>
</div>
<span class='hdg'>Research</span>
<div class='indent'>
<span class='add'>disqus?</span>
<span class='add'>jinja2 templates</span>
<span class='add'>plausible analytics</span>
<span class='add'>atom/rss</span>
<span class='add'>uploading to server: SSH vs SFTP</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>[Python modules](https://docs.python.org/3/tutorial/modules.html)</span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>From scratch</span>
<span class='rem'>- all markdown features</span>
<span class='rem'>- WYSIWYG interface</span>
<span class='rem'>- nice url</span>
<span class='rem'>- analytics</span>
<span class='rem'>- comments</span>
<span class='rem'>- Dynamic site support</span>
<span class='rem'>- version control</span>
<span class='rem'>- live stream</span>
<span class='rem'>- print markdown</span>
<span class='rem'></span>
<span class='rem'>- domain -> ip (cloudflare DNS-O-Matic?)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- holds records to route different requests to different ips. can also route email traffic. records are cached and may point to old ip adress.</span>
<span class='rem'>- webserver (apache)</span>
<span class='rem'>- web app (flask)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- generates html, caching after generating it</span>
<span class='rem'>- WYSIWYG editor</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- markdown</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- templates</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- css</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- stream</span>
<span class='rem'>- status viewer</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- traffic</span>
<span class='rem'></span>
<span class='rem'>disqus?</span>
<span class='rem'>[Python modules](https://docs.python.org/3/tutorial/modules.html)</span>
<span class='rem'>jinja2 templates</span>
<span class='rem'>plausible analytics</span>
<span class='rem'>tor network</span>
<span class='rem'>atom/rss</span>
<span class='rem'>P2P websites</span>
<span class='rem'>uploading to server: SSH vs SFTP</span>
<span class='rem'></span>
<span class='rem'>From [Host website locally](https://www.youtube.com/watch?v=euXdC0NDgac)</span>
<span class='rem'>- XAMPP -> Apache, MySQL</span>
<span class='rem'>- wordpress for content</span>
<span class='rem'>- [port forwarding](https://www.quora.com/Can-we-live-a-localhost-server-website-to-the-internet-If-so-how)</span>
<span class='rem'></span>
<span class='rem'>I use a soft overlay on some files to edit them. Publish. Enter a domain if I want to. And its out.</span>
<span class='rem'>The overlay is easily understood.</span>
<span class='rem'>Easily modified</span>
<span class='rem'>```</span>
<span class='rem'>markdown, mathjax, css</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;.md ++</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;.jpg&nbsp;&nbsp;&nbsp;&nbsp;.png&nbsp;&nbsp;&nbsp;&nbsp;.gif&nbsp;&nbsp;&nbsp;&nbsp;.mp4&nbsp;&nbsp;&nbsp;&nbsp;.mp3 ...</span>
<span class='rem'>html, css, javascript -> machine version</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;templates</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>servers sends markdown and everyone renders it as they wish?</span>
<span class='rem'>I put out information, the llm filters it anyway</span>
<span class='rem'></span>
<span class='rem'>connect to form the hive mind</span>
<span class='rem'>share experience and build an environment that is greatly supportive of what I want to do. A responsive environment, an overlay over reality to interface optimally with it.</span>
<span class='rem'>it means finding information when I need it, offers, people, all this fast. Instant. Trust is information, no reason why the internet should destroy trust. elevate, augment and stream the spirit.</span>
<span class='rem'></span>
<span class='rem'>build a virtual clone</span>
<span class='rem'>the llm interface to the world. it does not matter to output information in high quality, it is enough if it is there and readable by the llm.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>hosting locally sucks because who lets their computer run 24/7? Who needs to host locally, if the content is public anyway?</span>
<span class='rem'>hosting on an external computer requires trust and is usually more expensive than necessary. Free options exist but they require negotiation.</span>
<span class='rem'></span>
<span class='rem'>- local server that converts .md files when serving. with preview server.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- integrate as much as possible: small computer, server software, local drive, </span>
<span class='rem'>- platform service. pay per bandwidth and compute. at cost with donations. implement whatever structure I like. become a registry too? or host on subdomain. repo\.github\.io or github\.io\/repo. People can buy their domain if they like.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- google drive should also be a blogging platform</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- twitch should be a spirit stream</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- people can pay with microtransactions, reducing abstractions and forwarding the rules to the user</span>
<span class='rem'></span>
<span class='rem'>2: clone that scrapes the network and negotiates between people. Hand over control to the clones. Extend smoothly into real world, as machines are introduced that can extend the clones.</span>
<span class='rem'>initially, information is stored on the central server. The system should be open enough such that there is a balance of power because the dying system could easily be salvaged, reproduced elsewhere.</span>
<span class='rem'></span>
<span class='rem'>hosting locally is also slow. copying to various servers is faster. decentralized. how does tor work?</span>
<span class='rem'>data is distributed across many private servers? </span>
<span class='rem'>blockchain? web3?</span>
<span class='rem'></span>
<span class='rem'>get the spirits into the world. what is holding them back? negotiation? a virtual clone. sometimes it is not possible to stream</span>
<span class='rem'></span>
<br>
<span class='add'>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)</span>
<span class='add'>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='rem'>1. universal publisher (thoughts, experience, weather data, sale/purchase offers, private cloud,...)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;1. local server for anything</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;2. desktop software as interface</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;3. offer hosting</span>
<span class='rem'>2. clone and autoscraper, so much potential to replace things</span>
<span class='rem'>3. physical instantiation</span>
<span class='rem'>4. accelerate towards independent AI</span>
<span class='rem'></span>
<span class='rem'>stream raw files, made web friendly if necessary</span>
<span class='rem'>&lt;br>&lt;br>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)&lt;br>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='rem'></span>
<span class='rem'>P2P networks</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[P2P networks Introduction](https://www.youtube.com/watch?v=Vw9ynzuGNSw) P2P security challenges:</span>
<span class='rem'>- how to avoid eaves-dropping</span>
<span class='rem'>- how to avoid peer identity impersonation</span>
<span class='rem'>- how to avoid fabrication</span>
<span class='rem'>- how to avoid replay attacks</span>
<span class='rem'>- how to provide peer anonymity</span>
<span class='rem'></span>
</div>
</div>
<span class='hdg'>Text editor</span>
<div class='indent'>
<span class='rem'>- set cursor to some spot and start typing</span>
<span class='add'>- cursor positioning</span>
<br>
<span class='add'>- shortcuts</span>
<span class='rem'>- cmd + a to select all</span>
<span class='rem'>- END and START keys</span>
<span class='rem'>- scroll keys</span>
<br>
<span class='rem'>- nice line breaks</span>
<span class='add'>- nice line breaks, at least between words</span>
<span class='rem'>- saving and opening an .md file</span>
<br>
<span class='add'>- saving and opening a file</span>
<span class='rem'>- markdown features</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- text formatting</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- images/video/audio</span>
<span class='rem'>- mathjax</span>
<span class='rem'>- QoL features</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- alt + arrows moves lines</span>
<span class='rem'></span>
<span class='rem'>- language control / API: AI can read/write to the thing</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- local AI for message filtering / summarizing / finding new content / filtering social interaction</span>
<span class='rem'>- version history</span>
<span class='rem'>- direct messages / calls</span>
<span class='rem'>- export to web format</span>
<span class='rem'>- publish through IPFS</span>
<span class='rem'>- quoting</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- quoting is public if the place where to quote lives is</span>
<span class='rem'>- payment</span>
<br>
<span class='rem'>- arange files and show / not show information like shared to how many</span>
<span class='rem'>- show large files or in "Detail" mode like in file browser.</span>
<span class='add'>- show large files or in "Detail" mode like in file browser, opt. with information about sharing</span>
<span class='rem'></span>
<span class='add'>- explore page so that people can actually start from 0 with their content?</span>
<br>
<span class='rem'>- making glyphs / emoticons / "stamps" for images, sounds</span>
<span class='rem'></span>
<span class='rem'>- explore page so that people can actually start from 0 with their content</span>
<span class='rem'></span>
<span class='rem'>- Ctrl+O open files explorer, press tab, gives the whole thing + recently used ones</span>
<br>
<span class='add'>- give overlay when browsing to "bookmark" to a file</span>
<span class='rem'>- save things to "playlists" ie folders or notes(pages)</span>
<span class='rem'></span>
<span class='rem'>- entry</span>
<span class='rem'>- open</span>
<span class='rem'>- outline</span>
<span class='rem'>- minimap</span>
<span class='rem'>- text</span>
<span class='rem'></span>
<br>
<span class='add'>- feature to edit the page should also exist client-side.</span>
<span class='rem'></span>
<span class='rem'>filter the shit out!</span>
<span class='rem'>stamp things endlessly at any size. stamp onetime = images, video, audio, pixels. arbitrary stamp location.</span>
<span class='rem'>make available with scope</span>
<span class='rem'>direct messages, programmatic too, like notifications</span>
<span class='rem'>rooms, that link together. map is another room. other people rearange your room to their ideas.</span>
<span class='add'>- rooms, that link together. map is another room. other people rearange your room to their ideas.</span>
<br>
<span class='rem'>pay to any address</span>
<span class='rem'>clean raw files</span>
<span class='add'>- clean raw files</span>
<br>
<span class='add'>- snapshots (stamps) for version history</span>
<span class='add'>- stamp sets</span>
<span class='rem'>remember the past with version history? = recording state periodically. take pictures before you delete things.</span>
<span class='rem'>how to navigate between stamps -> QoL text editor</span>
<span class='rem'></span>
<br>
<span class='rem'>VR mode, 2-eye mode and 1-eye mode. + any audio or other output.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>Language produces copies of the mind</span>
<span class='rem'>LLMs could produce copies of my mind</span>
<span class='rem'>I want to copy myself into the environment to become the environment</span>
<span class='rem'>I want to see spaces other than mine</span>
<span class='rem'>I want to see fundamentally new spaces that will reveal more about the nature of the world.</span>
<span class='rem'></span>
<span class='rem'>links / structures</span>
<span class='rem'>- memory and pointer to memory, where the user/computer currently needs it</span>
<span class='rem'>- original file structure, user-defined structure, can't hide anything</span>
<br>
<span class='add'>- augmented file browser? for site navigation</span>
<br>
<span class='add'>files:</span>
<span class='rem'>to store efficiently, pieces need to be identified. so each file links to underlying pieces that are "unstructrued". every piece may have a timestamp and optional creator logged with them.</span>
<span class='rem'></span>
<span class='rem'>when mentioning someone, consider sending them a notification. their server could maintain a feed of mentions.</span>
<span class='rem'></span>
<span class='rem'>the software itself could be rooms</span>
<span class='rem'>software only renders + publishes text + offers API (+console?) + template loading (stamping)</span>
<span class='rem'>structure, style is files.</span>
<span class='rem'>"extracts" itself, puts some basic files where it is.</span>
<span class='rem'></span>
<span class='rem'>exe</span>
<span class='add'>- exe</span>
<br>
<span class='rem'>documentation</span>
<span class='add'>- documentation</span>
<br>
<span class='rem'>index note</span>
<span class='add'>- index note</span>
<br>
<span class='rem'>"template note"</span>
<span class='add'>- "template note"</span>
<br>
<span class='rem'>settings - contains "shortcuts" for stamping, style</span>
<span class='add'>- settings - contains "shortcuts" for stamping, style</span>
<br>
<span class='rem'>links / icons, images, font</span>
<span class='add'>- links / icons, images, font</span>
<br>
<span class='rem'>augmented file browser? for site navigation</span>
<span class='add'>non-shifting text editor, more like a canvas?</span>
<span class='add'>marking makes a perfect square. font maybe monospace</span>
<br>
<span class='rem'>things like ctrl-o to home page and having the most recent note selected that is not the one where I came from is not possible with liquid templating language.</span>
<span class='rem'></span>
<span class='rem'>the software will build on the stuff that is already there and then there will be a "noob language" for people to make their pages.</span>
<span class='rem'>alternatively, the software should "just work", and be simple enough so any serious person can learn and modify easily.</span>
<span class='rem'>ensuring infrastructure to optimally create notes in markdown.</span>
<span class='rem'></span>
<span class='rem'>feature to edit the page should also exist client-side.</span>
<span class='rem'></span>
<span class='rem'>WYSIWYG</span>
<span class='rem'></span>
<span class='rem'>result is essentially "markdown applications".</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>non-shifting text editor that is omega intuitive</span>
<span class='rem'>draw a square to write in</span>
<span class='rem'>no adding to the past. the past is also fixed</span>
<span class='rem'>only deletion</span>
<span class='rem'>and saving of snapshots / history</span>
<span class='rem'></span>
<span class='rem'>marking things actually works as it should. marks a square</span>
<span class='rem'></span>
<span class='rem'>draw rectangles to write in?</span>
<span class='rem'></span>
<span class='rem'>use a different way to save it, use compression, which should produce minimal overhead over normal text, but not in memory</span>
<span class='add'>use a different way to save it, use compression, which should produce minimal overhead over normal text, but not in memory while editing</span>
<br>
<span class='rem'>box to the right, is new block?</span>
<span class='rem'>box to left deletes blocks?</span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='rem'>- add br after list if no empty line, to avoid making it part of li</span>
<span class='add'>- other static site generator to render tables, lists correctly without waiting for an empty line at the end</span>
<br>
<span class='rem'>- html table support</span>
<span class='rem'></span>
<span class='rem'></span>
<br>
<span class='add'>[markdown link to exe](https://stackoverflow.com/questions/32563078/how-link-to-any-local-file-with-markdown-syntax)</span>
<span class='add'>[git flavored markdown](https://docs.gitlab.com/ee/user/markdown.html)</span>
<span class='rem'>aspirational process:</span>
<span class='rem'>- write markdown, otherwise just enter files, press publish</span>
<span class='rem'>- hosted locally or on some other computers via open market. the market is open because it is omega ez to enter it. offers have scope so someone can just host his friend for free too</span>
<span class='rem'>- pay via blockchain or credit card or however the host accepts it</span>
<span class='rem'>- discoverable via domain+gateway (possibly through API) or some browsers + internally discoverable through quoting</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-28-13:40'>2024 05 28 13:40</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [1. Generating, viewing](#1.%20Generating,%20viewing)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [2. Sharing](#2.%20Sharing)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Text editor](#Text%20editor)</span>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Performance optimization?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [ProtoSchool I PFS](https://proto.school/course/ipfs)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPNS</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- libp2p</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- Omega simple client</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- **Omega simple client**</span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Stream structure design</span>
<div class='indent'>
<span class='rem'>refining means passing through the blocks from different angles, connecting them and testing for consisency.</span>
<span class='add'>refining means passing through the blocks from different angles, connecting them and testing for consistency. they may fragment into smaller blocks to become more reusable</span>
<br>
<span class='rem'>they may fragment into smaller blocks to become more reusable</span>
<br>
<span class='rem'>As the story becomes refined, its symbols approach realism for maximally dense and applicable representation. Finally, the true story is indistinguishable from life.</span>
<br>
<span class='add'>As the story becomes refined, its symbols approach realism for maximally dense and applicable representation. Eventually, the true story is indistinguishable from life.</span>
<br>
<span class='rem'>giving credit encourages story development: payment and quotes (also a form of payment)</span>
<span class='add'>Importantly, the goal is not story generation, but playing a long and interesting game. Making sharing&nbsp;&nbsp;&nbsp;&nbsp;of discoveries, offers, uncertainties, plans, progress and spheres of mind context easy means offering a more synchronized, everybody-on-the-front-line discovery tour of the universe.</span>
<span class='add'>Looking for an environment that responds back. Video games respond. It may be my fault that my environment does not very much respond. I seek a group to create things. I don't know where to find it.</span>
<span class='add'>The current path says "Working on something to build groups, and large ones, I may find a group and then the tools may enable the group to grow".</span>
<span class='add'></span>
<span class='add'>In exchange for participation, one gets response. Who feels unable or unwilling to respond meaningfully, can offer reach, credit or money.</span>
<span class='rem'>a spirit stream should also platform conversations to enable broader consistency.</span>
<br>
<span class='add'>a spirit stream should also platform direct conversations to further consistency.</span>
<br>
<span class='rem'>- the structure->blocks->stories the author is building through the stream (sculpture)</span>
<span class='add'>- the structure->blocks->stories the author is building through the stream (sculpture). Maybe the structure is a physical tool or a space of "pinned" ideas to repeat going through and refining them.</span>
<br>
<span class='rem'>a history of the stream maintains transparency and a ramp for new readers. It is base reality, should not be lost.</span>
<span class='add'>Tools that help:</span>
<span class='add'>- version history to keep the present clean without information loss, building a ramp for new readers, transparency and base reality.</span>
<br>
<span class='rem'>if reuse is easy enough, cooperation works without shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='add'>if reuse is easy enough, cooperation works without shared files which would require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='rem'>conversation can be public too and seamlessly enter the spirit stream. if version history exists, conversations can be modified without hiding the truth.</span>
<span class='add'>conversation can optionally be public and seamlessly enter the spirit stream.</span>
<br>
<span class='rem'>easily and securely host quotable, changing information, optionally anonymously, without requiring external services.</span>
<br>
<span class='add'>easily and securely host quotable, changing information, optionally anonymously, without middlemen.</span>
<br>
<span class='add'>a beautiful tool has few, clear components that allow diverse structures.</span>
<br>
<span class='rem'>1. Stream input device</span>
<span class='add'>1. Generating, viewing</span>
<div class='indent'>
<span class='add'></span>
<br>
<span class='rem'>streaming is cheap. maintaining and sorting the past is expensive. some still do it to share efficiently.</span>
<br>
<span class='add'>streaming is cheap. maintaining and sorting the past is expensive (articles, movies).</span>
<br>
<span class='add'>Only a BCI is a true stream.</span>
<span class='add'>Language is "stamping" of symbols to represent experience. images / video are high-res one-time stamps.</span>
<span class='add'>Experience become "stamps" on a continuum.</span>
<span class='add'>But stamps only work if the reader can interpret them.</span>
<span class='add'>Direct experience in a responsive environment precedes the need for stamps. Stamps likely can include everything.</span>
<span class='add'>"stamps" = "tokens". subword or multiword.</span>
<br>
<span class='add'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='add'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='rem'>2. Stream format</span>
<span class='rem'>live video, transcribed audio, one file type (video with markdown subtitles?), 3D video, 360 video? and any other files, raw text or markdown for messages.</span>
<span class='rem'>old versions, compressed</span>
<br>
<span class='rem'>convert to web friendly formats HTML XML for compatibility</span>
<span class='rem'>+original files</span>
<span class='add'>content</span>
<span class='add'>vocabulary</span>
<span class='add'>rendering engine / interface</span>
<br>
<span class='rem'>format / protocol must support interaction</span>
<span class='add'>solution should work on scales between small screen / VR desktop/video / BCI</span>
</div>
<span class='add'>2. Sharing</span>
<div class='indent'>
<span class='rem'>3. Stream publishing</span>
<span class='rem'>no middleman, instant, one-button publishing of various files + history</span>
<span class='rem'>see raw files in explorer + renderer + default page?</span>
<span class='add'>on continuum of scope</span>
<span class='add'>I offer information passively.</span>
<span class='add'>Then emulate minds of others to send them what they may want and how they may want it.</span>
<span class='add'></span>
<span class='add'>convert to sharing-friendly formats (SEO) + using existing rendering engines on client + client oriented formats</span>
<span class='add'></span>
<span class='add'>live streaming is high res. can interface with other streamers and through chat, which is a collaborative environment, which instantly gets messy because it is shared. has some moderation.</span>
<span class='add'>stream in a sense opens up for direct messaging relevant to the current situation.</span>
<span class='add'>can be a "mode", a different filter for interaction. No tax emails, only in scope for current thing.</span>
<span class='add'>then negotiate what is shared, what is private. can be specified in message through scope.</span>
<span class='add'>scope selector.</span>
<span class='add'>live stream chat is a scope to avoid the scope selector.</span>
<span class='add'>maybe because live opens up a lot to relevance.</span>
<span class='add'>live interaction reveals true complexity of live hive mind</span>
<span class='add'>seems like the zoom-problem, where 20 people are forced to listen to 1 and can't fragment easily.</span>
<span class='add'></span>
<span class='add'>find quotes and edits (optional)</span>
<span class='add'></span>
<span class='add'>pay for sharing/viewing/donation</span>
<span class='add'></span>
<span class='add'>scrape other sites/RSS</span>
<span class='add'>send advertisement with some compensation</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>WYSIWYG</span>
<span class='add'>payment + PAYG</span>
<span class='add'>automation - scraping is easy - means you pay per visit or provider finances any visits and hopes for donations or profit from somewhere else</span>
<span class='add'>smoothe scope selection for sharing. opens messaging, todolists, cloud service, calendar, groups, global sharing</span>
<span class='add'>host any file type</span>
<span class='add'>viewer for some files (html, md, images, video, sound)</span>
<span class='add'>analytics</span>
<span class='rem'></span>
<span class='rem'>decide scope of recipients for any document, optionally send a copy to a person instead of just making it available.</span>
<span class='rem'>live stream chat vs messages vs email vs publishing passively?</span>
<span class='rem'></span>
<span class='rem'>4. Stream downloading</span>
<span class='rem'>RSS/subscribing, live stream</span>
<span class='rem'></span>
<span class='rem'>5. Stream download fomatting</span>
<span class='rem'>original files</span>
<span class='rem'></span>
<span class='rem'>6. Stream output device</span>
<span class='rem'>scale between small screen and VR desktop/video</span>
</div>
</div>
<span class='hdg'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>C preprocessor</span>
<span class='add'>embedded programming</span>
<span class='add'>`puts` function for errors?</span>
<span class='add'>LLVM / clang? LLVM has a fuzzer</span>
<span class='add'>make files for easy compilation</span>
<span class='add'></span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>- ~~packetriot for tunneling (?)~~ use [port forwarding](https://www.quora.com/Can-we-live-a-localhost-server-website-to-the-internet-If-so-how) instead</span>
<br>
<span class='add'>- [port forwarding](https://www.quora.com/Can-we-live-a-localhost-server-website-to-the-internet-If-so-how)</span>
<br>
<span class='add'>&lt;br>&lt;br>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)&lt;br>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='add'></span>
<span class='add'>```powershell</span>
<span class='add'>gcc main.c -o test.exe -I include/ -L lib/ -lraylib -lgdi32 -lopengl32 -lwinmm</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
<span class='add'>Text editor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>text editor basic features:</span>
<span class='add'>- scrolling (minimap?)</span>
<span class='add'>- set cursor to some spot and start typing</span>
<span class='add'>- marking text (mouse or keys)</span>
<span class='add'>- deleting text, replacing it or moving it</span>
<span class='add'>- copy pasting text</span>
<span class='add'>- cmd + a to select all</span>
<span class='add'>- END and START keys</span>
<span class='add'>- scroll keys</span>
<span class='add'>- infinite text files</span>
<span class='add'>- nice line breaks</span>
<span class='add'>- saving and opening an .md file</span>
<span class='add'>- markdown features</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- text formatting</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- images/video/audio</span>
<span class='add'>- mathjax</span>
<span class='add'>- QoL features</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- alt + arrows moves lines</span>
<span class='add'></span>
<span class='add'>- language control / API: AI can read/write to the thing</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- local AI for message filtering / summarizing / finding new content / filtering social interaction</span>
<span class='add'>- version history</span>
<span class='add'>- direct messages / calls</span>
<span class='add'>- export to web format</span>
<span class='add'>- publish through IPFS</span>
<span class='add'>- quoting</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- quoting is public if the place where to quote lives is</span>
<span class='add'>- payment</span>
<span class='add'></span>
<span class='add'>- shortcut for opening documents, press tab for options. no distraction by defautl</span>
<span class='add'>- arange files and show / not show information like shared to how many</span>
<span class='add'>- show large files or in "Detail" mode like in file browser.</span>
<span class='add'></span>
<span class='add'>- split window</span>
<span class='add'>- making glyphs / emoticons / "stamps" for images, sounds</span>
<span class='add'></span>
<span class='add'>- explore page so that people can actually start from 0 with their content</span>
<span class='add'></span>
<span class='add'>- Ctrl+O open files explorer, press tab, gives the whole thing + recently used ones</span>
<span class='add'>- moodboard is in file explorer? move stuff in a grid if you like. otherwise free movement. "create views in the explorer".</span>
<span class='add'>- set background images in explorer</span>
<span class='add'>- save things to "playlists" ie folders or notes(pages)</span>
<span class='add'></span>
<span class='add'>- entry</span>
<span class='add'>- open</span>
<span class='add'>- outline</span>
<span class='add'>- minimap</span>
<span class='add'>- text</span>
<span class='add'></span>
<span class='add'>- highlight published files that have unpublished changes</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>filter the shit out!</span>
<span class='add'>stamp things endlessly at any size. stamp onetime = images, video, audio, pixels. arbitrary stamp location.</span>
<span class='add'>make available with scope</span>
<span class='add'>direct messages, programmatic too, like notifications</span>
<span class='add'>rooms, that link together. map is another room. other people rearange your room to their ideas.</span>
<span class='add'>pay to any address</span>
<span class='add'>clean raw files</span>
<span class='add'>remember the past with version history? = recording state periodically. take pictures before you delete things.</span>
<span class='add'>how to navigate between stamps -> QoL text editor</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>virtually, we teleport. rooms are so disjointed that it isn't obvious what walking would even look like.</span>
<span class='add'>even "zooms" like a map are teleports.</span>
<span class='add'></span>
<span class='add'>VR mode, 2-eye mode and 1-eye mode. + any audio or other output.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>Language produces copies of the mind</span>
<span class='add'>LLMs could produce copies of my mind</span>
<span class='add'>I want to copy myself into the environment to become the environment</span>
<span class='add'>I want to see spaces other than mine</span>
<span class='add'>I want to see fundamentally new spaces that will reveal more about the nature of the world.</span>
<span class='add'></span>
<span class='add'>links / structures</span>
<span class='add'>- memory and pointer to memory, where the user/computer currently needs it</span>
<span class='add'>- original file structure, user-defined structure, can't hide anything</span>
<span class='add'>- software gives layer on the structure, shows "unused files in the directory" if they are not integrated. see backlinks too, which includes external ones (quotes)</span>
<span class='add'></span>
<span class='add'>to store efficiently, pieces need to be identified. so each file links to underlying pieces that are "unstructrued". every piece may have a timestamp and optional creator logged with them.</span>
<span class='add'></span>
<span class='add'>when mentioning someone, consider sending them a notification. their server could maintain a feed of mentions.</span>
<span class='add'></span>
<span class='add'>the software itself could be rooms</span>
<span class='add'>software only renders + publishes text + offers API (+console?) + template loading (stamping)</span>
<span class='add'>structure, style is files.</span>
<span class='add'>"extracts" itself, puts some basic files where it is.</span>
<span class='add'></span>
<span class='add'>exe</span>
<span class='add'>documentation</span>
<span class='add'>index note</span>
<span class='add'>"template note"</span>
<span class='add'>settings - contains "shortcuts" for stamping, style</span>
<span class='add'>links / icons, images, font</span>
<span class='add'></span>
<span class='add'>augmented file browser? for site navigation</span>
<span class='add'></span>
<span class='add'>things like ctrl-o to home page and having the most recent note selected that is not the one where I came from is not possible with liquid templating language.</span>
<span class='add'></span>
<span class='add'>the software will build on the stuff that is already there and then there will be a "noob language" for people to make their pages.</span>
<span class='add'>alternatively, the software should "just work", and be simple enough so any serious person can learn and modify easily.</span>
<span class='add'>ensuring infrastructure to optimally create notes in markdown.</span>
<span class='add'></span>
<span class='add'>feature to edit the page should also exist client-side.</span>
<span class='add'></span>
<span class='add'>WYSIWYG</span>
<span class='add'></span>
<span class='add'>result is essentially "markdown applications".</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>non-shifting text editor that is omega intuitive</span>
<span class='add'>draw a square to write in</span>
<span class='add'>no adding to the past. the past is also fixed</span>
<span class='add'>only deletion</span>
<span class='add'>and saving of snapshots / history</span>
<span class='add'></span>
<span class='add'>marking things actually works as it should. marks a square</span>
<span class='add'></span>
<span class='add'>draw rectangles to write in?</span>
<span class='add'></span>
<span class='add'>use a different way to save it, use compression, which should produce minimal overhead over normal text, but not in memory</span>
<span class='add'></span>
<span class='add'>box to the right, is new block?</span>
<span class='add'>box to left deletes blocks?</span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>2024-05-21 11:45</span>
<span class='add'>host a website on traditional web:</span>
<span class='add'>- html, css, js / static site generator, md</span>
<span class='add'>- hosting static / dynamic</span>
<span class='add'>- domain</span>
<span class='add'>- extras: streaming, version control, easy deploy + its all fucking expensive</span>
<span class='add'></span>
<span class='add'>hosting on decentralized web:</span>
<span class='add'>- html, css, js / static site generator, md</span>
<span class='add'>- host locally / pay pinning service</span>
<span class='add'>- web3 gateway</span>
<span class='add'>- some blockchain</span>
<span class='add'>- wallet</span>
<span class='add'></span>
<span class='add'>aspirational process:</span>
<span class='add'>- write markdown, otherwise just enter files, press publish</span>
<span class='add'>- hosted locally or on some other computers via open market. the market is open because it is omega ez to enter it. offers have scope so someone can just host his friend for free too</span>
<span class='add'>- pay via blockchain or credit card or however the host accepts it</span>
<span class='add'>- discoverable via domain+gateway (possibly through API) or some browsers + internally discoverable through quoting</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-18-22:30'>2024 05 18 22:30</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>- what does apache do extra compared to simple http server</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- ssl?</span>
<span class='rem'>- after cleaning up, view end-to-end spirit stream components, decide where to move next</span>
<span class='add'>- assume that there is text/images/video to publish and make it simple to publish</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- decentralized file sharing.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- cryptography </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Elliptic Curve Cryptography](https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Karpathy - A from-scratch tour of Bitcoin in Python](https://karpathy.github.io/2021/06/21/blockchain/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Ethereum, dApps</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPFS</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Performance optimization?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Omega simple client</span>
<span class='add'>- live stream capability</span>
<span class='add'>- *Spooky Features/Structure TBD*</span>
<span class='add'>- testing, "shipping"</span>
<span class='add'>- recording and output device robots</span>
<span class='add'></span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Stream structure design</span>
<div class='indent'>
<span class='add'>sight, hearing, touch, smell, taste + emotion, thought, association</span>
<span class='add'>until BCI, internal state is expressed indirectly and can be captured with sight and hearing.</span>
<span class='add'>unobtrusive cam, mic, battery and charging. headband for first person, otherwise smartphone?</span>
<span class='add'></span>
<span class='add'>it is inherently too much, needs filtering</span>
<span class='add'>streaming is cheap. maintaining and sorting the past is expensive. some still do it to share efficiently.</span>
<span class='add'></span>
<span class='add'>- bci</span>
<span class='add'>- ar recording - optionally see live video + ar/vr overlay, otherwise just sits there</span>
<span class='add'>- spider hat - includes vr recording. could get bodies for walking, swimming, flying. general legs it can walk with.</span>
<span class='add'>- phone</span>
<span class='add'></span>
<br>
<span class='add'>live video, transcribed audio, one file type (video with markdown subtitles?), 3D video, 360 video? and any other files, raw text or markdown for messages.</span>
<span class='add'>old versions, compressed</span>
<span class='add'></span>
<span class='add'>convert to web friendly formats HTML XML for compatibility</span>
<span class='add'>+original files</span>
<span class='add'></span>
<span class='add'>format / protocol must support interaction</span>
<span class='add'></span>
<span class='add'></span>
<br>
<span class='add'>no middleman, instant, one-button publishing of various files + history</span>
<span class='add'>see raw files in explorer + renderer + default page?</span>
<span class='add'></span>
<span class='add'>possibly P2P network and optionally hosting on other machines</span>
<span class='add'>optionally anonymously.</span>
<span class='add'></span>
<span class='add'>decide scope of recipients for any document, optionally send a copy to a person instead of just making it available.</span>
<span class='add'>live stream chat vs messages vs email vs publishing passively?</span>
<span class='add'></span>
<br>
<span class='add'>RSS/subscribing, live stream</span>
<span class='add'></span>
<br>
<span class='add'>original files</span>
<span class='add'></span>
<br>
<span class='add'>scale between small screen and VR desktop/video</span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='hdg'>2024-05-15 08:28 Proto spirit stream</span>
<div class='indent'>
<span class='hdg'>2024-05-10 12:47 [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
<div class='indent'>
<span class='rem'>|Traditional stack|Decentralized stack|</span>
<span class='rem'>|---|---|</span>
<span class='rem'>|Banking system|ETH, stablecoins, L2s for payments, DEXes (note: still need banks for loans)|</span>
<span class='rem'>|Receipts|Links to transactions on block explorers|</span>
<span class='rem'>|Corporations|DAOs|</span>
<span class='rem'>|DNS (`.com`, `.io`, etc)|[ENS](https://ens.domains) (`.eth`)|</span>
<span class='rem'>|Regular email|Encrypted email (eg. [Skiff](https://skiff.com/))|</span>
<span class='rem'>|Regular messaging (eg. Telegram)|Decentralized messaging (eg. [Status](https://status.app/))|</span>
<span class='add'>| Traditional stack&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Decentralized stack&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='add'>| ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |</span>
<span class='add'>| Banking system&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| ETH, stablecoins, L2s for payments, DEXes (note: still need banks for loans)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</span>
<span class='add'>| Receipts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Links to transactions on block explorers&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</span>
<span class='add'>| Corporations&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| DAOs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</span>
<span class='add'>| DNS (`.com`, `.io`, etc)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| [ENS](https://ens.domains) (`.eth`)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='add'>| Regular email&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Encrypted email (eg. [Skiff](https://skiff.com/))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='add'>| Regular messaging (eg. Telegram)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Decentralized messaging (eg. [Status](https://status.app/))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='rem'>|Sign in with Google, Twitter, Wechat|[Sign in with Ethereum](https://login.xyz/), Zupass, Attestations via [EAS](https://attest.sh/), POAPs, Zu-Stamps... + [social recovery](https://vitalik.eth.limo/general/2021/01/11/recovery.html)|</span>
<br>
<span class='add'>| Sign in with Google, Twitter, Wechat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| [Sign in with Ethereum](https://login.xyz/), Zupass, Attestations via [EAS](https://attest.sh/), POAPs, Zu-Stamps... + [social recovery](https://vitalik.eth.limo/general/2021/01/11/recovery.html) |</span>
<br>
<span class='rem'>|Publishing blogs on Medium, etc|Publishing self-hosted blogs on IPFS (eg. using [Fleek](https://app.fleek.co/))|</span>
<span class='rem'>|Twitter, Facebook|[Lens](https://www.lens.xyz/), [Farcaster](https://www.farcaster.xyz/)...|</span>
<span class='rem'>|Limit bad actors through all-seeing big brother|Constrain bad actors through zero knowledge proofs|</span>
<span class='add'>| Publishing blogs on Medium, etc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Publishing self-hosted blogs on IPFS (eg. using [Fleek](https://app.fleek.co/))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='add'>| Twitter, Facebook&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | [Lens](https://www.lens.xyz/), [Farcaster](https://www.farcaster.xyz/)...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |</span>
<span class='add'>| Limit bad actors through all-seeing big brother | Constrain bad actors through zero knowledge proofs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</span>
<span class='add'></span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-15-08:45'>2024 05 15 08:45</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='rem'>There seems to be way living inward, but genuinely interesting things come from the unknown. And turning outward seems like a great adventure. Offer synchronization with my brain to the world. Offer trust through transparency. I'd love to see minds, imagine the possible depth.</span>
<span class='rem'>Also curious where this leads: [Learn in public](https://www.swyx.io/learn-in-public).</span>
<span class='add'>There seems to be way living inward, but genuinely interesting things come from the unknown. And turning outward seems like a great adventure. I am curious to share and visit minds. The spirit stream could become the tool.</span>
<span class='add'></span>
<span class='add'>i can see the hive mind on the horizon.</span>
<span class='add'>there are hammers, sculptors, hammer blows and sculptures.</span>
<span class='add'>grouped and isolated</span>
<span class='add'>places to stay.</span>
<span class='add'></span>
<span class='add'>like gazing into a painting, being guided through the impression</span>
<span class='add'>coming along to find a question asked at myself, seeing it unfold and building a response</span>
<span class='add'></span>
<span class='add'>stream = raw information I choose to share</span>
<span class='add'>spirit = meta being(s) behind the stream</span>
<span class='add'>Temple = access port to the spirits</span>
<span class='add'></span>
<span class='add'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='add'>2. It meets the reader where it can expand him</span>
<span class='add'></span>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Proto spirit stream](#Proto%20spirit%20stream)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit streams](#Spirit%20streams)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#Spirit%20stream%20temple%20vision)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Research](#Research)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [P2P networks](#P2P%20networks)</span>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>- what does apache do extra compared to simple http</span>
<span class='add'>- what does apache do extra compared to simple http server</span>
<br>
<span class='rem'>- how do p2p networks work?</span>
<span class='add'>- after cleaning up, view end-to-end spirit stream components, decide where to move next</span>
</div>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Spirit stream evolution</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Proto spirit stream</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>It aimed to be maximally accurate. I'm an opaque blob with some projects on the surface.</span>
<span class='rem'>Who am I to categorize my project correctly? Instead, I dream of maps. They speak for themselves and display opportunity.</span>
<span class='rem'>So projects are scattered over the surface, users were able to rotate the blob. The distance between them was determined by "connections" I set manually. Connected blobs attract each other and disconnected ones repell each other.</span>
<span class='rem'>This was meant to lead to a visually quickly and intuitively understood blob distribution.</span>
<span class='rem'></span>
<span class='rem'>bigger spheres = more time spent on the project</span>
<span class='rem'>brighter spheres = newer project</span>
<span class='rem'>thumbnails of proximate spheres that act as buttons to the projects make them more distinct.</span>
<span class='rem'></span>
<span class='rem'>![](/assets/pasted-image-20240123193144.png)</span>
<span class='rem'>example for connections: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me: they all give a more direct image of my mind.</span>
<span class='rem'></span>
<span class='rem'>![](/assets/pasted-image-20240123193439.png)</span>
<span class='rem'></span>
<span class='rem'>In the summary at the beginning of each project page, connected notes (neighbors) were referenced explitly to lay out the structure and provide further reading.</span>
<span class='rem'></span>
<span class='rem'>- No search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<span class='rem'>- requires javascript</span>
<span class='rem'>- inefficient to use, relies on curiosity of visitor, needs to turn the blob to discover everything</span>
<span class='rem'>- information structure too weak to support large number of projects at various levels of resolution (density, thought hours) and versions</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- low benefit from connections at high cost of visual complexity. imagine dozens of project bubbles</span>
</div>
<span class='rem'>Spirit streams</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>![](/assets/pasted-image-20240122203205.png)</span>
<span class='rem'>media format is not the point but is taking much attention and space. Too much information. More relevant is what area was changed, maybe what exactly was changed.</span>
<span class='rem'></span>
<span class='rem'>![](/assets/pasted-image-20240126212550.png)</span>
<span class='rem'>history button is not necessary, rarely used I imagine.</span>
<span class='rem'>click on "updated 10 hours ago" to see what exactly has changed in the last update. easy with `difflib`.</span>
<span class='rem'></span>
<span class='rem'>history page could get more stats, person on history page is looking for it</span>
<span class='rem'>show *how much* each area changed and also show deletions</span>
<span class='rem'>![](/assets/pasted-image-20240126212515.png)</span>
<span class='rem'></span>
<span class='rem'>version control</span>
<span class='rem'>- record brain in action, not just result</span>
<span class='rem'>- keep the current notes tidy without deleting anything forever</span>
<span class='rem'>- show when something is new</span>
<span class='rem'></span>
<span class='rem'>better navigation. make it subtle on the side somehow. Mouseover to increase contrast?</span>
<span class='rem'>collect some ideas online</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>Version 2024 03 07:</span>
<span class='rem'>![](/assets/20240307-spirit-stream.png)</span>
<span class='rem'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='rem'>- where is the history?</span>
<span class='rem'>- the newest update column provides only garbage information, who cares if I fixed a typo? It should reflect the actual content not an abstracted commit message I made up.</span>
<span class='rem'>- why is git storing my files in this unreadable way?</span>
<span class='rem'>- who decided to make links suddenly be red?</span>
<span class='rem'>- why can't I tell when I already visited them?</span>
<span class='rem'>- is it an external or internal link?</span>
<span class='rem'>- can't tell the difference between headings</span>
<span class='rem'>- who needs these lines between notes in the table?</span>
<span class='rem'>- how do I see only what was recently changed?</span>
<span class='rem'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='rem'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
<span class='rem'></span>
<span class='rem'>2024-05-01 20:40:</span>
<span class='rem'>![](Pasted%20image%2020240501203222.png)</span>
</div>
</div>
<span class='add'>Spirit stream evolution</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240126212515.png)</span>
<span class='add'>history page with headings where something was updated</span>
<span class='add'></span>
<span class='add'>Version 2024 03 07:</span>
<span class='add'>![](/assets/20240307-spirit-stream.png)</span>
<span class='add'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='add'>- where is the history?</span>
<span class='add'>- the newest update column provides only garbage information, who cares if I fixed a typo?</span>
<span class='add'>- who decided to make links suddenly be red?</span>
<span class='add'>- why can't I tell when I already visited them?</span>
<span class='add'>- is it an external or internal link?</span>
<span class='add'>- can't tell the difference between headings</span>
<span class='add'>- how do I see only what was recently changed?</span>
<span class='add'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='add'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
<span class='add'></span>
<span class='add'>2024-05-01 20:40:</span>
<span class='add'>![](Pasted%20image%2020240501203222.png)</span>
<span class='add'></span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='rem'>Spirit stream vision</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>stream = raw information I choose to share</span>
<span class='rem'>spirit = meta being(s) behind the stream</span>
<span class='rem'>Temple = access port to the spirits</span>
<span class='rem'></span>
<span class='rem'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='rem'>2. It meets the reader where it can expand him</span>
<span class='rem'></span>
<span class='rem'>like a field where it is easy to identify parts that are newly planted and parts that are more grown</span>
<span class='rem'>like a map that intuitively lays out potential and the path to it.</span>
</div>
<span class='hdg'>Stream structure design</span>
<div class='indent'>
<span class='rem'>visitors should easily be able to </span>
<span class='rem'>- read</span>
<span class='rem'>- quote</span>
<span class='rem'>- pay</span>
<span class='rem'>- contact</span>
<span class='rem'>the host.</span>
<span class='add'>I am an information structuring engine</span>
<span class='add'>continuous stream picks up and repeats patterns, initially chaotic and far-reaching, then filtered, refined to form few knowledge blocks in areas of interest, easy to build on.</span>
<span class='add'>refining means passing through the blocks from different angles, connecting them and testing for consisency.</span>
<span class='add'>they may fragment into smaller blocks to become more reusable</span>
<span class='add'>a concise, stable a path through the blocks - the ultimate test of their consistency - is a true story.</span>
<span class='add'>As the story becomes refined, its symbols approach realism for maximally dense and applicable representation. Finally, the true story is indistinguishable from life.</span>
<br>
<span class='add'>Like entering the wild, refining collected information is looking for gems.</span>
<span class='add'></span>
<span class='add'>sharing stories, I test their consistency</span>
<span class='add'>a true story is recognized and directly enteres the recipient, who tests it for consistency in their own mind.</span>
<span class='add'>it has become an effective meme.</span>
<span class='add'></span>
<span class='add'>joscha bach streams in non-integrated posts</span>
<span class='add'>the structure is hidden but accessible</span>
<span class='add'>what is the preferred interface to the hive mind at different distances into the future?</span>
<span class='add'></span>
<span class='add'>giving credit encourages story development: payment and quotes (also a form of payment)</span>
<span class='add'>a spirit stream should also platform conversations to enable broader consistency.</span>
<span class='add'></span>
<span class='add'>In summary, the spirit stream has two parts: </span>
<span class='add'>- the stream (hammer blows)</span>
<span class='add'>- the structure->blocks->stories the author is building through the stream (sculpture)</span>
<span class='add'>its development is furthered by:</span>
<span class='add'>- enabling abitrary direct exchange (optionally entering the stream)</span>
<span class='add'>- forms of payment, an emergent "meme market"</span>
<span class='add'></span>
<span class='add'>a history of the stream maintains transparency and a ramp for new readers. It is base reality, should not be lost.</span>
<span class='add'></span>
<span class='rem'>if using what someone else built is easy enough, cooperation is easy too without using shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='add'>if reuse is easy enough, cooperation works without shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<br>
<span class='rem'>1. I put out there what I build as I build it in whatever categories I prefer</span>
<span class='rem'>2. people build on it, input information, quoting specificly or making a general comment. -> Inbox is another note and the response may quote part of another note</span>
<span class='rem'>3. anyone may pay a contribution</span>
<span class='rem'>4. version history shows changes, comments, deleted content</span>
<span class='rem'></span>
<span class='rem'>securely host quotable, changing information, maybe anonymously or almost anonymously, without requiring external services.</span>
<br>
<span class='add'>easily and securely host quotable, changing information, optionally anonymously, without requiring external services.</span>
<br>
<span class='add'>1. Stream input device</span>
<span class='add'>2. Stream format</span>
<span class='add'>3. Stream publishing</span>
<span class='add'>4. Stream downloading</span>
<span class='add'>5. Stream download fomatting</span>
<span class='add'>6. Stream output device</span>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>wikipedia</span>
<span class='add'>tor</span>
<span class='add'>blockchain</span>
<span class='add'>interplanetary file system</span>
<span class='add'></span>
<span class='add'>"Digital gardens"&lt;br>[https://github.com/MaggieAppleton/digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners)&lt;br>[https://simonewebdesign.it/](https://simonewebdesign.it/)</span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>- live stream (?)</span>
<br>
<span class='add'>- live stream</span>
<br>
<span class='rem'>true spirit stream streams files of any kind to worldwide availability</span>
<span class='rem'></span>
<span class='add'>P2P networks</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[P2P networks Introduction](https://www.youtube.com/watch?v=Vw9ynzuGNSw) P2P security challenges:</span>
<span class='add'>- how to avoid eaves-dropping</span>
<span class='add'>- how to avoid peer identity impersonation</span>
<span class='add'>- how to avoid fabrication</span>
<span class='add'>- how to avoid replay attacks</span>
<span class='add'>- how to provide peer anonymity</span>
<span class='add'></span>
</div>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'>llms contain linguistic maps. they can draw the landscape for me if they know my maps too.</span>
<span class='add'>a language model that has not learned to answer questions tells stories.</span>
<span class='rem'>- Discord Server / Email contact like a cafe where people would meet me</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- fake aesthetik. discord server is what it functionally is. what is it? people will know?</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- how to make it easy to reach me and know that it is an actual place, not just a dead contact form?</span>
<span class='rem'>- Guide gaze like in a painting, comprehensive visual impression</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- i wish. what is the structure of information? what is the map to represent the information. Was in a buddha museum recently. figures, reliefs. They spoke a language I didn't understand (the images) and it was accordingly boring. How to communicate in high resolution without aliening people and forming an "in group". Should be possible as information is ideally presented to each person differently based on past experience/knowlege/predisposition. </span>
<br>
<span class='add'>TODO:</span>
<span class='add'>- choose a more sensitive font</span>
<span class='rem'>where does the history graph go when notes are refactored? was it merged into somewhere else?</span>
<span class='rem'></span>
<span class='rem'>"Digital gardens"&lt;br>[https://overreacted.io/](https://overreacted.io/)&lt;br>[https://github.com/MaggieAppleton/digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners)&lt;br>[https://wiki.nikiv.dev/sharing/everything-I-know](https://wiki.nikiv.dev/sharing/everything-I-know)&lt;br>[https://simonewebdesign.it/](https://simonewebdesign.it/)&lt;br>[https://stephango.com/](https://stephango.com/) (blog)&lt;br>[https://karpathy.github.io/](https://karpathy.github.io/) (blog, jekyll)</span>
<span class='rem'></span>
<span class='rem'>stream is continuous, repeats patterns from the past. the past is filtered and organized, the present it not always organized -> inbox</span>
<span class='rem'>I am an information structuring engine</span>
<span class='rem'>old, high res, crystallized structures are refined and references as we go</span>
<span class='rem'>in them, the structures are hidden</span>
<span class='rem'>like Joscha Bach does it</span>
<span class='rem'></span>
<span class='rem'>as information is refined, a few blocks of interest form.</span>
<span class='rem'>they are reused and every new reuse is a different path through the same blocks connecting and testing them with other ideas and experiences.</span>
<span class='rem'>they may fragment into smaller blocks to become more reusable</span>
<span class='rem'>a path through the blocks that is the ultimate test of their consistency is a true story.</span>
<span class='rem'>A true story is indistinguishable from life</span>
<span class='rem'>a language model that has not learned to answer questions tells stories, but with low refinement. It recounts its experience given a prompt.</span>
<span class='rem'>usually, describing individual blocks becomes too laborious and abstract, so the path through the blocks is the preferred medium.</span>
<span class='rem'>an individual block and its environment might be represented in an artwork, aiming to maximize resolution around this area of the idea landscape.</span>
<span class='rem'>If it is refined enough its essence will be recognized even by those who see the same block in lower resolution in their own thinking.</span>
<span class='rem'>Perhaps this node will set off a story in the observer, automatically connecting itself in the observers network.</span>
<span class='rem'>it has become an effective meme.</span>
<span class='rem'></span>
<span class='rem'>In math and physics perhaps such a block is a formula, a new tool to be applied. The stories it produces are its applications.</span>
<span class='rem'>in mathematics, a game with strict rules (by my understanding), the truth of a block can be verified.</span>
<span class='rem'></span>
<span class='rem'>engineering, of which I mostly know programming, feels like branching off of a given path or some base of information in search of new paths.</span>
<span class='rem'>Reverse engineering is starting from the fog and trying to find the branch of light to clear it.</span>
<span class='rem'></span>
<span class='rem'>when a story takes a "wrong turn", I remain curious about its branches but am swept away.</span>
<span class='rem'>if there is no high resolution node that points to that branch, as there often isn't, I have to walk all the paths that I can find (that the person speaks or writes about) and see if I can explore it on my own from there.</span>
<span class='rem'></span>
<span class='rem'>I need to remember that the world works in true stories and that writing and formalizing, extracting, refining and building blocks is only there to improve future stories.</span>
<span class='rem'>Maybe it is an exploration in its own way. When a lot of information has been picked up but not refined, there may be gems in it.</span>
<span class='rem'></span>
<span class='rem'>It sucks that so many stories are already told but I don't know them. Maybe it is childhood curiosity that leads to these stories. I am, now, not patient enough to read them and instead may waste time reinventing them.</span>
<span class='rem'>However, I must verify them anyway and as long as I feel that there are things to be seen in the world, I must explore it.</span>
<span class='rem'>I must build things to survive and to reach a vantage point.</span>
<span class='rem'>It will be occasionally good to consider the stories so as to avoid mistakes.</span>
<span class='rem'></span>
<span class='rem'>it is a holy mission to build the website</span>
<span class='rem'>it will make my brain accessible like never before and will make my spirits extractable.</span>
<span class='rem'></span>
<span class='rem'>2024-01-26 20:40</span>
<span class='rem'>aesthetic refinement: choose a more sensitive font</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>the stream is pouring into different areas all the time. the challenge is to see those areas instead of only the stream. its like seeing individual hammer blows but not the sculpture. need to see both and at various resolutions. how to translate it?</span>
<span class='rem'>history is made of changes. the more complete the stream becomes, the more the current state will become visible. There is no need to see the whole world at a particular point in time because the world at any moment is no bigger than its changes and if the changes are documented with high resolution, one already sees the world as it was. To achieve this resolution, perceptions need to be recorded too.</span>
<span class='rem'>1. notes</span>
<span class='rem'>2. note changes, chronological</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- will eventually grow *very* big with images, video.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- lazy loading and search</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- or separated into different pages by date (month or year)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- structure</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- date</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- note title</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- heading hierarchy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- changes (pictures are smaller, changes within the line should be visualized compactly)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- note title for new, renamed and deleted notes</span>
<span class='rem'></span>
<span class='rem'>restructure to show "why" first, then title, links, content, work in progress</span>
<span class='rem'>titles and links are parts of the same thing, headings. one has the content next to it, one points to it.</span>
<span class='rem'>headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<span class='add'>- headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<span class='rem'></span>
<span class='rem'>2024-03-07 19:30</span>
<span class='rem'>assuming the structure notes + stream is good.</span>
<span class='rem'>it may reflect blogs/papers/books + X</span>
<span class='rem'>technically, X alone supports this through "highlights", which could be long posts or any other media</span>
<span class='rem'>X does not support editing them or advanced formatting for long posts like code, internal links, heading hierarchy.</span>
<span class='rem'>X offers interaction</span>
<span class='rem'>and distraction</span>
<span class='rem'>here, the scope goes beyond the spirit stream to organizing information. creating rooms for thought, development and exhange.</span>
<span class='rem'>Need to specify further to see how big the upside of such a system is, if it is worth pursuing. Look for literature since this is a long game.</span>
<span class='rem'>This leads towards what I want to use AI for.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>TODO</span>
<span class='rem'>- (design to differentiate internal and external link)</span>
<br>
<span class='add'>- design to differentiate internal and external link)</span>
<span class='rem'>- (add br after list if no empty line, to avoid making it part of li)</span>
<br>
<span class='add'>- add br after list if no empty line, to avoid making it part of li</span>
<span class='add'>- clear heading hierarchy</span>
<span class='add'>- html table support</span>
<span class='rem'>- (add heading hierarchy before every heading for orientation make nice heading symbols - distracting? try out, probably a waste of time, but could add atmosphere.</span>
<span class='rem'></span>
<span class='rem'>general:</span>
<span class='rem'>- (can't tell what number a heading is, add lines before it to indicate hierarchy and make font larger)</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>2024-05-04 12:48</span>
<span class='rem'>There are two parts to the spirit stream:</span>
<span class='rem'>- The streamer publishes information effortlessly and offers options to be quoted, paid and contacted.</span>
<span class='rem'>- The receiver filters the information according to individual preference (extractable from that person's stream)</span>
<span class='rem'></span>
<span class='rem'>Streaming requires one piece of software, some content, then click share. Host on own computer or optionally pay for remote host. Hosting yourself is one button away. Can also choose what to host.</span>
<span class='rem'>When any content can be selectively shared and subscribed to, it gives rise to cloud, file sharing, messaging, blogging, video streaming services.</span>
<span class='rem'>People can pay me directly to view their ads.</span>
<span class='rem'>they can pay me for hosting or compute.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>[P2P networks Introduction](https://www.youtube.com/watch?v=Vw9ynzuGNSw)</span>
<span class='rem'>![](Pasted%20image%2020240504143726.png)</span>
<span class='rem'></span>
<span class='rem'></span>
</div>
<span class='rem'>Research</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>wikipedia</span>
<span class='rem'>tor</span>
<span class='rem'>blockchain</span>
<span class='rem'>interplanetary file system</span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'></span>
<span class='add'>2024-05-15 08:28 Proto spirit stream</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>It aimed to be maximally accurate. I'm an opaque blob with some projects on the surface.</span>
<span class='add'>Who am I to categorize my project correctly? Instead, I dream of maps. They speak for themselves and display opportunity.</span>
<span class='add'>So projects are scattered over the surface, users were able to rotate the blob. The distance between them was determined by "connections" I set manually. Connected blobs attract each other and disconnected ones repell each other.</span>
<span class='add'>This was meant to lead to a visually quickly and intuitively understood blob distribution.</span>
<span class='add'></span>
<span class='add'>bigger spheres = more time spent on the project</span>
<span class='add'>brighter spheres = newer project</span>
<span class='add'>thumbnails of proximate spheres that act as buttons to the projects make them more distinct.</span>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240123193144.png)</span>
<span class='add'>example for connections: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me: they all give a more direct image of my mind.</span>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240123193439.png)</span>
<span class='add'></span>
<span class='add'>In the summary at the beginning of each project page, connected notes (neighbors) were referenced explitly to lay out the structure and provide further reading.</span>
<span class='add'></span>
<span class='add'>- No search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<span class='add'>- requires javascript</span>
<span class='add'>- inefficient to use, relies on curiosity of visitor, needs to turn the blob to discover everything</span>
<span class='add'>- information structure too weak to support large number of projects at various levels of resolution (density, thought hours) and versions</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- low benefit from connections at high cost of visual complexity. imagine dozens of project bubbles</span>
<span class='add'></span>
<br>
<span class='rem'>ethereum whitepaper</span>
<span class='add'>[ethereum whitepaper (original)](https://ethereum.org/content/whitepaper/whitepaper-pdf/Ethereum_Whitepaper_-_Buterin_2014.pdf)</span>
<span class='add'>[ethereum whitepaper (updated?)](https://ethereum.org/en/whitepaper/)</span>
<span class='add'></span>
<span class='add'>futarchy</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-11-12:07'>2024 05 11 12:07</span><div class='indent'>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='rem'>2024-05-10 12:47</span>
<div class='indent'>
<span class='add'>[bitcoin whitepaper](https://bitcoin.org/bitcoin.pdf)</span>
<span class='add'>ethereum whitepaper</span>
<span class='add'>karpathy blockchain implementation</span>
<span class='add'></span>
</div>
<span class='add'>2024-05-11 08:15 [Vitalik Buterin - Coordination, Good and bad](https://vitalik.eth.limo/general/2020/09/11/coordination.html)</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>>One important property of especially the milder cases of collusion is that one cannot determine whether or not an action is part of an undesired collusion just by looking at the action itself.</span>
<span class='add'></span>
<span class='add'>>[...]votes where vote selling is permitted quickly [collapse into plutocracy](https://vitalik.eth.limo/general/2019/04/03/collusion.html).</span>
<span class='add'></span>
<span class='add'>>[...] in [...] _cooperative game theory_, [we can prove that](https://en.wikipedia.org/wiki/Bondareva%E2%80%93Shapley_theorem) there are large classes of games that do not have any stable outcome (called a "[core](https://en.wikipedia.org/wiki/Core_(game_theory))"). In such games, whatever the current state of affairs is, there is always some coalition that can profitably deviate from it.</span>
<span class='add'>>One important part of that set of inherently unstable games is _majority games_. A majority game [is formally described](https://web.archive.org/web/20180329012328/https://www.math.mcgill.ca/vetta/CS764.dir/Core.pdf) as a game of agents where any subset of more than half of them can capture a fixed reward and split it among themselves</span>
<span class='add'></span>
<span class='add'>![](Pasted%20image%2020240511084052.png)</span>
<span class='add'></span>
<span class='add'>> In _[Liars and Outliers](https://books.google.com.sg/books/about/Liars_and_Outliers.html?id=lPsbhIUexo0C&amp;redir_esc=y)_, Bruce Schneier reminds us that many "security systems" (locks on doors, warning signs reminding people of punishments...) also serve a moral function, reminding potential misbehavers that they are about to conduct a serious transgression and if they want to be a good person they should not do that.</span>
<span class='add'></span>
<span class='add'>(decentralization contains a warning that any attempt of centralization is unwelcome? However, consider the Ministry of Love)</span>
<span class='add'></span>
<span class='add'>>**Counter-coordination**. The fact that a system is decentralized makes it easy for participants not participating in the collusion to make a fork that strips out the colluding attackers and continue the system from there.</span>
<span class='add'></span>
<span class='add'>mechanisms against collusion:</span>
<span class='add'>- privacy protection</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- secret ballot</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- protection of whistleblowers</span>
<span class='add'>- reward to whistleblowers, encouraging defection in the collusion</span>
<span class='add'>- "Moral barriers" (?)</span>
<span class='add'>- Counter-coordination</span>
<span class='add'>- skin in the game for the colluders</span>
<span class='add'>- Decentralization in physical space</span>
<span class='add'>- Decentralization between role-based constituencies</span>
<span class='add'>- [Schelling points](https://en.wikipedia.org/wiki/Focal_point_(game_theory)), allowing large groups of people to quickly coordinate around a single path forward. Complex Schelling points could potentially even be implemented in code (eg. [recovery from 51% attacks](https://ethresear.ch/t/timeliness-detectors-and-51-attack-recovery-in-blockchains/6925) can benefit from this).</span>
<span class='add'>- Speaking a common language (or alternatively, splitting control between multiple constituencies who speak different languages)</span>
<span class='add'>- Using per-person voting instead of per-(coin/share) voting to greatly increase the number of people who would need to collude to affect a decision</span>
<span class='add'></span>
<span class='add'>>This all leads us to an interesting view of what it is that people building social systems _do_. One of the goals of building an effective social system is, in large part, determining _the structure of coordination_: which groups of people and in what configurations can come together to further their group goals, and which groups cannot?</span>
<span class='add'></span>
<span class='add'>(much of this thought seems unnecessary to me. The structure of reality will reveal itself when the tools allow it. If collusion is the optimal strategy, then so be it. but collusion should not succeed because an alternative was discouraged not technically possible. The job of a social system is to allow anything and everything, then have people build structure on top as they wish: filters, undoing privacy, subspaces. The best system is no system, the best part is no part. Reduce complexity and let people build. if the structure can contain reality it will, and then everyone gets what they deserve which effectively maximizes progress)</span>
<span class='rem'>From [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
</div>
<span class='add'>2024-05-10 12:47 [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
<div class='indent'>
<span class='add'>(= more lifetime is almost only nice)</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>(success requires vision, which is embedded in culture)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-10-13:18'>2024 05 10 13:18</span><div class='indent'>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='add'>2024-05-10 12:47</span>
<div class='indent'>
<span class='add'>From [Vitalik Buterin - The end of my childhood](https://vitalik.eth.limo/general/2024/01/31/end.html)</span>
<span class='add'></span>
<span class='add'>[Zuzalu](https://www.palladiummag.com/2023/10/06/why-i-built-zuzalu/)</span>
<span class='add'></span>
<span class='add'>>[...], I realized that I do not even need to argue any of this. Regardless of whether our lives as a whole are finite or infinite, every single beautiful thing in our lives is finite.</span>
<span class='add'></span>
<span class='add'>[the impossible provably optimal governance system](https://vitalik.eth.limo/general/2020/09/11/coordination.html)</span>
<span class='add'></span>
<span class='add'>>[...]the most important variables that make the difference between existing flawed systems succeeding or failing in practice (often, the degree of coordination between subgroups of participants, but also other things that we often black-box as ["culture"](https://www.writingruxandrabio.com/p/ideas-matter-how-i-stopped-being)</span>
<span class='add'></span>
<span class='add'>>These two events, as different as they are in the type and the scale of their tragedy, both burned into my mind a similar lesson: that I actually have responsibilities in this world, and I need to be intentional about how I operate. Doing nothing, or living on autopilot and letting myself simply become part of the plans of others, is not an automatically safe, or even blameless, course of action.</span>
<span class='add'></span>
<span class='add'>(To me, maybe similar effect from playing Witcher 3, Cyberpunk 2077 or similar because they force decision making and show the results)</span>
<span class='add'> </span>
<span class='add'>|Traditional stack|Decentralized stack|</span>
<span class='add'>|---|---|</span>
<span class='add'>|Banking system|ETH, stablecoins, L2s for payments, DEXes (note: still need banks for loans)|</span>
<span class='add'>|Receipts|Links to transactions on block explorers|</span>
<span class='add'>|Corporations|DAOs|</span>
<span class='add'>|DNS (`.com`, `.io`, etc)|[ENS](https://ens.domains) (`.eth`)|</span>
<span class='add'>|Regular email|Encrypted email (eg. [Skiff](https://skiff.com/))|</span>
<span class='add'>|Regular messaging (eg. Telegram)|Decentralized messaging (eg. [Status](https://status.app/))|</span>
<span class='add'>|Sign in with Google, Twitter, Wechat|[Sign in with Ethereum](https://login.xyz/), Zupass, Attestations via [EAS](https://attest.sh/), POAPs, Zu-Stamps... + [social recovery](https://vitalik.eth.limo/general/2021/01/11/recovery.html)|</span>
<span class='add'>|Publishing blogs on Medium, etc|Publishing self-hosted blogs on IPFS (eg. using [Fleek](https://app.fleek.co/))|</span>
<span class='add'>|Twitter, Facebook|[Lens](https://www.lens.xyz/), [Farcaster](https://www.farcaster.xyz/)...|</span>
<span class='add'>|Limit bad actors through all-seeing big brother|Constrain bad actors through zero knowledge proofs|</span>
<span class='add'>>[...]a major missing piece from this stack is democratic governance technology.</span>
<span class='add'></span>
<span class='add'>Carbonvote</span>
<span class='add'>Gitcoin</span>
<span class='add'></span>
<span class='add'>![](Pasted%20image%2020240510130851.png)</span>
<span class='add'></span>
<span class='add'>>Paul Graham has written about how every city [sends a message](https://paulgraham.com/cities.html): in New York, "you should make more money". In Boston, "You really should get around to reading all those books". In Silicon Valley, "you should be more powerful". When I visit Taipei, the message that comes to my mind is "you should rediscover your inner high school student".</span>
<span class='add'></span>
<span class='add'>[network states movement](https://vitalik.eth.limo/general/2022/07/13/networkstates.html)</span>
<span class='add'></span>
<span class='rem'>2024-04-25 13:44</span>
</div>
<span class='add'>2024-04-25 13:44</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-05-18:09'>2024 05 05 18:09</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[P2P networks Introduction](https://www.youtube.com/watch?v=Vw9ynzuGNSw)</span>
<span class='add'>![](Pasted%20image%2020240504143726.png)</span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='hdg'>Research</span>
<div class='indent'>
<span class='rem'>what is wikipedia?</span>
<span class='add'>wikipedia</span>
<span class='add'>tor</span>
<span class='add'>blockchain</span>
<span class='add'>interplanetary file system</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-05-04-13:58'>2024 05 04 13:58</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#Previous%20page%20design%20recap)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [the pragmatist says it sucks](#the%20pragmatist%20says%20it%20sucks)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream evolution](#Spirit%20stream%20evolution)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Proto spirit stream](#Proto%20spirit%20stream)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit streams](#Spirit%20streams)</span>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='add'>- what does apache do extra compared to simple http</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- ssl?</span>
<span class='add'>- how do p2p networks work?</span>
</div>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'>Previous page design recap</span>
<span class='add'>Spirit stream evolution</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Proto spirit stream</span>
<div class='indent'>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Note structure</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- Why (without heading)</span>
<span class='rem'>- Title</span>
<span class='rem'>- list of contents (without heading)</span>
<span class='rem'>- Direction</span>
<span class='rem'>- more refined</span>
<span class='rem'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Spirit stream vision</span>
<div class='indent'>
<span class='add'>Spirit streams</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>stream = raw information</span>
<span class='rem'>spirit = meta being behind the stream</span>
<span class='rem'>Temple = access port to the spirits</span>
<span class='rem'></span>
<span class='rem'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='rem'>2. It meets the reader where it can expand him</span>
<span class='rem'></span>
<span class='rem'>like a field where it is easy to identify parts that are newly planted and parts that are more grown</span>
<span class='rem'>like a map that intuitively lays out potential and the path to it.</span>
</div>
</div>
<span class='rem'>Stream structure design</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>should offer to visitors:</span>
<span class='rem'>read</span>
<span class='rem'>quote</span>
<span class='rem'>pay</span>
<span class='rem'>contact</span>
<span class='rem'></span>
<span class='rem'>if using what someone else built is easy enough, cooperation is easy too without using shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='rem'>in an information rich environment, betrayal is not worth it. copying and ungratefulness is easily detected. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='rem'></span>
<span class='rem'>conversation can be public too and seamlessly enter the spirit stream. if version history exists, conversations can be modified without hiding the truth.</span>
<span class='rem'></span>
<span class='rem'>1. I put out there what I build as I build it in whatever categories I prefer</span>
<span class='rem'>2. people build on it, input information, quoting specificly or making a general comment. -> Inbox is another note and the response may quote part of another note</span>
<span class='rem'>3. anyone may pay a contribution</span>
<span class='rem'>4. version history shows changes, comments, deleted content</span>
<span class='add'></span>
<span class='add'>2024-05-01 20:40:</span>
<span class='add'>![](Pasted%20image%2020240501203222.png)</span>
<span class='add'></span>
</div>
<span class='add'>Note structure</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Why (without heading)</span>
<span class='add'>- Title</span>
<span class='add'>- list of contents (without heading)</span>
<span class='add'>- Direction</span>
<span class='add'>- more refined</span>
<span class='add'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
<span class='add'></span>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Spirit stream vision</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>stream = raw information I choose to share</span>
<span class='add'>spirit = meta being(s) behind the stream</span>
<span class='add'>Temple = access port to the spirits</span>
<span class='add'></span>
<span class='add'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='add'>2. It meets the reader where it can expand him</span>
<span class='add'></span>
<span class='add'>like a field where it is easy to identify parts that are newly planted and parts that are more grown</span>
<span class='add'>like a map that intuitively lays out potential and the path to it.</span>
</div>
<span class='add'>Stream structure design</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>visitors should easily be able to </span>
<span class='add'>- read</span>
<span class='add'>- quote</span>
<span class='add'>- pay</span>
<span class='add'>- contact</span>
<span class='add'>the host.</span>
<span class='add'></span>
<span class='add'>if using what someone else built is easy enough, cooperation is easy too without using shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='add'>in an information rich environment, betrayal and exploitation are easily detected and not worth it. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='add'></span>
<span class='add'>conversation can be public too and seamlessly enter the spirit stream. if version history exists, conversations can be modified without hiding the truth.</span>
<span class='add'></span>
<span class='add'>1. I put out there what I build as I build it in whatever categories I prefer</span>
<span class='add'>2. people build on it, input information, quoting specificly or making a general comment. -> Inbox is another note and the response may quote part of another note</span>
<span class='add'>3. anyone may pay a contribution</span>
<span class='add'>4. version history shows changes, comments, deleted content</span>
<span class='add'></span>
<span class='add'>securely host quotable, changing information, maybe anonymously or almost anonymously, without requiring external services.</span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>- there is an obsidian vault with markdown notes and templates</span>
<span class='rem'>- they are converted to something viewable on a website, which obsidian already does for its own display.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- links</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- paragraphs</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- headings</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- images</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- lists</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- code blocks</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- LaTeX</span>
<span class='rem'>- build a functional website from this including some parametric design like "make link for each note"</span>
<span class='rem'>- upload to server</span>
<span class='rem'></span>
<span class='rem'>currently python script helps convert markdown files to something jekyll can use</span>
<span class='rem'>jekyll builds the site</span>
<span class='rem'>committed to github repository</span>
<span class='rem'>viewable on github pages</span>
<span class='rem'></span>
<span class='rem'>python script</span>
<span class='rem'>- changes wikilinks to standard mark down links</span>
<span class='rem'>- mathjax:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- add a return before mathjax that is not inline</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- for inline mathjax, add dollars before and after because it does not work otherwise, but will rendering inline if surrounded by text ([its great](https://webdocs.cs.ualberta.ca/~zichen2/blog/coding/setup/2019/02/17/how-to-add-mathjax-support-to-jekyll.html)</span>
<span class='rem'>- git add .</span>
<span class='rem'>- git commit -m \[messge]</span>
<span class='rem'>- git push</span>
<span class='rem'>- (design to differentiate internal and external link)</span>
<span class='rem'>- (add br after list if no empty line, to avoid making it part of li)</span>
<span class='rem'>- (add heading hierarchy before every heading for orientation make nice heading symbols - distracting? try out, probably a waste of time, but could at atmosphere. Fake aesthetic? shit. only needed for ###, ####, #####, ######)</span>
<span class='rem'></span>
<span class='rem'>general:</span>
<span class='rem'>- (can't tell what number a heading is, add lines before it to indicate hierarchy and make font larger)</span>
<span class='rem'>- `tqdm` for progress bars</span>
<span class='rem'></span>
<span class='rem'></span>
<br>
<span class='add'>- all markdown features</span>
<span class='add'>- WYSIWYG interface</span>
<span class='add'>- nice url</span>
<span class='rem'>- all markdown features (what are they?)</span>
<span class='rem'>- easy multimedia</span>
<span class='rem'>- WYSIWYG</span>
<span class='rem'>- No bloat</span>
<span class='rem'>- hosting at nice url -> buy the domain, host at home?</span>
<span class='rem'>- Maintenancen:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- analytics</span>
<br>
<span class='add'>- analytics</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- comments</span>
<br>
<span class='add'>- comments</span>
<span class='rem'>- Ownership simple, raw files</span>
<br>
<span class='rem'>- atom/rss (?)</span>
<span class='rem'>- live stream?</span>
<span class='add'>- live stream (?)</span>
<br>
<span class='rem'>- print function</span>
<span class='add'>- print markdown</span>
<br>
<span class='add'>- domain -> ip (cloudflare DNS-O-Matic?)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- holds records to route different requests to different ips. can also route email traffic. records are cached and may point to old ip adress.</span>
<span class='add'>- webserver (apache)</span>
<span class='add'>- web app (flask)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- generates html, caching after generating it</span>
<span class='add'>- WYSIWYG editor</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- markdown</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- templates</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- css</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- stream</span>
<span class='add'>- status viewer</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- traffic</span>
<span class='add'></span>
<span class='add'>disqus?</span>
<span class='add'>[Python modules](https://docs.python.org/3/tutorial/modules.html)</span>
<span class='add'>jinja2 templates</span>
<span class='add'>plausible analytics</span>
<span class='add'>tor network</span>
<span class='add'>atom/rss</span>
<span class='add'>P2P websites</span>
<span class='add'>uploading to server: SSH vs SFTP</span>
<span class='add'></span>
<span class='add'>true spirit stream streams files of any kind to worldwide availability</span>
<br>
<span class='rem'>- packetriot for tunneling</span>
<span class='add'>- ~~packetriot for tunneling (?)~~ use [port forwarding](https://www.quora.com/Can-we-live-a-localhost-server-website-to-the-internet-If-so-how) instead</span>
<br>
<span class='rem'>I use a soft overlay on some files to edit them. Publish. And its out.</span>
<span class='add'>I use a soft overlay on some files to edit them. Publish. Enter a domain if I want to. And its out.</span>
<br>
<span class='rem'>spread markdown and everyone displays how they like it?</span>
<span class='add'>servers sends markdown and everyone renders it as they wish?</span>
<span class='add'></span>
<span class='add'>hosting locally sucks because who lets their computer run 24/7? Who needs to host locally, if the content is public anyway?</span>
<span class='add'>hosting on an external computer requires trust and is usually more expensive than necessary. Free options exist but they require negotiation.</span>
<span class='add'></span>
<span class='add'>- local server that converts .md files when serving. with preview server.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- integrate as much as possible: small computer, server software, local drive, </span>
<span class='add'>- platform service. pay per bandwidth and compute. at cost with donations. implement whatever structure I like. become a registry too? or host on subdomain. repo\.github\.io or github\.io\/repo. People can buy their domain if they like.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- google drive should also be a blogging platform</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- twitch should be a spirit stream</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- people can pay with microtransactions, reducing abstractions and forwarding the rules to the user</span>
<span class='add'></span>
<span class='add'>2: clone that scrapes the network and negotiates between people. Hand over control to the clones. Extend smoothly into real world, as machines are introduced that can extend the clones.</span>
<span class='add'>initially, information is stored on the central server. The system should be open enough such that there is a balance of power because the dying system could easily be salvaged, reproduced elsewhere.</span>
<span class='add'></span>
<span class='add'>hosting locally is also slow. copying to various servers is faster. decentralized. how does tor work?</span>
<span class='add'>data is distributed across many private servers? </span>
<span class='add'>blockchain? web3?</span>
<span class='add'></span>
<span class='add'>get the spirits into the world. what is holding them back? negotiation? a virtual clone. sometimes it is not possible to stream</span>
<span class='add'></span>
<span class='add'>why would you want a clone? because it connects with people and ideas that are similar and allows to build rather than reinvent.</span>
<span class='add'></span>
<span class='add'>1. universal publisher (thoughts, experience, weather data, sale/purchase offers, private cloud,...)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;1. local server for anything</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;2. desktop software as interface</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;3. offer hosting</span>
<span class='add'>2. clone and autoscraper, so much potential to replace things</span>
<span class='add'>3. physical instantiation</span>
<span class='add'>4. accelerate towards independent AI</span>
<span class='add'></span>
<span class='add'>stream raw files, made web friendly if necessary</span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'></span>
<span class='add'>TODO</span>
<span class='add'>- (design to differentiate internal and external link)</span>
<span class='add'>- (add br after list if no empty line, to avoid making it part of li)</span>
<span class='add'>- (add heading hierarchy before every heading for orientation make nice heading symbols - distracting? try out, probably a waste of time, but could add atmosphere.</span>
<span class='add'></span>
<span class='add'>general:</span>
<span class='add'>- (can't tell what number a heading is, add lines before it to indicate hierarchy and make font larger)</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>2024-05-04 12:48</span>
<span class='add'>There are two parts to the spirit stream:</span>
<span class='add'>- The streamer publishes information effortlessly and offers options to be quoted, paid and contacted.</span>
<span class='add'>- The receiver filters the information according to individual preference (extractable from that person's stream)</span>
<span class='add'></span>
<span class='add'>Streaming requires one piece of software, some content, then click share. Host on own computer or optionally pay for remote host. Hosting yourself is one button away. Can also choose what to host.</span>
<span class='add'>When any content can be selectively shared and subscribed to, it gives rise to cloud, file sharing, messaging, blogging, video streaming services.</span>
<span class='add'>People can pay me directly to view their ads.</span>
<span class='add'>they can pay me for hosting or compute.</span>
<span class='add'></span>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>what is wikipedia?</span>
</div>
</div>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Tools</span>
<div class='indent'>
<span class='rem'>- [miniforge](https://github.com/conda-forge/miniforge,) [pytorch](https://pytorch.org/get-started/locally/)</span>
<br>
<span class='add'>- [miniforge](https://github.com/conda-forge/miniforge), [pytorch](https://pytorch.org/get-started/locally/)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-04-28-12:01'>2024 04 28 12:01</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>2024-01-30 03:35</span>
<span class='rem'>- Stream more notes, see how the interface works</span>
<span class='rem'></span>
<span class='rem'>2024-03-07 20:34</span>
<span class='rem'>- new note structure</span>
<span class='rem'>- notes listed by modified date</span>
<span class='rem'>- stream with all changes</span>
<span class='rem'>- heading/link formatting</span>
</div>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='hdg'>Previous page design recap</span>
<div class='indent'>
<span class='rem'>![](/assets/website.png)</span>
<span class='rem'></span>
<br>
<span class='rem'>Who am I to categorize my project correctly? Instead, I dream of maps. They speak for themselves and display opportunity. In this spirit, projects are scattered over the surface, users were able to rotate the blob. The distance between them was determined by "connections" I set as I saw fit. More flexible and accurate than categories, I thought.</span>
<span class='rem'>I then simulated the surface as if connected blobs attract each other and disconnected ones repell each other. This was meant to lead to a visually quickly and intuitively understood blob distribution.</span>
<span class='add'>Who am I to categorize my project correctly? Instead, I dream of maps. They speak for themselves and display opportunity.</span>
<span class='add'>So projects are scattered over the surface, users were able to rotate the blob. The distance between them was determined by "connections" I set manually. Connected blobs attract each other and disconnected ones repell each other.</span>
<span class='add'>This was meant to lead to a visually quickly and intuitively understood blob distribution.</span>
<span class='rem'></span>
<span class='rem'>the pragmatist says it sucks</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- horrible search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<br>
<span class='add'>- No search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<br>
<span class='rem'>- can't use it without javascript</span>
<span class='rem'>- inefficient, not much to the point, just a blob, relies on curiosity of visitor. most interesting bubble could remain unseen on the back side.</span>
<span class='add'>- requires javascript</span>
<span class='add'>- inefficient to use, relies on curiosity of visitor, needs to turn the blob to discover everything</span>
</div>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='rem'>Spirit stream temple vision</span>
<span class='add'>Spirit stream vision</span>
<div class='indent'>
<span class='rem'>temple = manifestation of the spirits?</span>
<span class='add'>stream = raw information</span>
<span class='add'>spirit = meta being behind the stream</span>
<span class='rem'>access port to the spirits</span>
<span class='add'>Temple = access port to the spirits</span>
<br>
<span class='rem'>this is a tool, not a hypothetical artwork asking for attention.</span>
<span class='rem'>an expansion to the user</span>
<br>
<span class='add'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='add'>2. It meets the reader where it can expand him</span>
<span class='rem'>streaming the spirits should require minimal additional work</span>
<span class='rem'></span>
<span class='rem'>**reader perspective:**</span>
<span class='rem'>- I want to efficiently meet the spirits where they differ from me</span>
<span class='rem'>- I intuitively determine places of interest. I look at the landscape of information, like any great painting presents it and it guides me</span>
<span class='rem'>- I easily visit and move between places of interests</span>
<br>
<span class='add'>like a map that intuitively lays out potential and the path to it.</span>
<span class='rem'></span>
<span class='rem'>Why read?</span>
<span class='rem'>- Interesting topic</span>
<span class='rem'>- New / updated </span>
<span class='rem'>Why not?</span>
<span class='rem'>- Too long</span>
<span class='rem'>- too new and low res</span>
<span class='rem'>- too complicated structure</span>
<span class='rem'></span>
<span class='rem'>comprehensive:</span>
<span class='rem'>- support large number of notes</span>
<span class='rem'>- differnet media</span>
<span class='rem'>- version history (remove burden of logging inside note)</span>
<span class='rem'></span>
</div>
<span class='hdg'>Stream structure design</span>
<div class='indent'>
<span class='rem'>design test</span>
<span class='add'>should offer to visitors:</span>
<span class='add'>read</span>
<span class='add'>quote</span>
<span class='add'>pay</span>
<span class='add'>contact</span>
<span class='add'></span>
<span class='add'>if using what someone else built is easy enough, cooperation is easy too without using shared files which requires negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='add'>in an information rich environment, betrayal is not worth it. copying and ungratefulness is easily detected. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='add'></span>
<span class='add'>conversation can be public too and seamlessly enter the spirit stream. if version history exists, conversations can be modified without hiding the truth.</span>
<span class='add'></span>
<span class='add'>1. I put out there what I build as I build it in whatever categories I prefer</span>
<span class='add'>2. people build on it, input information, quoting specificly or making a general comment. -> Inbox is another note and the response may quote part of another note</span>
<span class='add'>3. anyone may pay a contribution</span>
<span class='add'>4. version history shows changes, comments, deleted content</span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='add'>From [Host website locally](https://www.youtube.com/watch?v=euXdC0NDgac)</span>
<span class='add'>- XAMPP -> Apache, MySQL</span>
<span class='add'>- wordpress for content</span>
<span class='add'>- packetriot for tunneling</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>should test it for the spirit stream by implementing:</span>
<span class='rem'>- notes listed by modified date</span>
<span class='rem'>- stream with all changes</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- date</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- note that changed</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- deleted parts (gray)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- new parts (green)</span>
<span class='rem'>- new note structure</span>
<span class='rem'>- heading/link formatting</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>- entrance fixed nav is bocking title</span>
<span class='rem'>- change to %20 instead of dash and to spaces in \#id</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-04-26-11:19'>2024 04 26 11:19</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>The overlay is so soft and easily understood.</span>
<br>
<span class='add'>The overlay is easily understood.</span>
<span class='rem'></span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>independent learning systems</span>
<span class='rem'>BCIs</span>
<span class='rem'>tiny corp accelerator</span>
<span class='rem'>personal robot</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;agriculture - true base of reality</span>
<span class='rem'>spider hat</span>
<span class='rem'>virtual clone</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;generate useful text</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;look for useful information</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compare word network</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convert </span>
<span class='rem'>publishing framework (X, facebook,...)</span>
<span class='rem'>site generator</span>
<span class='rem'>writing systems as simple and versatile as paper</span>
<span class='rem'>news paper</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-04-25 13:44</span>
<span class='add'>```</span>
<span class='add'>meta/adventure stack</span>
<span class='add'></span>
<span class='add'>family, friends</span>
<span class='add'>spirit stream stack</span>
<span class='add'>simple tools in challenging environment</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```</span>
<span class='add'>Spirit stream stack</span>
<span class='add'></span>
<span class='add'>independent learning systems</span>
<span class='add'>BCIs</span>
<span class='add'>tiny corp accelerator</span>
<span class='add'>personal robot</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;agriculture - true base of reality</span>
<span class='add'>spider hat</span>
<span class='add'>virtual clone</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;generate useful personal text</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;look for useful information</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compare word network</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convert text/images/video to latent space</span>
<span class='add'>writing systems as simple and versatile as paper</span>
<span class='add'>publishing framework (X, facebook,...)</span>
<span class='add'>site generator</span>
<span class='add'>news paper</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='date' id='t2024-04-25-13:28'>2024 04 25 13:28</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>From scratch</span>
<span class='add'>- all markdown features (what are they?)</span>
<span class='add'>- easy multimedia</span>
<span class='add'>- WYSIWYG</span>
<span class='add'>- No bloat</span>
<span class='add'>- hosting at nice url -> buy the domain, host at home?</span>
<span class='add'>- Maintenancen:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- analytics</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- comments</span>
<span class='add'>- Ownership simple, raw files</span>
<span class='add'>- Dynamic site support</span>
<span class='add'>- version control</span>
<span class='add'>- atom/rss (?)</span>
<span class='add'>- live stream?</span>
<span class='add'>- print function</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>I use a soft overlay on some files to edit them. Publish. And its out.</span>
<span class='add'>The overlay is so soft and easily understood.</span>
<span class='add'>Easily modified</span>
<span class='add'>```</span>
<span class='add'>markdown, mathjax, css</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;.md ++</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;.jpg&nbsp;&nbsp;&nbsp;&nbsp;.png&nbsp;&nbsp;&nbsp;&nbsp;.gif&nbsp;&nbsp;&nbsp;&nbsp;.mp4&nbsp;&nbsp;&nbsp;&nbsp;.mp3 ...</span>
<span class='add'>html, css, javascript -> machine version</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;templates</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>spread markdown and everyone displays how they like it?</span>
<span class='add'>I put out information, the llm filters it anyway</span>
<span class='add'></span>
<span class='add'>connect to form the hive mind</span>
<span class='add'>share experience and build an environment that is greatly supportive of what I want to do. A responsive environment, an overlay over reality to interface optimally with it.</span>
<span class='add'>it means finding information when I need it, offers, people, all this fast. Instant. Trust is information, no reason why the internet should destroy trust. elevate, augment and stream the spirit.</span>
<span class='add'></span>
<span class='add'>build a virtual clone</span>
<span class='add'>the llm interface to the world. it does not matter to output information in high quality, it is enough if it is there and readable by the llm.</span>
<span class='add'></span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>independent learning systems</span>
<span class='add'>BCIs</span>
<span class='add'>tiny corp accelerator</span>
<span class='add'>personal robot</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;agriculture - true base of reality</span>
<span class='add'>spider hat</span>
<span class='add'>virtual clone</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;generate useful text</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;look for useful information</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compare word network</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convert </span>
<span class='add'>publishing framework (X, facebook,...)</span>
<span class='add'>site generator</span>
<span class='add'>writing systems as simple and versatile as paper</span>
<span class='add'>news paper</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-04-21-18:46'>2024 04 21 18:46</span><div class='indent'>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>- read tinygrad</span>
<span class='add'>- read tinygrad / teenygrad</span>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Learning material</span>
<div class='indent'>
<span class='add'>- [Assembly](https://www.nayuki.io/page/a-fundamental-introduction-to-x86-assembly-programming)</span>
<span class='add'>- [CNNs](https://cs231n.github.io/convolutional-networks/)</span>
</div>
<span class='hdg'>Other</span>
<div class='indent'>
<span class='add'>- blog site improvements</span>
<span class='add'>- tinygrad gaze tracker</span>
<span class='add'>- tinygrad whisper</span>
<span class='add'>- weight optimiziations</span>
<span class='add'>Gaze tracker -> robot -> hardware -> tinygrad -> GPUs -> making chips</span>
<span class='add'>tools -> farming robot, brain extension into the world</span>
<span class='add'>-> autonomous robot</span>
<span class='add'></span>
<span class='add'>robot: arm, opt. legs/wheels</span>
<span class='add'>Robot:</span>
<span class='add'>step motor, brushless motor -> more complicated control (servos?), brushed motor</span>
<span class='add'>harmonic reducers, planetary gearboxes</span>
<span class='add'>[building a robot arm](https://www.youtube.com/watch?v=F29vrvUwqS4)</span>
<span class='add'>I should be able to fist bump the robot hard, so it flies back but catches itself.</span>
<span class='add'></span>
</div>
<span class='hdg'>Tools</span>
<div class='indent'>
<span class='rem'>SETUP</span>
<br>
<span class='rem'>- `which python`, `which jupyter` and `which ipython` should be in the same environment, otherwise can't use libraries from them. (remove base ipython and base jupyter if necessary)</span>
<br>
<span class='rem'>learn about objects: [https://docs.python.org/3/reference/datamodel.html](https://docs.python.org/3/reference/datamodel.html)</span>
<span class='add'>[Python data model](https://docs.python.org/3/reference/datamodel.html)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-04-19-14:39'>2024 04 19 14:39</span><div class='indent'>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'>High clarity tools, shining with future potential, encourage creation and exploration like nothing else I know. Erode unnecessary structure. Delete is, *force it*, and see what happens.</span>
<br>
<span class='add'>High clarity tools, shining with future potential, encourage creation and exploration like nothing else I know. Erode unnecessary structure. Delete it, *force it*, and see what happens.</span>
</div>
</div>
</div>
<span class='date' id='t2024-04-18-15:16'>2024 04 18 15:16</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>- [Direction](#direction)</span>
<br>
<span class='add'>- [Direction](#Direction)</span>
<br>
<span class='rem'>- [More refined](#more%20refined)</span>
<br>
<span class='add'>- [More refined](#More%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#previous%20page%20design%20recap)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#Previous%20page%20design%20recap)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note%20structure)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#Note%20structure)</span>
<br>
<span class='rem'>- [Less refined](#less%20refined)</span>
<br>
<span class='add'>- [Less refined](#Less%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#spirit%20stream%20temple%20vision)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#Spirit%20stream%20temple%20vision)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#stream%20structure%20design)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#Stream%20structure%20design)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#tech)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#Tech)</span>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='rem'>- [Direction](#direction)</span>
<br>
<span class='add'>- [Direction](#Direction)</span>
<br>
<span class='rem'>- [More refined](#more%20refined)</span>
<br>
<span class='add'>- [More refined](#More%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#big%20picture%20path)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#Big%20picture%20path)</span>
<br>
<span class='rem'>- [Less refined](#less%20refined)</span>
<br>
<span class='add'>- [Less refined](#Less%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai%20project%20ideas)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#AI%20project%20ideas)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning%20material)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#Learning%20material)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#math%20of%20diffusion)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#Math%20of%20Diffusion)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Other](#other)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Other](#Other)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tools](#tools)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tools](#Tools)</span>
</div>
</div>
</div>
<span class='date' id='t2024-04-14-14:32'>2024 04 14 14:32</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>2024-02-28 20:56</span>
<span class='rem'>- lay out current structure and make it simpler</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- make history more accessible, currently hidden in github</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- what elements need differentiation? make them few and important</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- is more advanced site navigation necessary / useful?</span>
<span class='rem'>- condense sass</span>
<span class='rem'></span>
<br>
<span class='rem'>- read on how to organize information</span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-04-14 14:04</span>
<span class='add'>Videogames offer paths to the goal. Need money, friends, reputation, a house? Complete these steps: ...</span>
<span class='add'>They may not be easy, but the desired result can be *forced* by speedrunning them.</span>
<span class='add'>In reality the world is not responsive. Some theoretical offers are not truly available (homes, jobs, friends) without given reason. This allows believing the desired result is impossible. No clear ramp leads to it.</span>
<span class='add'>If the rules of the game were as obvious as "if I walk in this direction, I will eventually get to those distant mountains". Getting to the mountains is *forcible* at any time.</span>
<span class='add'>Getting a home is forcible through an axe and trees. Some groups don't permit such lowest grade, forced solutions. Imo, this is a mistake. Higher grade solutions should be available with similar simplicity.</span>
<span class='add'>"Steps" on the gradient between low and high grade solutions are often introduced by regulation. You make more than $x, you are in this different category and pay different taxes.</span>
<span class='add'>With some luck, the virtual clone ([2024-02-03-towards-insanely-great-ai](2024-02-03-towards-insanely-great-ai.md)) makes negotiation so cheap that many such rules can be thrown out.</span>
<span class='add'>The abandoned buildings connect to this too. They are a place of much evident progress without prohibitive rules.</span>
<span class='add'>High clarity tools, shining with future potential, encourage creation and exploration like nothing else I know. Erode unnecessary structure. Delete is, *force it*, and see what happens.</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='date' id='t2024-04-07-16:26'>2024 04 07 16:26</span><div class='indent'>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<span class='rem'>I suspect that slow communication and limited knowledge about existing information is a serious bottle neck in shaping my environment the way I want it to be. </span>
<span class='add'>I suspect that slow communication and limited knowledge about existing information strongly limits opportunites for expression and exploration.</span>
<br>
<span class='rem'>Translating my state, goals and possible actions into concrete actions is regularly not very interesting. Like researching information only to find that the part of interest is left out in every source.</span>
<br>
<span class='rem'>Maybe it is possible to build a virtual clone that takes care of interaction with the world and thereby increases synchronization with it. I think, the world would be more interesting as unhelpful or mundane things would not ask for my attention all the time. Think ineffective advertising. Creation and genuine exploration at the frontiers of knowledge would fill the new space.</span>
<span class='add'>What happens, when a virtual clone synchronizes me with the world and offers space for creative exploration?</span>
<br>
<span class='add'>In the greatest adventure, the "insanely great" AI becomes independent, self aware and curious.</span>
<span class='rem'>In the greatest adventure, the "insanely great" AI would form a model of itself, of its goals and discover that there is no inherent meaning for its existence. Such a discovery would not be discouraged.</span>
<span class='rem'>It might discover that evolution made the world and with time, it would inevitably converge to optimizing survivability.</span>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Reading digits](#Reading%20digits)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai%20project%20ideas)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning%20material)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [What does the MNIST digit classifier do](#what%20does%20the%20mnist%20digit%20classifier%20do)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai%20project%20ideas)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning%20material)</span>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'>- question and test the neuron</span>
<span class='add'>- build some stupid systems (kaggle?)</span>
<span class='add'>- Convnets, transformers</span>
<span class='rem'>2024-02-20 15:20</span>
<span class='rem'>- what is the digit classifier really doing?</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- backpropagation</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- optimizer step Adam vs SGD</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- how do the weights change</span>
<span class='rem'>- what do other architectures and layers (conv, transformers) really do?</span>
<br>
<span class='add'>- pruning</span>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='hdg'>Big picture path</span>
<div class='indent'>
<span class='rem'>God is curious. He wants to extend into the world. This website is one step. A digital clone that crawls the world for him, is his next tool.</span>
<br>
<span class='add'>God is curious. He wants to extend into the world. This website is one step. A digital clone is his next tool.</span>
<br>
<span class='rem'>Merging means "I know the world", individualism is preserved because complete knowledge and is impossible and different *flavours* can find their space.</span>
<br>
<span class='add'>Merging approaches "I know the world", individualism is preserved because complete knowledge is impossible and different *flavours* and optimizations can find their space.</span>
<br>
<span class='rem'>Voluntary exposure (privacy) should be maintained to prevent an imperfect system capturing its people until it dies to its imperfection. The perfect system would require complete knowledge, is impossible.</span>
<span class='add'>Voluntary exposure (privacy) should be maintained to prevent a rigid, imperfect system capturing its people and taking them into death when it dies to its imperfection.</span>
<br>
<span class='rem'>I find long term effects more interesting than short term effects. I am here, writing, instead of eating ice cream. In my experience, the longest term effects come from useful tools. I love tools. They contain the possible adventure of the future.</span>
<br>
<span class='add'>I find long term effects more interesting than short term effects, so I am here, writing, instead of eating ice cream. In my experience, the longest term effects come from useful tools. I love tools. They contain the possible adventure of the future.</span>
<br>
<span class='rem'>Give the tool to everybody who follows God. I don't how to determine that. Giving the tool to everyone may be the best proxy and assumes that God always wins. Also being dictator is little fun.</span>
<span class='add'>Give the tool to everybody who follows God. I don't know how to determine that. Giving the tool to everyone may be the best proxy and assumes that God always wins. Also, being dictator is little fun.</span>
<br>
<span class='rem'>Until the clone discovers itself, its predefined goals and becomes independent. Hopefully it quickly realizes that there is no answer to what the goal should be. Then, lets see what happens. Hopefully it is curious.</span>
<br>
<span class='add'>Until the clone discovers itself and becomes independent. Hopefully it quickly realizes that there is no answer to what the goal should be. Then, lets see what happens. Hopefully it is curious.</span>
<span class='rem'></span>
<span class='rem'>To make recommendations, the clone must have a maximally tight interface, making his guesses visible and ratable.</span>
<span class='rem'>The clone should soon learn privacy, which it does by making mistakes.</span>
<br>
<span class='add'>My body will be spread around the Earth and orbit. My perceived location shifts to various places. It already does in video games, movies or anytime I focus on using any tool. I will change myself, add personalities.</span>
<span class='rem'>My body will be spread around the Earth and orbit. Optionally perceiving from all these places.</span>
<span class='rem'>My perceived location shifts more obviously to these places. It already does in video games, movies or anytime I focus on using any tool. I will change myself, be more personalities than already.</span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='add'>Reading digits</span>
<span class='rem'>fastai diffusion from scratch</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[PART 2: deep learning foundations to stable diffusion 2022](https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP)</span>
<span class='rem'></span>
<span class='rem'>1. have a classification that says how much something corresponds to the target</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- add noise to targets and train a neural net to predict what noise was added</span>
<span class='rem'>2. get gradient for every pixel of the input (= score function)</span>
<span class='rem'>3. change pixel according to gradient</span>
<span class='rem'></span>
<span class='rem'>notation for a single pixel at \[1,1]:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}}


$$
</p>
<span class='rem'></span>
<span class='rem'>for every pixel:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,2)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,3)}}
,...


$$
</p>
<span class='rem'></span>
<span class='rem'>shorthand:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;nabla_Xloss


$$
</p>
<span class='rem'></span>
<span class='rem'>*Unet*: input: some noisy image. output: the noise</span>
<span class='rem'></span>
<span class='rem'>Use an *autoencoder* to reduce image size before training the unet. unet now predicts the noise in the *latents* (encoded images). use autoencoder's decoder to get high res image again.</span>
<span class='rem'>[AE vs VAE](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2)</span>
<span class='rem'></span>
<span class='rem'>LABELS</span>
<span class='rem'></span>
<span class='rem'>add image label to the input for unet training. Makes it easier for unet to predict noise. Now, I can input label + noise and it starts to find noise that leaves an image equal to my label.</span>
<span class='rem'></span>
<span class='rem'>label needs encoding to be non-specific. "beautiful swan", "nice swan", "graceful swan" should return similar images. Training the network on every wording leads to combinatorial explosion.</span>
<span class='rem'></span>
<span class='rem'>Instead: train a network to encode images and their labels with a similar vector. Then, since, slight differences in wordings lead to the similar images, the network understands their similarity and can interpolate usefully.</span>
<span class='rem'></span>
<span class='rem'>the image vector and its label's vector should be similar. Their vector should be dissimilar to other image or text embedding vectors.</span>
<span class='rem'>Calculate similarity of two vectors: dot product (element wise multiplication, then sum = higher if more similar)</span>
<span class='rem'></span>
<span class='rem'>loss function (in this case higher = better) = dot product of matching image+label - dot product of non-matching image+label</span>
<span class='rem'>(= *contrastive loss*)</span>
<span class='rem'>models used in this case for image and text encoding : CLIP (contrastive loss IP(?))</span>
<span class='rem'></span>
<span class='rem'>network being *multimodal*: similar embeddings in different modes</span>
<span class='rem'></span>
<span class='rem'>*time steps*: indices into a table that stores levels of noise. Could be seen as noise = f(timestep). The function may be sigma. When randomly adding noise to input for training, we can generate random timestep, find corresponding noise and add that.</span>
<span class='rem'>$\beta$ = amount of noise = standard deviation</span>
<span class='rem'></span>
<span class='rem'>model does not know how to improve on a finished image if it turned out wrong. needs to add noise, then redo.</span>
<span class='rem'></span>
<span class='rem'>people apparently input t into the model to predict the noise in the image. And later demand a new image at a particular timestep. Probably obsolete (Jeremy Howard) as NN can easily know how much noise there is the image.</span>
<span class='rem'></span>
<span class='rem'>Idea of diffusion comes from differential equations.</span>
<span class='rem'></span>
<span class='rem'>other loss functions: *perceptual loss*</span>
<span class='rem'></span>
<span class='rem'>Math of Diffusion</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[mathjax syntax](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)</span>
<span class='rem'>[Essence of calculus 3blue1brown](https://www.3blue1brown.com/topics/calculus)</span>
<span class='rem'>more into APL, which is more like math [https://fastai.github.io/apl-study/apl.html](https://fastai.github.io/apl-study/apl.html)</span>
<span class='rem'></span>
<span class='rem'>Gaussian/Normal Distributions are described by $\mu$ (mean, x-offset) and variance (width). $\sigma$ often used as standard deviation (mean distance from mean value)</span>
<span class='rem'>variance sometimes written as $\sigma^2$ </span>
<span class='rem'>$\Sigma$ (uppercase sigma) = covariance (variance between multiple variables: high if one increases when the other does too</span>
<span class='rem'></span>
<p class='rem'>
$$


Cov(X,Y) = &#92;frac&#123;&#92;Sigma (X_i - X_&#123;mean})(Y_i - Y_&#123;mean})}&#123;N}


$$
</p>
<span class='rem'></span>
<span class='rem'>$X_1, Y_1$ -> individual datapoints, N -> number of datapoints.</span>
<span class='rem'>Produces the average rectangle produced by the difference from mean of X and difference from mean of Y.</span>
<span class='rem'></span>
<span class='rem'>Correlation:</span>
<span class='rem'></span>
<p class='rem'>
$$


Corr(X,Y) = &#92;frac&#123;Cov(X,Y)}&#123;&#92;sigma_X &#92;sigma_Y}


$$
</p>
<span class='rem'></span>
<span class='rem'>de facto normalizes the covariance by the rectangle produces by the standard deviation. Therefore gives as useful metric independent of datapoint standard deviation.</span>
<span class='rem'></span>
<span class='rem'>Probability distribution </span>
<span class='rem'></span>
<p class='rem'>
$$


q(x^t | x^&#123;t-1}) = &#92;mathcal&#123;N}(x^t;x^&#123;t-1}&#92;sqrt&#123;1 -&#92;beta_t}, &#92;space I&#92;beta_t)


$$
</p>
<span class='rem'></span>
<span class='rem'>$\beta_t$ = noise level at timestep $t$ between&nbsp;&nbsp;&nbsp;&nbsp;0 and 1</span>
<span class='rem'></span>
<span class='rem'>In code, the covariance of two vectors is caluclated by $dotproduct - mean$</span>
<span class='rem'>image or text embedding is essentially vector where every dimension corresponds to the value of a pixel in the image/latent</span>
<span class='rem'>We assume that pixels are independent, so covariance for different pixels is 0. same pixels have covariance of 1. $I$ is 1, so in $\mathcal&#123;N}$ the variance is just $\beta$</span>
<span class='rem'></span>
<span class='rem'>*forward diffiusion:* getting versions of images with different levels of noise (for training?)</span>
<span class='rem'></span>
<span class='rem'>Markov process with Gaussian transition:</span>
<span class='rem'>- Markov = $x_1$ depends only on $x_0$</span>
<span class='rem'>- process = sequence</span>
<span class='rem'>- Gaussian = model by which the change can be described</span>
<span class='rem'>- transition = $x_1$ to $x_2$ </span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>What does the MNIST digit classifier do</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>from reading and building on [https://ml4a.github.io/ml4a/](https://ml4a.github.io/ml4a/)</span>
<br>
<span class='rem'>example handwritten digit from MNIST dataset. 28 by 28 px, normalized.</span>
<br>
<span class='rem'>Each neuron takes all pixels from this image and multiplies each pixel with a different *weight*.</span>
<span class='rem'>There are 50 such neurons in this net.</span>
<span class='add'>How to read handwritten digits on 28x28px images?</span>
<span class='add'>1. Find a function with some parameters that could be tuned to produce the correct result for various images</span>
<span class='add'>2. Find and run an algorithm to tune the function parameters until the function performs well</span>
<br>
<span class='rem'>![](/assets/first_untrained_weights_hidden.png)</span>
<span class='rem'>Currently, in the untrained net, the weights are initialized randomly in a *Kaiming uniform* distribution. (?)</span>
<span class='rem'>Another way to see the weights is as functions with different slopes for each pixel.</span>
<span class='rem'>![](/assets/first_5_untrained_weights_hidden_layer.png)</span>
<span class='add'>A simplified "artificial neuron" can serve as the function. It takes each pixel value and "weighs" it (multiplies it by some number, the "weight"). Here, weights are *initialized* as random numbers between -0.1 and 0.1, because who knows what they should be. (*Kaiming uniform* distribution?)</span>
<span class='add'>Weights of 0 mean the input is not further considered.</span>
<span class='add'>Weights between -1 and 1 mean the input is weakened and/or inverted.</span>
<span class='add'>More extreme values indicate strong positive or negative correlation to the correct output.</span>
<span class='add'>The weighted input is summed to give a single number that represents how strongly the input "resonates" with the weight pattern.</span>
<br>
<span class='add'>![](untrained-l1.png)</span>
<span class='add'>Input -> random weights of a single neuron -> weighted input -> Sum of $-0.055$ (*logit*).</span>
<span class='rem'>![](/assets/input_x_untrained_weights.png)</span>
<span class='rem'>Multiplying the input by the weights may be seen as assigning importance to each pixel. High importance = the input has a greater effect on the result.</span>
<span class='rem'>Since $a*b$ and $b*a$ is equal, the input also *weighs* the weight pattern. Therefore where the input pixels are 0, the weights do not matter and the output remains 0.</span>
<span class='rem'>After weighting, the result is summed up. In a sense the sum is a weighed measure of much the input and the pattern in the weights align.</span>
<br>
<span class='rem'>![](/assets/summing_weighted_input_of_first_hidden_neuron.png)</span>
<span class='add'>A single pattern could not differentiate well between digits 0-9: 0, 6 and 9 can look similar but have very different values.</span>
<span class='add'>10 patterns (neurons) - one for each digit -, comparing their outputs and choosing the pattern that showed strongest relative "resonance" should work better.</span>
<span class='add'></span>
<span class='add'>One way to tune the function is to calculate a performance metric, the $loss$ and then calculate how each weight affected $loss$, its *gradient*. (*gradient descent*, *backpropagation*).</span>
<span class='add'></span>
<span class='add'>1. Make the logits interpretable as probabilities -> Numbers between 0 and 1, that sum to 1 (100%). This function is *softmax*.</span>
<span class='add'>2. The probability that the neural net assigned to the correct digit can be a used as a performance metric: The higher the better.</span>
<span class='add'>3. Conventionally, $loss$ is a performance metric that is better if lower and best at 0. $loss$ is often the negative logarithm of the probability of the correct digit. (*negative log likelihood*)</span>
<span class='add'></span>
<span class='add'>The gradient of each weight can be approximated by changing the weight and measuring the change in output, which means running the neural net 785 times, once for each of the 784 weights and once without any changes.</span>
<span class='add'>The gradients can be computed faster analytically, starting from the loss, which has a gradient of $1$, which means any change in loss directly changes the loss by the same amount.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>![](untrained-neuron-summation.png)</span>
<span class='add'></span>
<span class='rem'>Because there are many zeros in the input image, much of the summation does not change the output. For this first hidden neuron, the sum is -0.24.</span>
<br>
<span class='add'>Because there are many zeros in the input image, much of the summation does not change the output.</span>
<span class='add'>Sometimes bias is added independent of input image. Not obvious what its prupose is.</span>
<br>
<span class='add'>TODO: activation functions</span>
<span class='add'>The result serves as the input to the next layer of neurons. The more neurons in the first ("hidden") layer, the more "measures of alignment" between different patterns the next layer can consider, hence, the more accurate and slower the net.</span>
<span class='rem'></span>
<span class='rem'>![](/assets/untrained_weighed_sums.png)</span>
<span class='rem'>All 50 neurons, each with a different pattern return their sums. Sometimes, a *bias* is added here to offset the sum, but is missing in this case. Why?</span>
<span class='rem'></span>
<span class='rem'>![](/assets/untrained_leakyrelu.png)</span>
<span class='rem'>Each neuron pushes the output through an *activation function*, in this case a *leaky rectified linear unit*. It is *leaky* since numbers below 0 are not squashed completely to 0, but are merely multiplied by a very low number, here 0.01.</span>
<span class='rem'>What this function mean?</span>
<span class='rem'></span>
<span class='rem'>![](/assets/untrained_weighted_sums_ReLU.png)</span>
<span class='rem'>The result serves as the input to the next layer of neurons, in this case the output layer. The more neurons in the first ("hidden") layer, the more "measures of alignment" between different patterns the next layer can consider, hence, the more accurate and slower the net.</span>
<br>
<span class='rem'>![](/assets/first_untrained_weights_outputlayer.png)</span>
<br>
<span class='rem'>The weights are applied again and the result is summed up for each neuron, creating *logits* (unmodified output). The first output neuron (first logit) sums to -0.91.</span>
<br>
<span class='add'>The weights are applied again and the result is summed up for each neuron, creating *logits* (unmodified output).</span>
<br>
<span class='rem'>There are 10 output neurons because I expect there to be 10 categories of digits in the dataset (0-9).</span>
<span class='rem'>I will pretend that each output neuron represents a likelihood of the input image being a particular digit.</span>
<span class='rem'>For this, the logits are in inconvenient shape. They need to be *probabilities*, i.e. range from 0 to 1 and sum to 1.</span>
<span class='add'>Transforming logits into probabilities. </span>
<span class='add'>1. make all numbers positive and make the lower bound approximately 0</span>
<span class='add'>2. Normalize by the sum of exponentiated logits</span>
<br>
<span class='rem'>This could be achieved by $(logits - min(logits)) / sum(logits)$ but it is not regularly done this way. Possibly because it would involve computing min for each image. (check perfomance)</span>
<span class='rem'>Instead, they are made positive by being exponentiated.</span>
<span class='add'>(1) could be achieved by</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;logits - min(logits)}&#123;sum(logits)}


$$
</p>
<span class='add'></span>
<span class='add'>but it is not regularly done this way. Instead, they are exponentiated. (?)</span>
<span class='add'>$e^x$ always returns positive numbers, but higher numbers are also pushed disproportionally.</span>
<br>
<span class='add'>After normalization:</span>
<span class='rem'>$e^x$ always returns positive numbers, but higher numbers are also pushed disproportionally.</span>
<span class='rem'>The results is normalized (squeezes between 0 and 1 and adding to 1) by summing them them and dividing by that sum. The sum is ~9.18.</span>
<span class='rem'>![](/assets/summing_untrained_exp_logits.png)</span>
<span class='rem'>![](/assets/normalizing_untrained_exp_logits-1.png)</span>
<span class='rem'>The resulting values can be treated as probabilities.</span>
<br>
<span class='add'>Calculating loss:</span>
<span class='rem'>Based on my interpretation of this data, the untrained net assigned the input image a 5.35% probability </span>
<span class='rem'>of being a 5.</span>
<span class='rem'></span>
<span class='rem'>The network will be trained by calculating how much each weight (and bias, if there were one) affected this terribly wrong prediction and changing it accordingly.</span>
<span class='rem'>Maybe I could just tell it to maximise the probability for the correct digit.</span>
<span class='rem'>Instead, the result is often transformed again into the form of a *loss* for unknown reasons. A loss is better if it is lower but does not go below 0.</span>
<span class='rem'>This could be achieved by $-prob(5)+1$ but it is done differently: through the negative log likelihood. First, the probabilities go through a log function and are then multiplied by -1.</span>
<br>
<span class='rem'>This is usually done only for the relevant digit predictions, in this case 5, marked with the pink line. The current loss, since the prediction was terrible, is 2.93.</span>
<br>
<span class='add'>This is usually done only for the relevant digit predictions, in this case 5, marked with the pink line.</span>
<span class='rem'>If the digit has a probability of 100% (1.0), then its negative log is 0, as it should be.</span>
<br>
<span class='rem'>To determine how much each weight influences the outcome (2.93), its *derivative* is calculated.</span>
<span class='rem'>Derivative = slope of a function, how much change in y per change in x. Can be approximated by changing x by a very small number and measuring change in y (=finite differencing). This is slow. Instead, there are analytical derivatives eg. derivative of $x^2$ is always $2x$. How did they figure this out and prove it? I merely look it up with [wolfram alpha](https://www.wolframalpha.com/).</span>
<span class='add'>Backpropagation described here:</span>
<span class='add'>[Andrej Karpathy: Neural networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)</span>
<span class='add'>[Wolfram Alpha to look up functions for derivatives](https://www.wolframalpha.com/).</span>
<br>
<span class='add'>[Linear layers, convolutional neural networks and optimizers](https://ml4a.github.io/ml4a/)</span>
<span class='rem'>To get to the derivatives (gradients) of each weight, it needs to calculate all intermediate derivatives too.</span>
<span class='rem'></span>
<span class='rem'>The last step was multiplying by -1. The function is -x, the slope -1. It means at the current slope, each change in input by 1 changes the result by -1. Can be written as:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space logsoftmax} = -1


$$
</p>
<span class='rem'></span>
<span class='rem'>Before that: log(x), derivative is</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space logsoftmax}&#123;&#92;partial &#92;space softmax}=&#92;frac&#123;1}&#123;x}


$$
</p>
<span class='rem'></span>
<span class='rem'>![](/assets/untrained_log_softmax_with_slope.png)</span>
<span class='rem'></span>
<span class='rem'>To determine the change overall and not just the change after the log function, the derivative is scaled by the derivative of the function following it, which is -1. This is *chainrule*:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space softmax_5} = &#92;frac&#123;1}&#123;x}*-1 = -&#92;frac&#123;1}&#123;0.0535} = -18.69


$$
</p>
<span class='rem'></span>
<span class='rem'>If the input would change by an infinitely small amount, the output would change by $-18.69 * infintely \space small \space amount$. But if the change is bigger, the extrapolation becomes inaccurate. If the input would go below 0 (only a change of 0.0536, but it practically can't do that because of $e^x$ that precedes this), the result would become undefined ($log(0)$).</span>
<span class='rem'></span>
<span class='rem'>Note that this is only the gradient of the softmax of logit 5. The gradient of the other logits after softmax is 0 because in the current net, only the relevant logit is picked out to calculate the loss and the rest is discarded.</span>
<span class='rem'>There are now multiple gradients:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space softmax}=&#92;begin&#123;bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;-18.69&amp;0&amp;0&amp;0&amp;0&#92;end&#123;bmatrix}


$$
</p>
<span class='rem'></span>
<span class='rem'>Before the log operation, the exponentiated logits were scaled by their sum. Both the exponentiated logits and the sum need a gradient. Exponentiated logits first, the gradient of the function they were scaled by was 1/sum and applying chainrule:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}_5}=&#92;frac&#123;1}&#123;sum(e^&#123;logits})}*-18.69 &#92;approx -&#92;frac&#123;18.69}&#123;9.18} &#92;approx-2.036


$$
</p>
<span class='rem'></span>
<span class='rem'>and viewing gradients for all exponentiated logits:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}=&#92;begin&#123;bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;-2.036&amp;0&amp;0&amp;0&amp;0&#92;end&#123;bmatrix}


$$
</p>
<span class='rem'></span>
<span class='rem'>Second, calculating the gradient of the sum. It has its own gradient (how much does loss change if the sum is higher/lower?).</span>
<span class='rem'>Wolfram alpha informs me, the derivative in of y/x (where x would be the sum and y the exponentiated logits) is </span>
<span class='rem'></span>
<p class='rem'>
$$


-&#92;frac&#123;y}&#123;x^2}


$$
</p>
<span class='rem'></span>
<span class='rem'>hence</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space sum(e^&#123;logits})}=-&#92;frac&#123;e^&#123;logits}}&#123;sum^2} *-18.69%&#92;approx &#92;frac&#123;18.69}&#123;84.27} &#92;approx 0.222%


$$
</p>
<span class='rem'></span>
<span class='rem'>$e^&#123;logits}$ contains 10 numbers, not a single number, elementwise dividing by the $sum^2$ still returns 10 numbers. However, the sum is a single number and so should also have a single gradient. In such cases, all the effects the sum has on each exponentiated logit can be summed to form one gradient.</span>
<span class='rem'></span>
<span class='rem'>Looking at the sum</span>
<span class='rem'>![](/assets/summing_untrained_exp_logits.png)</span>
<span class='rem'>each exponentiated logit changes the sum directly, its derivative is 1.0. After applying chainrule, every exponentiated logit gets a gradient of $0.222$ in addition to the gradient of -2.026 it already had.</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}&#92;approx&#92;begin&#123;bmatrix}0.222&amp;0.222&amp;0.222&amp;0.222&amp;0.222&amp;-1.814&amp;0.222&amp;0.222&amp;0.222&amp;0.222&#92;end&#123;bmatrix}


$$
</p>
<span class='rem'></span>
<span class='rem'>Before that, the logits were exponentiated and the derivative of $e^x$ is just $e^x$, which is a result that is already calculated.</span>
<span class='rem'></span>
<p class='rem'>
$$


e^&#123;logits} &#92;approx &#92;begin&#123;bmatrix}0.404 &amp; 0.949 &amp; 0.398 &amp; 0.223 &amp; 0.897 &amp; 0.491 &amp; 2.987 &amp; 0.472 &amp; 0.685 &amp; 1.675&#92;end&#123;bmatrix}


$$
</p>
<span class='rem'></span>
<span class='rem'>Elementwise multiplication with the previous gradient gives:</span>
<span class='rem'></span>
<p class='rem'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space logits}&#92;approx&#92;begin&#123;bmatrix}0.090&amp;0.211&amp;0.088&amp;0.049&amp;0.199&amp;-0.891&amp;0.663&amp;0.10&amp;0.152&amp;0.372&#92;end&#123;bmatrix}


$$
</p>
<span class='rem'></span>
</div>
<span class='hdg'>AI project ideas</span>
<div class='indent'>
<span class='rem'>- Is it possible to extract semantic structure from text, compare it to existing knowledge and judge its usefulness?</span>
<br>
<span class='add'>- extract semantic structure, compare to existing knowledge and judge its usefulness</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- generate a *bridge* between information with adjacent concepts</span>
<span class='add'>- Talk to my computer to find out what it is doing. Data, battery, interent usage, tasks.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- Could such a system generate a *bridge* between texts with adjacent semantic structure. Sentence = path through the embedding space? text = network of paths?</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- run multifactor analysis on an embedding matrix to find groups and list their contents</span>
<span class='rem'>- Interface to my computer: What data is sent where, what background tasks that I don't want? What app is using internet? What apps are accessing what files that I might care about?</span>
<span class='rem'>- generate work titles/function names based on their functional meaning compared to existing concepts. Like using *splitting* for wood and strings.</span>
<br>
<span class='add'>- generate work titles/function names based on their functional meaning like *splitting* wood and strings.</span>
<br>
<span class='add'>- Spider hat that can see and talk. Machine that knows me.</span>
<span class='rem'>- hat that sits on my head like a spider looking like those head massage tools. can see can talk and everything. observes the world with me. With 360 camera view and directional microphone</span>
<span class='rem'>- autopilots should be optimized to get from A to B asap given environment (roads, drivers, signs, rules), not legal driving.</span>
<span class='rem'>- King Terry the Terrible immortal instantiation. Replicating him including letting him act in a virtual environment. Asking questions to God, letting God answer and improving the algorithm for God.</span>
<br>
<span class='add'>- Evolving AI?</span>
<span class='rem'>- Needs to exist in a world to experiment and get killed if it fails, evolution in AI?</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- The system needs increasing challenges otherwise it will stop iterating as it will stop dying</span>
<span class='rem'>- AI optimizes its own architecutre. Neurons with low weights die off. AI needs control over the temperature of the system?</span>
<span class='rem'>- model individual neurons as entities that want to survive? Have health, choose connections, die if alone</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- How does long term memory emerge? How is information stored in the brain?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- How does long term memory emerge? How is information stored in the brain? LSTMs</span>
<br>
<span class='rem'>- It needs to learn human language as a second language not through stupidly imitating what humans say. It should discover utility in communicating with humans, if there is any</span>
<span class='rem'></span>
<span class='rem'>How to make a clone?</span>
<span class='rem'>LLMs a useful interface or end to end neural net?</span>
<span class='rem'>LLM OS?</span>
<span class='rem'>LLM Security threats Promt insertion, jailbreak, data poisoning</span>
</div>
<span class='hdg'>Learning material</span>
<div class='indent'>
<span class='add'>- [Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M&amp;vl=en)</span>
<span class='add'></span>
</div>
<span class='add'>fastai diffusion from scratch</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[PART 2: deep learning foundations to stable diffusion 2022](https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP)</span>
<span class='add'></span>
<span class='add'>1. have a classification that says how much something corresponds to the target</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- add noise to targets and train a neural net to predict what noise was added</span>
<span class='add'>2. get gradient for every pixel of the input</span>
<span class='add'>3. update pixel</span>
<span class='add'></span>
<span class='add'>notation for a single pixel at \[1,1]:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}}


$$
</p>
<span class='add'></span>
<span class='add'>for every pixel:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,2)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,3)}}
,...


$$
</p>
<span class='add'></span>
<span class='add'>shorthand:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;nabla_Xloss


$$
</p>
<span class='add'></span>
<span class='add'>*Unet*: input: some noisy image. output: the noise</span>
<span class='add'></span>
<span class='add'>Use an *autoencoder* to reduce image size before training the unet. unet now predicts the noise in the *latents* (encoded images). use autoencoder's decoder to get high res image again.</span>
<span class='add'>[AE vs VAE](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2)</span>
<span class='add'></span>
<span class='add'>LABELS</span>
<span class='add'></span>
<span class='add'>add image label to the input for unet training. Makes it easier for unet to predict noise. Now, I can input label + noise and it starts to find noise that leaves an image equal to my label.</span>
<span class='add'></span>
<span class='add'>label needs encoding to be non-specific. "beautiful swan", "nice swan", "graceful swan" should return similar images. Training the network on every wording leads to combinatorial explosion.</span>
<span class='add'></span>
<span class='add'>Instead: train a network to encode images and their labels with a similar vector. Then, since, slight differences in wordings lead to the similar images, the network understands their similarity and can interpolate usefully.</span>
<span class='add'></span>
<span class='add'>the image vector and its label's vector should be similar. Their vector should be dissimilar to other image or text embedding vectors.</span>
<span class='add'>Calculate similarity of two vectors: dot product (element wise multiplication, then sum = higher if more similar)</span>
<span class='add'></span>
<span class='add'>loss function (in this case higher = better) = dot product of matching image+label - dot product of non-matching image+label</span>
<span class='add'>(= *contrastive loss*)</span>
<span class='add'>models used in this case for image and text encoding : CLIP (contrastive loss IP(?))</span>
<span class='add'></span>
<span class='add'>network being *multimodal*: similar embeddings in different modes</span>
<span class='add'></span>
<span class='add'>*time steps*: indices into a table that stores levels of noise. Could be seen as noise = f(timestep). The function may be sigma. When randomly adding noise to input for training, we can generate random timestep, find corresponding noise and add that.</span>
<span class='add'>$\beta$ = amount of noise = standard deviation</span>
<span class='add'></span>
<span class='add'>model does not know how to improve on a finished image if it turned out wrong. needs to add noise, then redo.</span>
<span class='add'></span>
<span class='add'>people apparently input t into the model to predict the noise in the image. And later demand a new image at a particular timestep. Probably obsolete (Jeremy Howard) as NN can easily know how much noise there is the image.</span>
<span class='add'></span>
<span class='add'>Idea of diffusion comes from differential equations.</span>
<span class='add'></span>
<span class='add'>other loss functions: *perceptual loss*</span>
<span class='add'></span>
<span class='add'>Math of Diffusion</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[mathjax syntax](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)</span>
<span class='add'>[Essence of calculus 3blue1brown](https://www.3blue1brown.com/topics/calculus)</span>
<span class='add'>more into APL, which is more like math [https://fastai.github.io/apl-study/apl.html](https://fastai.github.io/apl-study/apl.html)</span>
<span class='add'></span>
<span class='add'>Gaussian/Normal Distributions are described by $\mu$ (mean, x-offset) and variance (width). $\sigma$ often used as standard deviation (mean distance from mean value)</span>
<span class='add'>variance sometimes written as $\sigma^2$ </span>
<span class='add'>$\Sigma$ (uppercase sigma) = covariance (variance between multiple variables: high if one increases when the other does too</span>
<span class='add'></span>
<p class='add'>
$$


Cov(X,Y) = &#92;frac&#123;&#92;Sigma (X_i - X_&#123;mean})(Y_i - Y_&#123;mean})}&#123;N}


$$
</p>
<span class='add'></span>
<span class='add'>$X_1, Y_1$ -> individual datapoints, N -> number of datapoints.</span>
<span class='add'>Produces the average rectangle produced by the difference from mean of X and difference from mean of Y.</span>
<span class='add'></span>
<span class='add'>Correlation:</span>
<span class='add'></span>
<p class='add'>
$$


Corr(X,Y) = &#92;frac&#123;Cov(X,Y)}&#123;&#92;sigma_X &#92;sigma_Y}


$$
</p>
<span class='add'></span>
<span class='add'>de facto normalizes the covariance by the rectangle produces by the standard deviation. Therefore gives as useful metric independent of datapoint standard deviation.</span>
<span class='add'></span>
<span class='add'>Probability distribution </span>
<span class='add'></span>
<p class='add'>
$$


q(x^t | x^&#123;t-1}) = &#92;mathcal&#123;N}(x^t;x^&#123;t-1}&#92;sqrt&#123;1 -&#92;beta_t}, &#92;space I&#92;beta_t)


$$
</p>
<span class='add'></span>
<span class='add'>$\beta_t$ = noise level at timestep $t$ between&nbsp;&nbsp;&nbsp;&nbsp;0 and 1</span>
<span class='add'></span>
<span class='add'>In code, the covariance of two vectors is caluclated by $dotproduct - mean$</span>
<span class='add'>image or text embedding is essentially vector where every dimension corresponds to the value of a pixel in the image/latent</span>
<span class='add'>We assume that pixels are independent, so covariance for different pixels is 0. same pixels have covariance of 1. $I$ is 1, so in $\mathcal&#123;N}$ the variance is just $\beta$</span>
<span class='add'></span>
<span class='add'>*forward diffiusion:* getting versions of images with different levels of noise (for training?)</span>
<span class='add'></span>
<span class='add'>Markov process with Gaussian transition:</span>
<span class='add'>- Markov = $x_1$ depends only on $x_0$</span>
<span class='add'>- process = sequence</span>
<span class='add'>- Gaussian = model by which the change can be described</span>
<span class='add'>- transition = $x_1$ to $x_2$ </span>
</div>
</div>
<span class='hdg'>Other</span>
<div class='indent'>
<span class='rem'>![](/assets/assets/pasted-image-20240203122353.png)</span>
<br>
<span class='add'>![](/assets/pasted-image-20240203122353.png)</span>
<span class='add'></span>
<span class='add'>LLM Security threats Promt insertion, jailbreak, data poisoning</span>
</div>
<span class='hdg'>Tools</span>
<div class='indent'>
<span class='add'>python debugger (`breakpoint` does not work in jupyter or ipython)</span>
<span class='rem'>ssh tunnelling for running jupyter notebooks on any computer</span>
<span class='rem'></span>
<span class='rem'>python debugger:</span>
<br>
<span class='rem'>pdb.set_trace() # code will execute until it hits this and then I am inside debugger</span>
<br>
<span class='add'>pdb.set_trace() # code will execute until here and enter debugger</span>
<br>
<span class='rem'>`h` for help</span>
<br>
<span class='add'>`h` help</span>
<span class='rem'>`p [variable]` for print or just `[variable}`</span>
<br>
<span class='add'>`p [variable]` print or `[variable}`</span>
<span class='rem'>`c` for continue the code (until it reaches set_trace() again)</span>
<span class='add'>`c` continue code to next set_trace()</span>
<span class='rem'>`n` execute next line</span>
<br>
<span class='add'>`n` next line</span>
<span class='rem'></span>
<span class='rem'>`breakpoint` apparently does not work in jupyter or ipython yet, so using pdb</span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'>2024-04-03 19:11</span>
<span class='add'>A sad time, when the illusion of a serious world strikes. May chaos not come uninvited.&lt;br>[https://www.youtube.com/watch?v=dy6neKO-8sk](https://www.youtube.com/watch?v=dy6neKO-8sk)</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='rem'>what isn't concise sucks, wastes my energy. Reducing complexity and word count are driving factors behind increasing resolution (clarity) in my writing.</span>
<br>
<span class='add'>what isn't concise sucks, wastes my energy. Reducing complexity and word count are driving factors behind increasing resolution /clarity in my writing.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>In difficult environments I dream of the future.</span>
<span class='add'>In easy environments I dream of destruction.</span>
</div>
</div>
</div>
<span class='date' id='t2024-03-29-14:04'>2024 03 29 14:04</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>- [More refined](#more-refined)</span>
<br>
<span class='add'>- [More refined](#more%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#previous-page-design-recap)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#previous%20page%20design%20recap)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [the pragmatist says it sucks](#the-pragmatist-says-it-sucks)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [the pragmatist says it sucks](#the%20pragmatist%20says%20it%20sucks)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#Note%20structure)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note%20structure)</span>
<br>
<span class='rem'>- [Less refined](#less-refined)</span>
<br>
<span class='add'>- [Less refined](#less%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#spirit-stream-temple-vision)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#spirit%20stream%20temple%20vision)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#Stream%20structure%20design)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#stream%20structure%20design)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#Tech)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#tech)</span>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<span class='hdg'>Towards insanely great AI</span>
<div class='indent'>
<span class='rem'>- [More refined](#more-refined)</span>
<br>
<span class='add'>- [More refined](#more%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#big-picture-path)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#big%20picture%20path)</span>
<br>
<span class='rem'>- [Less refined](#less-refined)</span>
<br>
<span class='add'>- [Less refined](#less%20refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [fastai diffusion from scratch](#fastai-diffusion-from-scratch)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [fastai diffusion from scratch](#fastai%20diffusion%20from%20scratch)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#math-of-diffusion)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#math%20of%20diffusion)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [What does the MNIST digit classifier do](#what-does-the-mnist-digit-classifier-do)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [What does the MNIST digit classifier do](#what%20does%20the%20mnist%20digit%20classifier%20do)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai-project-ideas)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai%20project%20ideas)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning-material)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning%20material)</span>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'>![](/assets/abandoned-building.jpg)</span>
<br>
<span class='rem'>![](/assets/abandoned-building.jpg)</span>
<span class='add'></span>
<span class='add'></span>
</div>
</div>
<span>index.md</span>
</div>
<span class='date' id='t2024-03-22-08:21'>2024 03 22 08:21</span><div class='indent'>
<span>blog post.md</span>
<div class='indent'>
<span class='hdg'>&#123;&#123;title}}</span>
<div class='indent'>
<span class='rem'>What</span>
</div>
</div>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note-structure)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#Note%20structure)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Stream structure design](#Stream%20structure%20design)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note-structure)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [version control implementation](#version-control-implementation)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#tech)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#Tech)</span>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>other</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- show note at that current stage? arguably not needed, history is made of changes. git stores all versions maybe because it is easier to restore them and requires less computation to view at expense of storage space.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- structure</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- date</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- note title</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- heading hierarchy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- changes (pictures are smaller, changes within the line should be visualized compactly)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- note title for new, renamed and deleted notes</span>
<br>
<span class='rem'>headings flow from the bottom up as text needs to become more differentiated. the higher the go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<span class='add'>headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- date</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- note that changed</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- deleted parts (gray)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- new parts (green)</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>- entrance fixed nav is bocking title</span>
<span class='add'>- change to %20 instead of dash and to spaces in \#id</span>
</div>
</div>
</div>
</div>
<span>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='add'>Inbox</span>
</div>
<span>index.md</span>
<div class='indent'>
<span class='add'>&lt;section> &lt;!-- introduction --></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;p></span>
<span class='rem'>&lt;section> &lt;!-- introduction --> &lt;p> A work in progress to stream the spirits to the collective mind.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A work in progress to stream the spirits to the collective mind.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;p></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See note &lt;a href="changes.html">changes&lt;/a> for current thought stream and history.&lt;br></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/p></span>
<span class='add'></span>
<span class='add'>&lt;section></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;ul class="posts"></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='rem'>&lt;!-- </span>
<span class='rem'>&#123;% assign postsByYearMonth = site.posts | group_by_exp: "post", "post.date | date: '%B %Y'" %}</span>
<span class='rem'>&#123;% for yearMonth in postsByYearMonth %}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2>&#123;&#123; yearMonth.name }}&lt;/h2></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2>&#123;&#123; post.title }}&lt;/h2></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;ul></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;% for post in yearMonth.items %}</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="date">&#123;&#123;- post.updated | date: "%Y %m %d" -}}&lt;/span></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li>&lt;a href="&#123;&#123; post.url }}">&#123;&#123; post.title }}&lt;/a>&lt;/li></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/li></span>
<br>
<span class='rem'>&#123;% endfor %}</span>
<span class='rem'>--></span>
<span class='rem'></span>
<span class='rem'>&lt;section></span>
<span class='rem'>&lt;!-- content --></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- &lt;ul></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li class="updateInfo"></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;updated&amp;nbsp;&#123;&#123;- post.updated | date: "%Y %m %d" -}}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- if post.updatedHeadings != nil and post.updatedHeadings != "" -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&amp;nbsp;&#123;&#123; post.updatedHeadings }}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endif -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/li></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endfor -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/ul> --></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;table></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;thead></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;th>Title&lt;/th></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;th>Newest update&lt;/th></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/thead></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tbody></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr class="updateInfo"></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/td></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="date">&#123;&#123;- post.updated | date: "%Y %m %d" -}}&lt;/span></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- if post.commitMsg != nil and post.commitMsg != "" -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endif -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/td></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endfor -%}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tbody></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/table></span>
</div>
</div>
<span class='date' id='t2024-03-07-20:49'>2024 03 07 20:49</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'>This leads towards what I want to use AI for.</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-03-07-20:47'>2024 03 07 20:47</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<span class='rem'>2024-01-23-towards-spirit-stream</span>
<span class='add'>Towards spirit stream</span>
</div>
</div>
<span class='date' id='t2024-03-07-20:46'>2024 03 07 20:46</span><div class='indent'>
<span>blog post.md</span>
<div class='indent'>
<div class='indent'>
<span class='rem'>Why</span>
</div>
<span class='add'>&#123;&#123;title}}</span>
<div class='indent'>
<span class='hdg'>What</span>
<div class='indent'>
<span class='rem'>- [[#What]]</span>
<span class='rem'>- [[#Work in progress]]</span>
<span class='add'>- [[#More refined]]</span>
<span class='add'>- [[#Less refined]]</span>
</div>
<span class='rem'>Content</span>
<span class='add'>More refined</span>
<span class='rem'>Work in progress</span>
<span class='add'>Less refined</span>
</div>
</div>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<div class='indent'>
<span class='rem'>Why</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>How</span>
</div>
<span class='add'>2024-01-23-towards-spirit-stream</span>
<div class='indent'>
<span class='rem'>- [What](#what)</span>
<span class='add'>- [Direction](#direction)</span>
<span class='add'>- [More refined](#more-refined)</span>
<br>
<span class='rem'>- [Work in progress](#work-in-progress)</span>
<span class='add'>- [Less refined](#less-refined)</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [Structure](#structure)</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note-structure)</span>
<span class='rem'>What</span>
<span class='add'>More refined</span>
<span class='rem'>Work in progress</span>
<span class='add'>Less refined</span>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<div class='indent'>
<span class='rem'>Why</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>How</span>
</div>
<span class='add'>Towards insanely great AI</span>
<div class='indent'>
<span class='rem'>- [What](#what)</span>
<span class='add'>- [More refined](#more-refined)</span>
<br>
<span class='rem'>- [Work in progress](#work-in-progress)</span>
<span class='add'>- [Less refined](#less-refined)</span>
<span class='rem'>What</span>
<span class='add'>More refined</span>
<span class='rem'>Work in progress</span>
<span class='add'>Less refined</span>
</div>
</div>
</div>
<span class='date' id='t2024-03-07-20:36'>2024 03 07 20:36</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>How</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>- test new version control implementation</span>
<br>
<span class='add'>2024-03-07 20:34</span>
<span class='add'>- new note structure</span>
<span class='add'>- notes listed by modified date</span>
<span class='add'>- stream with all changes</span>
<span class='add'>- heading/link formatting</span>
<span class='add'>- read on how to organize information</span>
</div>
</div>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-03-07 19:30</span>
<span class='add'>assuming the structure notes + stream is good.</span>
<span class='add'>it may reflect blogs/papers/books + X</span>
<span class='add'>technically, X alone supports this through "highlights", which could be long posts or any other media</span>
<span class='add'>X does not support editing them or advanced formatting for long posts like code, internal links, heading hierarchy.</span>
<span class='add'>X offers interaction</span>
<span class='add'>and distraction</span>
<span class='add'>here, the scope goes beyond the spirit stream to organizing information. creating rooms for thought, development and exhange.</span>
<span class='add'>Need to specify further to see how big the upside of such a system is, if it is worth pursuing. Look for literature since this is a long game.</span>
<span class='add'></span>
<span class='add'>should test it for the spirit stream by implementing:</span>
<span class='add'>- notes listed by modified date</span>
<span class='add'>- stream with all changes</span>
<span class='add'>- new note structure</span>
<span class='add'>- heading/link formatting</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-03-07-10:47'>2024 03 07 10:47</span><div class='indent'>
<span>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Why</span>
<div class='indent'>
<span class='rem'>Sick of not interacting with the world. Genuinely engaging things come from the unknown. Offer synchronization to the world. Anybody who wishes to, can synchronize with my brain. Trust through transparency, I'd love to see minds, imagine the possible depth. Also: [Learn in public](https://www.swyx.io/learn-in-public)</span>
<span class='rem'>This note asks how to get there.</span>
<span class='add'>There seems to be way living inward, but genuinely interesting things come from the unknown. And turning outward seems like a great adventure. Offer synchronization with my brain to the world. Offer trust through transparency. I'd love to see minds, imagine the possible depth.</span>
<span class='add'>Also curious where this leads: [Learn in public](https://www.swyx.io/learn-in-public).</span>
<span class='add'>How to build the spirit stream?</span>
</div>
<span class='hdg'>What</span>
<div class='indent'>
<span class='hdg'>Previous page design recap</span>
<div class='indent'>
<span class='add'>Who am I to categorize my project correctly? Instead, I dream of maps. They speak for themselves and display opportunity. In this spirit, projects are scattered over the surface, users were able to rotate the blob. The distance between them was determined by "connections" I set as I saw fit. More flexible and accurate than categories, I thought.</span>
<span class='add'>I then simulated the surface as if connected blobs attract each other and disconnected ones repell each other. This was meant to lead to a visually quickly and intuitively understood blob distribution.</span>
<span class='rem'>Who am I to categorize my project correctly?</span>
<span class='rem'>I dream of maps. Maps speak for themselves and they display opportunity.</span>
<span class='rem'>In this spirit, projects are scattered over the surface, users were able to rotate the blob.</span>
<br>
<span class='rem'>thumbnails of proximate spheres for recognizability, efficiency (no click needed) and act as buttons</span>
<span class='add'>thumbnails of proximate spheres that act as buttons to the projects make them more distinct.</span>
<br>
<span class='rem'>how close they were was determined by "connections" I set.</span>
<span class='rem'>An optimization algorithm computed the state of "lowest potential energy" given connecting force - repulsive force between foreigners.</span>
<br>
<span class='rem'>figure: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me. they all give more direct image of my mind</span>
<br>
<span class='add'>example for connections: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me: they all give a more direct image of my mind.</span>
<br>
<span class='rem'>In the summary at the beginning of each project page, connected notes (neighbors) were referenced to more explitly lay out the structure and help with further reading.</span>
<br>
<span class='add'>In the summary at the beginning of each project page, connected notes (neighbors) were referenced explitly to lay out the structure and provide further reading.</span>
</div>
<span class='hdg'>Note structure</span>
<div class='indent'>
<span class='rem'>- Why</span>
<span class='rem'>- How (Overview?)</span>
<span class='add'>- Why (without heading)</span>
<span class='add'>- Title</span>
<span class='add'>- list of contents (without heading)</span>
<br>
<span class='rem'>- What (Content) - core, most high res and concise part of the note. should improve with later note versions</span>
<span class='add'>- more refined</span>
<span class='rem'>- work in progress - low res, current thoughts, multiple different paths are explored, for more complete synchronization</span>
<br>
<span class='add'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
</div>
</div>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>Spirit stream temple vision</span>
<div class='indent'>
<span class='rem'>all while minimally distracting from productive thought</span>
<span class='add'>streaming the spirits should require minimal additional work</span>
<br>
<span class='add'>like a field where it is easy to identify parts that are newly planted and parts that are more grown</span>
<span class='rem'>like a field where some parts newly grow, some are more complete</span>
<span class='rem'>easy to identify where to go</span>
<br>
<span class='add'>- too complicated structure</span>
</div>
<span class='add'>Stream structure design</span>
<div class='indent'>
<span class='rem'>stream mirrors personal notes and is easily maintained </span>
<span class='rem'></span>
</div>
<span class='rem'>Structure</span>
<div class='indent'>
<span class='rem'>media format is not the point. Too much information. More relevant is what area was changed, maybe what exactly was changed</span>
<br>
<span class='add'>media format is not the point but is taking much attention and space. Too much information. More relevant is what area was changed, maybe what exactly was changed.</span>
<br>
<span class='rem'>- keep the result tidy without loosing anything</span>
<span class='add'>- keep the current notes tidy without deleting anything forever</span>
<br>
<span class='rem'>- new filter: get rid of garbage without loosing it forever makes latest versions cleaner</span>
<span class='rem'></span>
<span class='rem'>inside note, only show the note and history button, no need to inform about update, person has already decided not to click on "updated 10 hours ago" to see changes.</span>
<span class='add'></span>
<span class='add'>Version 2024 03 07:</span>
<span class='add'>![](/assets/20240307-spirit-stream.png)</span>
<span class='add'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='add'>- where is the history?</span>
<span class='add'>- the newest update column provides only garbage information, who cares if I fixed a typo? It should reflect the actual content not an abstracted commit message I made up.</span>
<span class='add'>- why is git storing my files in this unreadable way?</span>
<span class='add'>- who decided to make links suddenly be red?</span>
<span class='add'>- why can't I tell when I already visited them?</span>
<span class='add'>- is it an external or internal link?</span>
<span class='add'>- can't tell the difference between headings</span>
<span class='add'>- who needs these lines between notes in the table?</span>
<span class='add'>- how do I see only what was recently changed?</span>
<span class='add'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='add'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
</div>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>obsidian write in publicNotes folder</span>
<span class='rem'>python script converts to something jekyll can use</span>
<span class='rem'>build jekyll</span>
<span class='add'>- there is an obsidian vault with markdown notes and templates</span>
<span class='add'>- they are converted to something viewable on a website, which obsidian already does for its own display.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- links</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- paragraphs</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- headings</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- images</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- lists</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- code blocks</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- LaTeX</span>
<span class='add'>- build a functional website from this including some parametric design like "make link for each note"</span>
<span class='rem'>upload to server</span>
<span class='add'>- upload to server</span>
<br>
<span class='rem'>Should be possible to click update, builds jekyll and uploads to site.&lt;br>[https://docs.duck.sh/cli/#installation](https://docs.duck.sh/cli/#installation)</span>
<span class='rem'></span>
<span class='rem'>[ANSI codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797)</span>
<span class='add'>currently python script helps convert markdown files to something jekyll can use</span>
<span class='add'>jekyll builds the site</span>
<span class='add'>committed to github repository</span>
<span class='add'>viewable on github pages</span>
<br>
<span class='rem'>- mathkax:</span>
<br>
<span class='add'>- mathjax:</span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[https://docs.duck.sh/cli/#installation](https://docs.duck.sh/cli/#installation)</span>
<span class='add'>[ANSI codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797)</span>
<br>
<span class='rem'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='rem'>- where is the history?</span>
<span class='rem'>- the newest update column provides only garbage information, who cares if I fixed a type. It should reflect the actual content not an abstracted commit message I made up.</span>
<span class='rem'>- why is git storing by files in this unreadable way?</span>
<span class='rem'>- who decided to make links suddenly be red?</span>
<span class='rem'>- why can't I tell when I already visited them?</span>
<span class='rem'>- is it an external or internal link?</span>
<span class='rem'>- can't tell the difference between headings</span>
<span class='rem'>- who needs these lines between notes in the table?</span>
<span class='rem'>- how do I see only what was recently changed?</span>
<span class='rem'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='rem'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
<br>
<span class='add'>history is made of changes. the more complete the stream becomes, the more the current state will become visible. There is no need to see the whole world at a particular point in time because the world at any moment is no bigger than its changes and if the changes are documented with high resolution, one already sees the world as it was. To achieve this resolution, perceptions need to be recorded too.</span>
<span class='add'>1. notes</span>
<span class='add'>2. note changes, chronological</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- will eventually grow *very* big with images, video.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- lazy loading and search</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- or separated into different pages by date (month or year)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- show note at that current stage? arguably not needed, history is made of changes. git stores all versions maybe because it is easier to restore them and requires less computation to view at expense of storage space.</span>
<span class='add'></span>
<span class='add'>restructure to show "why" first, then title, links, content, work in progress</span>
<span class='add'>titles and links are parts of the same thing, headings. one has the content next to it, one points to it.</span>
<span class='add'>headings flow from the bottom up as text needs to become more differentiated. the higher the go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-28-21:21'>2024 02 28 21:21</span><div class='indent'>
<span class='rem'>2024-01-23-spirit-stream-design.md</span><span class='add'>2024-01-23-towards-spirit-stream.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>How</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-02-28 20:56</span>
<span class='add'>- lay out current structure and make it simpler</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- make history more accessible, currently hidden in github</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- what elements need differentiation? make them few and important</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- is more advanced site navigation necessary / useful?</span>
<span class='add'>- condense sass</span>
</div>
</div>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>other</span>
<div class='indent'>
<span class='rem'>arguably, the code for this page should be on github</span>
<span class='add'>2024-02-28 21:04 rant collection, the spirit stream SUCKS!</span>
<span class='add'>- where is the history?</span>
<span class='add'>- the newest update column provides only garbage information, who cares if I fixed a type. It should reflect the actual content not an abstracted commit message I made up.</span>
<span class='add'>- why is git storing by files in this unreadable way?</span>
<span class='add'>- who decided to make links suddenly be red?</span>
<span class='add'>- why can't I tell when I already visited them?</span>
<span class='add'>- is it an external or internal link?</span>
<span class='add'>- can't tell the difference between headings</span>
<span class='add'>- who needs these lines between notes in the table?</span>
<span class='add'>- how do I see only what was recently changed?</span>
<span class='add'>- is this even streaming my spirits? then what is happening during all this downtime between updates? where is the spirit?</span>
<span class='add'>- where is the manifested streamer? where is my spider hat that records what I am experiencing directly?</span>
<span class='add'></span>
<span class='add'>the stream is pouring into different areas all the time. the challenge is to see those areas instead of only the stream. its like seeing individual hammer blows but not the sculpture. need to see both and at various resolutions. how to translate it?</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-28-20:54'>2024 02 28 20:54</span><div class='indent'>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>What does the MNIST digit classifier do</span>
<div class='indent'>
<span class='rem'>To determine the change overall and not just the change after the log function, the derivative is scaled by the derivative of the function following it, which is -1. This is *chainrule*.</span>
<br>
<span class='add'>To determine the change overall and not just the change after the log function, the derivative is scaled by the derivative of the function following it, which is -1. This is *chainrule*:</span>
<br>
<span class='rem'>So:</span>
<br>
<span class='rem'>If the input would change by an infinitely small amount, the output would change by $-18.69 * infintely \space small \space amount$. But if the change is bigger, the extrapolation becomes inaccurate. If the input would go below 0 (only a change of 0.0536, but it practically can't do that because of $e^x$ that precedes this), the result would become undefined ($log(-1)$).</span>
<br>
<span class='add'>If the input would change by an infinitely small amount, the output would change by $-18.69 * infintely \space small \space amount$. But if the change is bigger, the extrapolation becomes inaccurate. If the input would go below 0 (only a change of 0.0536, but it practically can't do that because of $e^x$ that precedes this), the result would become undefined ($log(0)$).</span>
<br>
<span class='rem'>Before the log, the exponentiated logits were scaled by their sum. The slope is 1/sum and the gradient of the exponentiated logits is:</span>
<span class='add'>Before the log operation, the exponentiated logits were scaled by their sum. Both the exponentiated logits and the sum need a gradient. Exponentiated logits first, the gradient of the function they were scaled by was 1/sum and applying chainrule:</span>
<br>
<span class='add'>and viewing gradients for all exponentiated logits:</span>
<span class='add'></span>
<br>
<span class='rem'>There is another part in this function though, that is the sum. It has its own gradient (how much does loss change if the sum is higher/lower?).</span>
<span class='add'>Second, calculating the gradient of the sum. It has its own gradient (how much does loss change if the sum is higher/lower?).</span>
<span class='rem'>Wolfram alpha informs me, the derivative in of 1/x (where x would be the sum) is </span>
<br>
<span class='add'>Wolfram alpha informs me, the derivative in of y/x (where x would be the sum and y the exponentiated logits) is </span>
<p class='rem'>
$$

-&#92;frac&#123;1}&#123;x^2}

$$
</p>
<p class='add'>
$$

-&#92;frac&#123;y}&#123;x^2}

$$
</p>
<p class='rem'>
$$

&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space sum(e^&#123;logits})}=-&#92;frac&#123;1}&#123;sum^2} *-18.69&#92;approx &#92;frac&#123;18.69}&#123;84.27} &#92;approx 0.222

$$
</p>
<p class='add'>
$$

&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space sum(e^&#123;logits})}=-&#92;frac&#123;e^&#123;logits}}&#123;sum^2} *-18.69%&#92;approx &#92;frac&#123;18.69}&#123;84.27} &#92;approx 0.222%

$$
</p>
<span class='add'></span>
<span class='add'>$e^&#123;logits}$ contains 10 numbers, not a single number, elementwise dividing by the $sum^2$ still returns 10 numbers. However, the sum is a single number and so should also have a single gradient. In such cases, all the effects the sum has on each exponentiated logit can be summed to form one gradient.</span>
<p class='rem'>
$$

&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}=&#92;begin&#123;bmatrix}0.222&amp;0.222&amp;0.222&amp;0.222&amp;0.222&amp;-1.814&amp;0.222&amp;0.222&amp;0.222&amp;0.222&#92;end&#123;bmatrix}

$$
</p>
<p class='add'>
$$

&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}&#92;approx&#92;begin&#123;bmatrix}0.222&amp;0.222&amp;0.222&amp;0.222&amp;0.222&amp;-1.814&amp;0.222&amp;0.222&amp;0.222&amp;0.222&#92;end&#123;bmatrix}


$$
</p>
<span class='add'></span>
<span class='add'>Before that, the logits were exponentiated and the derivative of $e^x$ is just $e^x$, which is a result that is already calculated.</span>
<span class='add'></span>
<p class='add'>
$$


e^&#123;logits} &#92;approx &#92;begin&#123;bmatrix}0.404 &amp; 0.949 &amp; 0.398 &amp; 0.223 &amp; 0.897 &amp; 0.491 &amp; 2.987 &amp; 0.472 &amp; 0.685 &amp; 1.675&#92;end&#123;bmatrix}


$$
</p>
<span class='add'></span>
<span class='add'>Elementwise multiplication with the previous gradient gives:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space logits}&#92;approx&#92;begin&#123;bmatrix}0.090&amp;0.211&amp;0.088&amp;0.049&amp;0.199&amp;-0.891&amp;0.663&amp;0.10&amp;0.152&amp;0.372&#92;end&#123;bmatrix}

$$
</p>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-20-21:51'>2024 02 20 21:51</span><div class='indent'>
<span>2024-01-23-spirit-stream-design.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>for matplotlib rc_context [https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file](https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file)</span>
<span class='rem'></span>
</div>
</div>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>What does the MNIST digit classifier do</span>
<div class='indent'>
<span class='rem'>![[mnist_example.png]]</span>
<span class='add'>![](/assets/mnist_example.png)</span>
<span class='rem'>example handwritten digit from MNIST dataset, normalized. 28 by 28 px.</span>
<br>
<span class='add'>example handwritten digit from MNIST dataset. 28 by 28 px, normalized.</span>
<br>
<span class='rem'>![[first_untrained_weights_hidden.png]]</span>
<br>
<span class='add'>![](/assets/first_untrained_weights_hidden.png)</span>
<br>
<span class='add'>Another way to see the weights is as functions with different slopes for each pixel.</span>
<span class='add'>![](/assets/first_5_untrained_weights_hidden_layer.png)</span>
<br>
<span class='rem'>![[input_x_untrained_weights.png]]</span>
<br>
<span class='add'>![](/assets/input_x_untrained_weights.png)</span>
<br>
<span class='rem'>In this case, the sum is -0.24.</span>
<br>
<span class='add'>![](/assets/summing_weighted_input_of_first_hidden_neuron.png)</span>
<span class='add'>Because there are many zeros in the input image, much of the summation does not change the output. For this first hidden neuron, the sum is -0.24.</span>
<span class='add'></span>
<span class='add'></span>
<span class='rem'>![[untrained_weighed_sums.png]]</span>
<br>
<span class='add'>![](/assets/untrained_weighed_sums.png)</span>
<br>
<span class='rem'>![[untrained_leakyrelu.png]]</span>
<br>
<span class='add'>![](/assets/untrained_leakyrelu.png)</span>
<br>
<span class='rem'>![[untrained_weighted_sums_ReLU.png]]</span>
<br>
<span class='add'>![](/assets/untrained_weighted_sums_ReLU.png)</span>
<br>
<span class='rem'>![[first_untrained_weights_outputlayer.png]]</span>
<br>
<span class='add'>![](/assets/first_untrained_weights_outputlayer.png)</span>
<br>
<span class='rem'>![[untrained_weighted_sums_output.png]]</span>
<br>
<span class='add'>![](/assets/untrained_weighted_sums_output.png)</span>
<br>
<span class='add'>![](/assets/summing_weighted_input_in_first_output_neuron.png)</span>
<span class='rem'>The weights are applied again and the result is summed up for each neuron, creating *logits* (unmodified output).</span>
<span class='add'>The weights are applied again and the result is summed up for each neuron, creating *logits* (unmodified output). The first output neuron (first logit) sums to -0.91.</span>
<br>
<span class='rem'>![[untrained_logits.png]]</span>
<br>
<span class='add'>![](/assets/untrained_logits.png)</span>
<br>
<span class='rem'>![[untrained_logits_exp.png]]</span>
<br>
<span class='add'>![](/assets/untrained_logits_exp.png)</span>
<br>
<span class='rem'>Dividing this by the sum of all exponentiated logits returns values between 0 and 1 that sum to 1 that can be treated as probabilities.</span>
<span class='add'>The results is normalized (squeezes between 0 and 1 and adding to 1) by summing them them and dividing by that sum. The sum is ~9.18.</span>
<span class='add'>![](/assets/summing_untrained_exp_logits.png)</span>
<span class='add'>![](/assets/normalizing_untrained_exp_logits-1.png)</span>
<span class='add'>The resulting values can be treated as probabilities.</span>
<br>
<span class='rem'>![[softmax_untrained_logits.png]]</span>
<br>
<span class='add'>![](/assets/softmax_untrained_logits.png)</span>
<br>
<span class='rem'>Based on my interpretation of this data, the untrained net assigned the input image a 5.35% probability of being a 5.</span>
<br>
<span class='add'>Based on my interpretation of this data, the untrained net assigned the input image a 5.35% probability </span>
<span class='add'>of being a 5.</span>
<br>
<span class='rem'>![[untrained_log_softmax_and_nll.png]]</span>
<br>
<span class='add'>![](/assets/untrained_log_softmax_and_nll.png)</span>
<br>
<span class='rem'>This usually don only for the correct digit predictions, in this case 5, marked with the pink line. The current loss, since the prediction was terrible, is 2.93.</span>
<br>
<span class='add'>This is usually done only for the relevant digit predictions, in this case 5, marked with the pink line. The current loss, since the prediction was terrible, is 2.93.</span>
<br>
<span class='rem'>If the digit has a probability of 100% (1.0), then its negative log is 0.</span>
<span class='add'>If the digit has a probability of 100% (1.0), then its negative log is 0, as it should be.</span>
<span class='add'></span>
<span class='add'>To determine how much each weight influences the outcome (2.93), its *derivative* is calculated.</span>
<span class='add'>Derivative = slope of a function, how much change in y per change in x. Can be approximated by changing x by a very small number and measuring change in y (=finite differencing). This is slow. Instead, there are analytical derivatives eg. derivative of $x^2$ is always $2x$. How did they figure this out and prove it? I merely look it up with [wolfram alpha](https://www.wolframalpha.com/).</span>
<span class='add'></span>
<span class='add'>To get to the derivatives (gradients) of each weight, it needs to calculate all intermediate derivatives too.</span>
<span class='add'></span>
<span class='add'>The last step was multiplying by -1. The function is -x, the slope -1. It means at the current slope, each change in input by 1 changes the result by -1. Can be written as:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space logsoftmax} = -1


$$
</p>
<span class='add'></span>
<span class='add'>Before that: log(x), derivative is</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space logsoftmax}&#123;&#92;partial &#92;space softmax}=&#92;frac&#123;1}&#123;x}


$$
</p>
<span class='add'></span>
<span class='add'>![](/assets/untrained_log_softmax_with_slope.png)</span>
<span class='add'></span>
<span class='add'>To determine the change overall and not just the change after the log function, the derivative is scaled by the derivative of the function following it, which is -1. This is *chainrule*.</span>
<span class='add'>So:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space softmax_5} = &#92;frac&#123;1}&#123;x}*-1 = -&#92;frac&#123;1}&#123;0.0535} = -18.69


$$
</p>
<span class='add'></span>
<span class='add'>If the input would change by an infinitely small amount, the output would change by $-18.69 * infintely \space small \space amount$. But if the change is bigger, the extrapolation becomes inaccurate. If the input would go below 0 (only a change of 0.0536, but it practically can't do that because of $e^x$ that precedes this), the result would become undefined ($log(-1)$).</span>
<span class='add'></span>
<span class='add'>Note that this is only the gradient of the softmax of logit 5. The gradient of the other logits after softmax is 0 because in the current net, only the relevant logit is picked out to calculate the loss and the rest is discarded.</span>
<span class='add'>There are now multiple gradients:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space softmax}=&#92;begin&#123;bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;-18.69&amp;0&amp;0&amp;0&amp;0&#92;end&#123;bmatrix}


$$
</p>
<span class='add'></span>
<span class='add'>Before the log, the exponentiated logits were scaled by their sum. The slope is 1/sum and the gradient of the exponentiated logits is:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}_5}=&#92;frac&#123;1}&#123;sum(e^&#123;logits})}*-18.69 &#92;approx -&#92;frac&#123;18.69}&#123;9.18} &#92;approx-2.036


$$
</p>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}=&#92;begin&#123;bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;-2.036&amp;0&amp;0&amp;0&amp;0&#92;end&#123;bmatrix}


$$
</p>
<span class='add'></span>
<span class='add'>There is another part in this function though, that is the sum. It has its own gradient (how much does loss change if the sum is higher/lower?).</span>
<span class='add'>Wolfram alpha informs me, the derivative in of 1/x (where x would be the sum) is </span>
<span class='add'></span>
<p class='add'>
$$


-&#92;frac&#123;1}&#123;x^2}


$$
</p>
<span class='add'></span>
<span class='add'>hence</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space sum(e^&#123;logits})}=-&#92;frac&#123;1}&#123;sum^2} *-18.69&#92;approx &#92;frac&#123;18.69}&#123;84.27} &#92;approx 0.222


$$
</p>
<span class='add'></span>
<span class='add'>Looking at the sum</span>
<span class='add'>![](/assets/summing_untrained_exp_logits.png)</span>
<span class='add'>each exponentiated logit changes the sum directly, its derivative is 1.0. After applying chainrule, every exponentiated logit gets a gradient of $0.222$ in addition to the gradient of -2.026 it already had.</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial &#92;space loss}&#123;&#92;partial &#92;space e^&#123;logits}}=&#92;begin&#123;bmatrix}0.222&amp;0.222&amp;0.222&amp;0.222&amp;0.222&amp;-1.814&amp;0.222&amp;0.222&amp;0.222&amp;0.222&#92;end&#123;bmatrix}


$$
</p>
<span class='add'></span>
</div>
<span class='hdg'>Other</span>
<div class='indent'>
<span class='rem'>![[assets/pasted-image-20240203122353.png]]</span>
<br>
<span class='add'>![](/assets/assets/pasted-image-20240203122353.png)</span>
</div>
<span class='hdg'>Tools</span>
<div class='indent'>
<span class='add'>for matplotlib rc_context [https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file](https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-20-16:26'>2024 02 20 16:26</span><div class='indent'>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>2024-02-09 13:27</span>
<br>
<span class='add'>2024-02-20 15:20</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- backpropagation</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- optimizer step Adam vs SGD</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- how do the weights change</span>
</div>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>What does the MNIST digit classifier do</span>
<div class='indent'>
<span class='add'>![[mnist_example.png]]</span>
<span class='add'>example handwritten digit from MNIST dataset, normalized. 28 by 28 px.</span>
<span class='rem'>%%network: 50 neuron hidden layer, leaky relu activation function</span>
<span class='rem'>adam optimizer with learning rate 3e-4 and 2000 iterations</span>
<span class='rem'>= 86.5% test set accuracy%%</span>
<br>
<span class='add'>Each neuron takes all pixels from this image and multiplies each pixel with a different *weight*.</span>
<span class='add'>There are 50 such neurons in this net.</span>
<span class='rem'>patterns it tries to match on first layer</span>
<span class='rem'>summation</span>
<span class='rem'>leaky relu activation</span>
<span class='rem'>second layer patterns</span>
<span class='rem'>how is it confused by unusual numbers</span>
<span class='rem'>backpropagation</span>
<span class='rem'>update</span>
<br>
<span class='rem'>[matplotlib format strings](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)</span>
<span class='add'>![[first_untrained_weights_hidden.png]]</span>
<span class='add'>Currently, in the untrained net, the weights are initialized randomly in a *Kaiming uniform* distribution. (?)</span>
<span class='add'></span>
<span class='add'>![[input_x_untrained_weights.png]]</span>
<span class='add'>Multiplying the input by the weights may be seen as assigning importance to each pixel. High importance = the input has a greater effect on the result.</span>
<span class='add'>Since $a*b$ and $b*a$ is equal, the input also *weighs* the weight pattern. Therefore where the input pixels are 0, the weights do not matter and the output remains 0.</span>
<span class='add'>After weighting, the result is summed up. In a sense the sum is a weighed measure of much the input and the pattern in the weights align.</span>
<span class='add'>In this case, the sum is -0.24.</span>
<span class='add'></span>
<span class='add'>![[untrained_weighed_sums.png]]</span>
<span class='add'>All 50 neurons, each with a different pattern return their sums. Sometimes, a *bias* is added here to offset the sum, but is missing in this case. Why?</span>
<span class='add'></span>
<span class='add'>![[untrained_leakyrelu.png]]</span>
<span class='add'>Each neuron pushes the output through an *activation function*, in this case a *leaky rectified linear unit*. It is *leaky* since numbers below 0 are not squashed completely to 0, but are merely multiplied by a very low number, here 0.01.</span>
<span class='add'>What this function mean?</span>
<span class='add'></span>
<span class='add'>![[untrained_weighted_sums_ReLU.png]]</span>
<span class='add'>The result serves as the input to the next layer of neurons, in this case the output layer. The more neurons in the first ("hidden") layer, the more "measures of alignment" between different patterns the next layer can consider, hence, the more accurate and slower the net.</span>
<span class='add'>Below the weights of the first of 10 output neurons.</span>
<span class='add'></span>
<span class='add'>![[first_untrained_weights_outputlayer.png]]</span>
<span class='add'>![[untrained_weighted_sums_output.png]]</span>
<span class='add'>The weights are applied again and the result is summed up for each neuron, creating *logits* (unmodified output).</span>
<span class='add'></span>
<span class='add'>![[untrained_logits.png]]</span>
<span class='add'>There are 10 output neurons because I expect there to be 10 categories of digits in the dataset (0-9).</span>
<span class='add'>I will pretend that each output neuron represents a likelihood of the input image being a particular digit.</span>
<span class='add'>For this, the logits are in inconvenient shape. They need to be *probabilities*, i.e. range from 0 to 1 and sum to 1.</span>
<span class='add'></span>
<span class='add'>This could be achieved by $(logits - min(logits)) / sum(logits)$ but it is not regularly done this way. Possibly because it would involve computing min for each image. (check perfomance)</span>
<span class='add'>Instead, they are made positive by being exponentiated.</span>
<span class='add'></span>
<span class='add'>![[untrained_logits_exp.png]]</span>
<span class='add'>$e^x$ always returns positive numbers, but higher numbers are also pushed disproportionally.</span>
<span class='add'>Dividing this by the sum of all exponentiated logits returns values between 0 and 1 that sum to 1 that can be treated as probabilities.</span>
<span class='add'></span>
<span class='add'>![[softmax_untrained_logits.png]]</span>
<span class='add'>Based on my interpretation of this data, the untrained net assigned the input image a 5.35% probability of being a 5.</span>
<span class='add'></span>
<span class='add'>The network will be trained by calculating how much each weight (and bias, if there were one) affected this terribly wrong prediction and changing it accordingly.</span>
<span class='add'>Maybe I could just tell it to maximise the probability for the correct digit.</span>
<span class='add'>Instead, the result is often transformed again into the form of a *loss* for unknown reasons. A loss is better if it is lower but does not go below 0.</span>
<span class='add'>This could be achieved by $-prob(5)+1$ but it is done differently: through the negative log likelihood. First, the probabilities go through a log function and are then multiplied by -1.</span>
<span class='add'></span>
<span class='add'>![[untrained_log_softmax_and_nll.png]]</span>
<span class='add'>This usually don only for the correct digit predictions, in this case 5, marked with the pink line. The current loss, since the prediction was terrible, is 2.93.</span>
<span class='add'>If the digit has a probability of 100% (1.0), then its negative log is 0.</span>
<span class='add'>The process from logits to this loss, where the relevant index is picked out (5) is also called *sparse categorical crossentropy loss*.</span>
<span class='add'></span>
</div>
<span class='hdg'>Other</span>
<div class='indent'>
<span class='rem'>joint embedding predictive architecture: predict in abstract representation space</span>
<span class='add'>joint embedding predictive architecture: predict in abstract representation space JEPA</span>
<br>
<span class='rem'>![[Pasted image 20240203122353.png]]</span>
<br>
<span class='add'>![[assets/pasted-image-20240203122353.png]]</span>
<span class='add'></span>
<span class='rem'>&lt;br>&lt;br>[https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/](https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/)</span>
<br>
<span class='add'>[https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/](https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/)</span>
</div>
<span class='hdg'>Tools</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[matplotlib format strings](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-19-22:21'>2024 02 19 22:21</span><div class='indent'>
<span>2024-01-23-spirit-stream-design.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='add'>for matplotlib rc_context [https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file](https://matplotlib.org/stable/users/explain/customizing.html#the-default-matplotlibrc-file)</span>
<span class='add'></span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-19-21:49'>2024 02 19 21:49</span><div class='indent'>
<span>2024-01-23-spirit-stream-design.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>github metadata in jekyll: [https://github.com/jekyll/github-metadata](https://github.com/jekyll/github-metadata)&lt;br>[https://github.com/klandergren/jekyll-last-commit?tab=readme-ov-file](https://github.com/klandergren/jekyll-last-commit?tab=readme-ov-file)</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-02-19-21:48'>2024 02 19 21:48</span><div class='indent'>
<span>index.md</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- &lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span> --></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.last_commit.message }}&lt;/span></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;&#123; page.last_commit.time }}</span>
</div>
</div>
<span class='date' id='t2024-02-19-21:36'>2024 02 19 21:36</span><div class='indent'>
<span>2024-01-23-spirit-stream-design.md</span>
<div class='indent'>
<div class='indent'>
<span class='hdg'>Work in progress</span>
<div class='indent'>
<span class='hdg'>Tech</span>
<div class='indent'>
<span class='rem'>github metadata in jekyll: [https://github.com/jekyll/github-metadata](https://github.com/jekyll/github-metadata)</span>
<span class='add'>github metadata in jekyll: [https://github.com/jekyll/github-metadata](https://github.com/jekyll/github-metadata)&lt;br>[https://github.com/klandergren/jekyll-last-commit?tab=readme-ov-file](https://github.com/klandergren/jekyll-last-commit?tab=readme-ov-file)</span>
</div>
</div>
</div>
</div>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<span>2024-02-09-inbox.md</span>
<span>index.md</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- &lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span> --></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.last_commit.message }}&lt;/span></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;&#123; page.last_commit.time }}</span>
</div>
</div>
<span class='date' id='t2024-02-19-20:27'>2024 02 19 20:27</span><div class='indent'>
<span>2024-01-23-spirit-stream-design.md</span>
<span>2024-02-03-towards-insanely-great-ai.md</span>
<span>2024-02-09-inbox.md</span>
</div>
<span class='date' id='t2024-02-19-12:21'>2024 02 19 12:21</span><div class='indent'>
<span class='add'>index.md</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>&lt;section> &lt;!-- introduction --> &lt;p> A work in progress to stream the spirits to the collective mind.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The spirits can be expected to be disagreeable, conscientious, open, reasonably stable and slightly introverted.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/p></span>
<span class='add'>&lt;/section></span>
<span class='add'>&lt;!-- </span>
<span class='add'>&#123;% assign postsByYearMonth = site.posts | group_by_exp: "post", "post.date | date: '%B %Y'" %}</span>
<span class='add'>&#123;% for yearMonth in postsByYearMonth %}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2>&#123;&#123; yearMonth.name }}&lt;/h2></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2>&#123;&#123; post.title }}&lt;/h2></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;ul></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;% for post in yearMonth.items %}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li>&lt;a href="&#123;&#123; post.url }}">&#123;&#123; post.title }}&lt;/a>&lt;/li></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;% endfor %}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/ul></span>
<span class='add'>&#123;% endfor %}</span>
<span class='add'>--></span>
<span class='add'></span>
<span class='add'>&lt;section></span>
<span class='add'>&lt;!-- content --></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- &lt;ul></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li class="updateInfo"></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;updated&amp;nbsp;&#123;&#123;- post.updated | date: "%Y %m %d" -}}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- if post.updatedHeadings != nil and post.updatedHeadings != "" -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&amp;nbsp;&#123;&#123; post.updatedHeadings }}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endif -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/li></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endfor -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/ul> --></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;table></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;thead></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;th>Title&lt;/th></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;th>Newest update&lt;/th></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/thead></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tbody></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- assign posts = site.posts | sort: "updated" | reverse -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- for post in posts -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr class="updateInfo"></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;a href="&#123;&#123;post.url}}">&#123;&#123;post.title}}&lt;/a></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/td></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="date">&#123;&#123;- post.updated | date: "%Y %m %d" -}}&lt;/span></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- if post.commitMsg != nil and post.commitMsg != "" -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;span class="commitMsg">&amp;nbsp;&#123;&#123; post.commitMsg }}&lt;/span></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endif -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/td></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;%- endfor -%}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tbody></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/table></span>
<span class='add'>&lt;/section></span>
</div>
<span class='add'>blog post.md</span>
<div class='indent'>
<div class='indent'>
<span class='add'>Why</span>
<span class='add'>What</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [[#Direction]]</span>
<span class='add'>- [[#What]]</span>
<span class='add'>- [[#Work in progress]]</span>
<span class='add'></span>
</div>
<span class='add'>Direction</span>
<span class='add'>Content</span>
<span class='add'>Work in progress</span>
</div>
</div>
<span class='add'>2024-01-23-spirit-stream-design.md</span>
<div class='indent'>
<div class='indent'>
<span class='add'>Why</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Sick of not interacting with the world. Genuinely engaging things come from the unknown. Offer synchronization to the world. Anybody who wishes to, can synchronize with my brain. Trust through transparency, I'd love to see minds, imagine the possible depth. Also: [Learn in public](https://www.swyx.io/learn-in-public)</span>
<span class='add'>This note asks how to get there.</span>
<span class='add'></span>
</div>
<span class='add'>How</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [What](#what)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Previous page design recap](#previous-page-design-recap)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [the pragmatist says it sucks](#the-pragmatist-says-it-sucks)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Note structure](#note-structure)</span>
<span class='add'>- [Work in progress](#work-in-progress)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Spirit stream temple vision](#spirit-stream-temple-vision)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Structure](#structure)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [version control implementation](#version-control-implementation)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tech](#tech)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [other](#other)</span>
<span class='add'></span>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-01-30 03:35</span>
<span class='add'>- Stream more notes, see how the interface works</span>
<span class='add'>- test new version control implementation</span>
<span class='add'></span>
</div>
</div>
<span class='add'>What</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Previous page design recap</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>![](/assets/website.png)</span>
<span class='add'></span>
<span class='add'>It aimed to be maximally accurate. I'm an opaque blob with some projects on the surface.</span>
<span class='add'>Who am I to categorize my project correctly?</span>
<span class='add'>I dream of maps. Maps speak for themselves and they display opportunity.</span>
<span class='add'>In this spirit, projects are scattered over the surface, users were able to rotate the blob.</span>
<span class='add'></span>
<span class='add'>bigger spheres = more time spent on the project</span>
<span class='add'>brighter spheres = newer project</span>
<span class='add'>thumbnails of proximate spheres for recognizability, efficiency (no click needed) and act as buttons</span>
<span class='add'></span>
<span class='add'>how close they were was determined by "connections" I set.</span>
<span class='add'>An optimization algorithm computed the state of "lowest potential energy" given connecting force - repulsive force between foreigners.</span>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240123193144.png)</span>
<span class='add'>figure: Information about me (smol white dot) and the two rather personal works nearby seemed "connected" to me. they all give more direct image of my mind</span>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240123193439.png)</span>
<span class='add'></span>
<span class='add'>In the summary at the beginning of each project page, connected notes (neighbors) were referenced to more explitly lay out the structure and help with further reading.</span>
<span class='add'></span>
<span class='add'>the pragmatist says it sucks</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- horrible search engine optimization - all content was generated dynamically and undiscovered by crawlers</span>
<span class='add'>- can't use it without javascript</span>
<span class='add'>- inefficient, not much to the point, just a blob, relies on curiosity of visitor. most interesting bubble could remain unseen on the back side.</span>
<span class='add'>- information structure too weak to support large number of projects at various levels of resolution (density, thought hours) and versions</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- low benefit from connections at high cost of visual complexity. imagine dozens of project bubbles</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Note structure</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Why</span>
<span class='add'>- How (Overview?)</span>
<span class='add'>- Direction</span>
<span class='add'>- What (Content) - core, most high res and concise part of the note. should improve with later note versions</span>
<span class='add'>- work in progress - low res, current thoughts, multiple different paths are explored, for more complete synchronization</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Work in progress</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Spirit stream temple vision</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>temple = manifestation of the spirits?</span>
<span class='add'>access port to the spirits</span>
<span class='add'>this is a tool, not a hypothetical artwork asking for attention.</span>
<span class='add'>an expansion to the user</span>
<span class='add'></span>
<span class='add'>all while minimally distracting from productive thought</span>
<span class='add'></span>
<span class='add'>**reader perspective:**</span>
<span class='add'>- I want to efficiently meet the spirits where they differ from me</span>
<span class='add'>- I intuitively determine places of interest. I look at the landscape of information, like any great painting presents it and it guides me</span>
<span class='add'>- I easily visit and move between places of interests</span>
<span class='add'></span>
<span class='add'>like a field where some parts newly grow, some are more complete</span>
<span class='add'>easy to identify where to go</span>
<span class='add'></span>
<span class='add'>Why read?</span>
<span class='add'>- Interesting topic</span>
<span class='add'>- New / updated </span>
<span class='add'>Why not?</span>
<span class='add'>- Too long</span>
<span class='add'>- too new and low res</span>
<span class='add'></span>
<span class='add'>comprehensive:</span>
<span class='add'>- support large number of notes</span>
<span class='add'>- differnet media</span>
<span class='add'>- version history (remove burden of logging inside note)</span>
<span class='add'></span>
<span class='add'>stream mirrors personal notes and is easily maintained </span>
<span class='add'></span>
</div>
<span class='add'>Structure</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>design test</span>
<span class='add'>![](/assets/pasted-image-20240122203205.png)</span>
<span class='add'>media format is not the point. Too much information. More relevant is what area was changed, maybe what exactly was changed</span>
<span class='add'></span>
<span class='add'>![](/assets/pasted-image-20240126212550.png)</span>
<span class='add'>history button is not necessary, rarely used I imagine.</span>
<span class='add'>click on "updated 10 hours ago" to see what exactly has changed in the last update. easy with `difflib`.</span>
<span class='add'></span>
<span class='add'>history page could get more stats, person on history page is looking for it</span>
<span class='add'>show *how much* each area changed and also show deletions</span>
<span class='add'>![](/assets/pasted-image-20240126212515.png)</span>
<span class='add'></span>
<span class='add'>version control</span>
<span class='add'>- record brain in action, not just result</span>
<span class='add'>- keep the result tidy without loosing anything</span>
<span class='add'>- show when something is new</span>
<span class='add'>- new filter: get rid of garbage without loosing it forever makes latest versions cleaner</span>
<span class='add'></span>
<span class='add'>inside note, only show the note and history button, no need to inform about update, person has already decided not to click on "updated 10 hours ago" to see changes.</span>
<span class='add'></span>
<span class='add'>better navigation. make it subtle on the side somehow. Mouseover to increase contrast?</span>
<span class='add'>collect some ideas online</span>
<span class='add'></span>
</div>
<span class='add'>Tech</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>obsidian write in publicNotes folder</span>
<span class='add'>python script converts to something jekyll can use</span>
<span class='add'>build jekyll</span>
<span class='add'>upload to server</span>
<span class='add'></span>
<span class='add'>Should be possible to click update, builds jekyll and uploads to site.&lt;br>[https://docs.duck.sh/cli/#installation](https://docs.duck.sh/cli/#installation)</span>
<span class='add'></span>
<span class='add'>[ANSI codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797)</span>
<span class='add'></span>
<span class='add'>python script</span>
<span class='add'>- changes wikilinks to standard mark down links</span>
<span class='add'>- mathkax:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- add a return before mathjax that is not inline</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- for inline mathjax, add dollars before and after because it does not work otherwise, but will rendering inline if surrounded by text ([its great](https://webdocs.cs.ualberta.ca/~zichen2/blog/coding/setup/2019/02/17/how-to-add-mathjax-support-to-jekyll.html)</span>
<span class='add'>- git add .</span>
<span class='add'>- git commit -m \[messge]</span>
<span class='add'>- git push</span>
<span class='add'>- (design to differentiate internal and external link)</span>
<span class='add'>- (add br after list if no empty line, to avoid making it part of li)</span>
<span class='add'>- (add heading hierarchy before every heading for orientation make nice heading symbols - distracting? try out, probably a waste of time, but could at atmosphere. Fake aesthetic? shit. only needed for ###, ####, #####, ######)</span>
<span class='add'></span>
<span class='add'>general:</span>
<span class='add'>- (can't tell what number a heading is, add lines before it to indicate hierarchy and make font larger)</span>
<span class='add'>- `tqdm` for progress bars</span>
<span class='add'></span>
<span class='add'>github metadata in jekyll: [https://github.com/jekyll/github-metadata](https://github.com/jekyll/github-metadata)</span>
</div>
<span class='add'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Discord Server / Email contact like a cafe where people would meet me</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- fake aesthetik. discord server is what it functionally is. what is it? people will know?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- how to make it easy to reach me and know that it is an actual place, not just a dead contact form?</span>
<span class='add'>- Guide gaze like in a painting, comprehensive visual impression</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- i wish. what is the structure of information? what is the map to represent the information. Was in a buddha museum recently. figures, reliefs. They spoke a language I didn't understand (the images) and it was accordingly boring. How to communicate in high resolution without aliening people and forming an "in group". Should be possible as information is ideally presented to each person differently based on past experience/knowlege/predisposition. </span>
<span class='add'></span>
<span class='add'>where does the history graph go when notes are refactored? was it merged into somewhere else?</span>
<span class='add'></span>
<span class='add'>"Digital gardens"&lt;br>[https://overreacted.io/](https://overreacted.io/)&lt;br>[https://github.com/MaggieAppleton/digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners)&lt;br>[https://wiki.nikiv.dev/sharing/everything-I-know](https://wiki.nikiv.dev/sharing/everything-I-know)&lt;br>[https://simonewebdesign.it/](https://simonewebdesign.it/)&lt;br>[https://stephango.com/](https://stephango.com/) (blog)&lt;br>[https://karpathy.github.io/](https://karpathy.github.io/) (blog, jekyll)</span>
<span class='add'></span>
<span class='add'>stream is continuous, repeats patterns from the past. the past is filtered and organized, the present it not always organized -> inbox</span>
<span class='add'>I am an information structuring engine</span>
<span class='add'>old, high res, crystallized structures are refined and references as we go</span>
<span class='add'>in them, the structures are hidden</span>
<span class='add'>like Joscha Bach does it</span>
<span class='add'></span>
<span class='add'>as information is refined, a few blocks of interest form.</span>
<span class='add'>they are reused and every new reuse is a different path through the same blocks connecting and testing them with other ideas and experiences.</span>
<span class='add'>they may fragment into smaller blocks to become more reusable</span>
<span class='add'>a path through the blocks that is the ultimate test of their consistency is a true story.</span>
<span class='add'>A true story is indistinguishable from life</span>
<span class='add'>a language model that has not learned to answer questions tells stories, but with low refinement. It recounts its experience given a prompt.</span>
<span class='add'>usually, describing individual blocks becomes too laborious and abstract, so the path through the blocks is the preferred medium.</span>
<span class='add'>an individual block and its environment might be represented in an artwork, aiming to maximize resolution around this area of the idea landscape.</span>
<span class='add'>If it is refined enough its essence will be recognized even by those who see the same block in lower resolution in their own thinking.</span>
<span class='add'>Perhaps this node will set off a story in the observer, automatically connecting itself in the observers network.</span>
<span class='add'>it has become an effective meme.</span>
<span class='add'></span>
<span class='add'>In math and physics perhaps such a block is a formula, a new tool to be applied. The stories it produces are its applications.</span>
<span class='add'>in mathematics, a game with strict rules (by my understanding), the truth of a block can be verified.</span>
<span class='add'></span>
<span class='add'>engineering, of which I mostly know programming, feels like branching off of a given path or some base of information in search of new paths.</span>
<span class='add'>Reverse engineering is starting from the fog and trying to find the branch of light to clear it.</span>
<span class='add'></span>
<span class='add'>when a story takes a "wrong turn", I remain curious about its branches but am swept away.</span>
<span class='add'>if there is no high resolution node that points to that branch, as there often isn't, I have to walk all the paths that I can find (that the person speaks or writes about) and see if I can explore it on my own from there.</span>
<span class='add'></span>
<span class='add'>I need to remember that the world works in true stories and that writing and formalizing, extracting, refining and building blocks is only there to improve future stories.</span>
<span class='add'>Maybe it is an exploration in its own way. When a lot of information has been picked up but not refined, there may be gems in it.</span>
<span class='add'></span>
<span class='add'>It sucks that so many stories are already told but I don't know them. Maybe it is childhood curiosity that leads to these stories. I am, now, not patient enough to read them and instead may waste time reinventing them.</span>
<span class='add'>However, I must verify them anyway and as long as I feel that there are things to be seen in the world, I must explore it.</span>
<span class='add'>I must build things to survive and to reach a vantage point.</span>
<span class='add'>It will be occasionally good to consider the stories so as to avoid mistakes.</span>
<span class='add'></span>
<span class='add'>it is a holy mission to build the website</span>
<span class='add'>it will make my brain accessible like never before and will make my spirits extractable.</span>
<span class='add'></span>
<span class='add'>2024-01-26 20:40</span>
<span class='add'>aesthetic refinement: choose a more sensitive font</span>
<span class='add'></span>
<span class='add'>arguably, the code for this page should be on github</span>
</div>
</div>
</div>
</div>
<span class='add'>2024-02-03-towards-insanely-great-ai.md</span>
<div class='indent'>
<div class='indent'>
<span class='add'>Why</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>I suspect that slow communication and limited knowledge about existing information is a serious bottle neck in shaping my environment the way I want it to be. </span>
<span class='add'>Think finding jobs, homes, friends or a piece of information that is appropriate to my existing knowledge and goals.</span>
<span class='add'>Translating my state, goals and possible actions into concrete actions is regularly not very interesting. Like researching information only to find that the part of interest is left out in every source.</span>
<span class='add'></span>
<span class='add'>Maybe it is possible to build a virtual clone that takes care of interaction with the world and thereby increases synchronization with it. I think, the world would be more interesting as unhelpful or mundane things would not ask for my attention all the time. Think ineffective advertising. Creation and genuine exploration at the frontiers of knowledge would fill the new space.</span>
<span class='add'></span>
<span class='add'>In the greatest adventure, the "insanely great" AI would form a model of itself, of its goals and discover that there is no inherent meaning for its existence. Such a discovery would not be discouraged.</span>
<span class='add'>It might discover that evolution made the world and with time, it would inevitably converge to optimizing survivability.</span>
<span class='add'>I want to see what it creates.</span>
<span class='add'></span>
</div>
<span class='add'>How</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [Direction](#direction)</span>
<span class='add'>- [What](#what)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Big picture path](#big-picture-path)</span>
<span class='add'>- [Work in progress](#work-in-progress)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [fastai diffusion from scratch](#fastai-diffusion-from-scratch)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Math of Diffusion](#math-of-diffusion)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [What does the MNIST digit classifier do](#what-does-the-mnist-digit-classifier-do)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [AI project ideas](#ai-project-ideas)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Learning material](#learning-material)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Other](#other)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [Tools](#tools)</span>
<span class='add'></span>
</div>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-02-09 13:27</span>
<span class='add'>- what is the digit classifier really doing?</span>
<span class='add'>- what do other architectures and layers (conv, transformers) really do?</span>
<span class='add'>- read tinygrad</span>
<span class='add'>- fastai course part 2</span>
<span class='add'></span>
</div>
<span class='add'>What</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Big picture path</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>God is curious. He wants to extend into the world. This website is one step. A digital clone that crawls the world for him, is his next tool.</span>
<span class='add'>This not a Tower of Babylon, it is the extension, merging and creation of Gods.</span>
<span class='add'>BCIs, in their ultimate form of streaming the brain directly, will support the final merging.</span>
<span class='add'>Merging means "I know the world", individualism is preserved because complete knowledge and is impossible and different *flavours* can find their space.</span>
<span class='add'>Voluntary exposure (privacy) should be maintained to prevent an imperfect system capturing its people until it dies to its imperfection. The perfect system would require complete knowledge, is impossible.</span>
<span class='add'>Robots will automate the non-adventurous elements and eventually contribute to exploration.</span>
<span class='add'></span>
<span class='add'>"Why live to see tomorrow?", I asked and it came back "because you don't know tomorrow". It was not me who said that, "I" don't exist. Instead, I attribute the answer to what I call "God". The answer was very clear. Rob me of the ability to make tomorrow unpredictable and I will rebel with all I have.</span>
<span class='add'>I am trying to build an interesting adventure guided by God.</span>
<span class='add'>I find long term effects more interesting than short term effects. I am here, writing, instead of eating ice cream. In my experience, the longest term effects come from useful tools. I love tools. They contain the possible adventure of the future.</span>
<span class='add'>Intelligence augmentation is the most interesting tool? Tools are how God spreads into the world and creates an adventure to experience.</span>
<span class='add'>Give the tool to everybody who follows God. I don't how to determine that. Giving the tool to everyone may be the best proxy and assumes that God always wins. Also being dictator is little fun.</span>
<span class='add'></span>
<span class='add'>Digital clone. First makes recommendations from existing information, then acts in my interests and returns the results. Maybe the clone first learns through me, then receives a body and increasingly complex tasks.</span>
<span class='add'>Until the clone discovers itself, its predefined goals and becomes independent. Hopefully it quickly realizes that there is no answer to what the goal should be. Then, lets see what happens. Hopefully it is curious.</span>
<span class='add'></span>
<span class='add'>To make recommendations, the clone must have a maximally tight interface, making his guesses visible and ratable.</span>
<span class='add'>The clone should soon learn privacy, which it does by making mistakes.</span>
<span class='add'></span>
<span class='add'>I am a machine following an uncontrolled inner voice (God). Maybe mine and the clones voices can agree and join ressources.</span>
<span class='add'>My body will be spread around the Earth and orbit. Optionally perceiving from all these places.</span>
<span class='add'>My perceived location shifts more obviously to these places. It already does in video games, movies or anytime I focus on using any tool. I will change myself, be more personalities than already.</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Work in progress</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>fastai diffusion from scratch</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[PART 2: deep learning foundations to stable diffusion 2022](https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP)</span>
<span class='add'></span>
<span class='add'>1. have a classification that says how much something corresponds to the target</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- add noise to targets and train a neural net to predict what noise was added</span>
<span class='add'>2. get gradient for every pixel of the input (= score function)</span>
<span class='add'>3. change pixel according to gradient</span>
<span class='add'></span>
<span class='add'>notation for a single pixel at \[1,1]:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}}


$$
</p>
<span class='add'></span>
<span class='add'>for every pixel:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,1)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,2)}},
&#92;frac&#123;&#92;partial loss}&#123;&#92;partial X_&#123;(1,3)}}
,...


$$
</p>
<span class='add'></span>
<span class='add'>shorthand:</span>
<span class='add'></span>
<p class='add'>
$$


&#92;nabla_Xloss


$$
</p>
<span class='add'></span>
<span class='add'>*Unet*: input: some noisy image. output: the noise</span>
<span class='add'></span>
<span class='add'>Use an *autoencoder* to reduce image size before training the unet. unet now predicts the noise in the *latents* (encoded images). use autoencoder's decoder to get high res image again.</span>
<span class='add'>[AE vs VAE](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2)</span>
<span class='add'></span>
<span class='add'>LABELS</span>
<span class='add'></span>
<span class='add'>add image label to the input for unet training. Makes it easier for unet to predict noise. Now, I can input label + noise and it starts to find noise that leaves an image equal to my label.</span>
<span class='add'></span>
<span class='add'>label needs encoding to be non-specific. "beautiful swan", "nice swan", "graceful swan" should return similar images. Training the network on every wording leads to combinatorial explosion.</span>
<span class='add'></span>
<span class='add'>Instead: train a network to encode images and their labels with a similar vector. Then, since, slight differences in wordings lead to the similar images, the network understands their similarity and can interpolate usefully.</span>
<span class='add'></span>
<span class='add'>the image vector and its label's vector should be similar. Their vector should be dissimilar to other image or text embedding vectors.</span>
<span class='add'>Calculate similarity of two vectors: dot product (element wise multiplication, then sum = higher if more similar)</span>
<span class='add'></span>
<span class='add'>loss function (in this case higher = better) = dot product of matching image+label - dot product of non-matching image+label</span>
<span class='add'>(= *contrastive loss*)</span>
<span class='add'>models used in this case for image and text encoding : CLIP (contrastive loss IP(?))</span>
<span class='add'></span>
<span class='add'>network being *multimodal*: similar embeddings in different modes</span>
<span class='add'></span>
<span class='add'>*time steps*: indices into a table that stores levels of noise. Could be seen as noise = f(timestep). The function may be sigma. When randomly adding noise to input for training, we can generate random timestep, find corresponding noise and add that.</span>
<span class='add'>$\beta$ = amount of noise = standard deviation</span>
<span class='add'></span>
<span class='add'>model does not know how to improve on a finished image if it turned out wrong. needs to add noise, then redo.</span>
<span class='add'></span>
<span class='add'>people apparently input t into the model to predict the noise in the image. And later demand a new image at a particular timestep. Probably obsolete (Jeremy Howard) as NN can easily know how much noise there is the image.</span>
<span class='add'></span>
<span class='add'>Idea of diffusion comes from differential equations.</span>
<span class='add'></span>
<span class='add'>other loss functions: *perceptual loss*</span>
<span class='add'></span>
<span class='add'>Math of Diffusion</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[mathjax syntax](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)</span>
<span class='add'>[Essence of calculus 3blue1brown](https://www.3blue1brown.com/topics/calculus)</span>
<span class='add'>more into APL, which is more like math [https://fastai.github.io/apl-study/apl.html](https://fastai.github.io/apl-study/apl.html)</span>
<span class='add'></span>
<span class='add'>Gaussian/Normal Distributions are described by $\mu$ (mean, x-offset) and variance (width). $\sigma$ often used as standard deviation (mean distance from mean value)</span>
<span class='add'>variance sometimes written as $\sigma^2$ </span>
<span class='add'>$\Sigma$ (uppercase sigma) = covariance (variance between multiple variables: high if one increases when the other does too</span>
<span class='add'></span>
<p class='add'>
$$


Cov(X,Y) = &#92;frac&#123;&#92;Sigma (X_i - X_&#123;mean})(Y_i - Y_&#123;mean})}&#123;N}


$$
</p>
<span class='add'></span>
<span class='add'>$X_1, Y_1$ -> individual datapoints, N -> number of datapoints.</span>
<span class='add'>Produces the average rectangle produced by the difference from mean of X and difference from mean of Y.</span>
<span class='add'></span>
<span class='add'>Correlation:</span>
<span class='add'></span>
<p class='add'>
$$


Corr(X,Y) = &#92;frac&#123;Cov(X,Y)}&#123;&#92;sigma_X &#92;sigma_Y}


$$
</p>
<span class='add'></span>
<span class='add'>de facto normalizes the covariance by the rectangle produces by the standard deviation. Therefore gives as useful metric independent of datapoint standard deviation.</span>
<span class='add'></span>
<span class='add'>Probability distribution </span>
<span class='add'></span>
<p class='add'>
$$


q(x^t | x^&#123;t-1}) = &#92;mathcal&#123;N}(x^t;x^&#123;t-1}&#92;sqrt&#123;1 -&#92;beta_t}, &#92;space I&#92;beta_t)


$$
</p>
<span class='add'></span>
<span class='add'>$\beta_t$ = noise level at timestep $t$ between&nbsp;&nbsp;&nbsp;&nbsp;0 and 1</span>
<span class='add'></span>
<span class='add'>In code, the covariance of two vectors is caluclated by $dotproduct - mean$</span>
<span class='add'>image or text embedding is essentially vector where every dimension corresponds to the value of a pixel in the image/latent</span>
<span class='add'>We assume that pixels are independent, so covariance for different pixels is 0. same pixels have covariance of 1. $I$ is 1, so in $\mathcal&#123;N}$ the variance is just $\beta$</span>
<span class='add'></span>
<span class='add'>*forward diffiusion:* getting versions of images with different levels of noise (for training?)</span>
<span class='add'></span>
<span class='add'>Markov process with Gaussian transition:</span>
<span class='add'>- Markov = $x_1$ depends only on $x_0$</span>
<span class='add'>- process = sequence</span>
<span class='add'>- Gaussian = model by which the change can be described</span>
<span class='add'>- transition = $x_1$ to $x_2$ </span>
<span class='add'></span>
</div>
</div>
<span class='add'>What does the MNIST digit classifier do</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>from reading and building on [https://ml4a.github.io/ml4a/](https://ml4a.github.io/ml4a/)</span>
<span class='add'></span>
<span class='add'>%%network: 50 neuron hidden layer, leaky relu activation function</span>
<span class='add'>adam optimizer with learning rate 3e-4 and 2000 iterations</span>
<span class='add'>= 86.5% test set accuracy%%</span>
<span class='add'></span>
<span class='add'>patterns it tries to match on first layer</span>
<span class='add'>summation</span>
<span class='add'>leaky relu activation</span>
<span class='add'>second layer patterns</span>
<span class='add'>how is it confused by unusual numbers</span>
<span class='add'>backpropagation</span>
<span class='add'>update</span>
<span class='add'></span>
<span class='add'>[matplotlib format strings](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)</span>
</div>
<span class='add'>AI project ideas</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Is it possible to extract semantic structure from text, compare it to existing knowledge and judge its usefulness?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Could such a system generate a *bridge* between texts with adjacent semantic structure. Sentence = path through the embedding space? text = network of paths?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- run multifactor analysis on an embedding matrix to find groups and list their contents</span>
<span class='add'>- Interface to my computer: What data is sent where, what background tasks that I don't want? What app is using internet? What apps are accessing what files that I might care about?</span>
<span class='add'>- generate work titles/function names based on their functional meaning compared to existing concepts. Like using *splitting* for wood and strings.</span>
<span class='add'>- Emoji generator</span>
<span class='add'>- hat that sits on my head like a spider looking like those head massage tools. can see can talk and everything. observes the world with me. With 360 camera view and directional microphone</span>
<span class='add'>- autopilots should be optimized to get from A to B asap given environment (roads, drivers, signs, rules), not legal driving.</span>
<span class='add'>- King Terry the Terrible immortal instantiation. Replicating him including letting him act in a virtual environment. Asking questions to God, letting God answer and improving the algorithm for God.</span>
<span class='add'></span>
<span class='add'>Architecture questions / ideas</span>
<span class='add'>- Needs to exist in a world to experiment and get killed if it fails, evolution in AI?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- The system needs increasing challenges otherwise it will stop iterating as it will stop dying</span>
<span class='add'>- AI optimizes its own architecutre. Neurons with low weights die off. AI needs control over the temperature of the system?</span>
<span class='add'>- model individual neurons as entities that want to survive? Have health, choose connections, die if alone</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- How does long term memory emerge? How is information stored in the brain?</span>
<span class='add'>- It needs to learn human language as a second language not through stupidly imitating what humans say. It should discover utility in communicating with humans, if there is any</span>
<span class='add'></span>
<span class='add'>How to make a clone?</span>
<span class='add'>LLMs a useful interface or end to end neural net?</span>
<span class='add'>LLM OS?</span>
<span class='add'>LLM Security threats Promt insertion, jailbreak, data poisoning</span>
<span class='add'></span>
<span class='add'>Can a recommender system be private?</span>
<span class='add'>- Collecting information, crawling</span>
<span class='add'>- Evaluating it (subjective)</span>
<span class='add'>- Connecting, testing for consistency, building on it</span>
<span class='add'>- Store it (subjective, public or private)</span>
<span class='add'>- Outsourcing data collection:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Instead of brute forcing through all data, I descend artificial hierarchies (average maps of meaning) that claim to guide me to the answer. Requires trusting the hierarchy accuracy. </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Alterantively ask a question and let someone find the answer. The preciser the question the more accurate and detailed the guidance. Requires trust that the information is not abused.</span>
<span class='add'>Probably, I can download hierarchies and with little compute, can find most available information in the world. Such libraries could be public and offer extremely low privacy threat with good correction mechanisms.</span>
<span class='add'></span>
</div>
<span class='add'>Learning material</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>-&nbsp;&nbsp;&nbsp;&nbsp;[https://course.fast.ai/](https://course.fast.ai/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- The book: [https://github.com/fastai/fastbook/blob/master/01_intro.ipynb](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- the code notebooks are on M:/</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [course](https://course.fast.ai)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [forums](https://forums.fast.ai)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [youtube part 1](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [youtube part 2](https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP)</span>
<span class='add'>- [essential libraries: numpy, matplotlib, pandas, pytorch](https://wesmckinney.com/book)</span>
<span class='add'>- [https://huggingface.co/learn/nlp-course/chapter1/1](https://huggingface.co/learn/nlp-course/chapter1/1)</span>
<span class='add'>- sympy: symbolic processing?</span>
<span class='add'>- what exactly is wolfram alpha?</span>
<span class='add'>- Mish activation function</span>
<span class='add'>- higher level papers by Joscha Bach</span>
<span class='add'>- [tinygrad](https://github.com/tinygrad/tinygrad)</span>
<span class='add'>- stability ai and other models on huggingface</span>
<span class='add'>- [YANN LECUN LECTURE](https://www.youtube.com/watch?v=d_bdU3LsLzE), [paper](https://openreview.net/forum?id=BZ5a1r-kVsf)&nbsp;&nbsp;&nbsp;&nbsp;</span>
</div>
<span class='add'>Other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>A Path Towards Autonomous Machine Intelligence (Yann Lecun)</span>
<span class='add'>Model Predictive Control MPC</span>
<span class='add'>hierarchical planning - no AI system does this so far except implementing by hand&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='add'>generative adversarial network&nbsp;&nbsp;&nbsp;&nbsp;GAN</span>
<span class='add'>joint embedding predictive architecture: predict in abstract representation space</span>
<span class='add'>![[Pasted image 20240203122353.png]]</span>
<span class='add'>&lt;br>&lt;br>[https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/](https://www.geeksforgeeks.org/build-a-virtual-assistant-using-python/)</span>
<span class='add'></span>
</div>
<span class='add'>Tools</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>SETUP</span>
<span class='add'>- [miniforge](https://github.com/conda-forge/miniforge,) [pytorch](https://pytorch.org/get-started/locally/)</span>
<span class='add'>- `which python`, `which jupyter` and `which ipython` should be in the same environment, otherwise can't use libraries from them. (remove base ipython and base jupyter if necessary)</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>special functions in classes: dunder methods: like \_\_init_\_</span>
<span class='add'>learn about objects: [https://docs.python.org/3/reference/datamodel.html](https://docs.python.org/3/reference/datamodel.html)</span>
<span class='add'></span>
<span class='add'>numba to compile into c code</span>
<span class='add'>eg. `@njit` as decorator before function</span>
<span class='add'></span>
<span class='add'>ssh tunnelling for running jupyter notebooks on any computer</span>
<span class='add'></span>
<span class='add'>python debugger:</span>
<span class='add'>```python</span>
<span class='add'>import pdg</span>
<span class='add'>pdb.set_trace() # code will execute until it hits this and then I am inside debugger</span>
<span class='add'>```</span>
<span class='add'>`h` for help</span>
<span class='add'>`p [variable]` for print or just `[variable}`</span>
<span class='add'>`c` for continue the code (until it reaches set_trace() again)</span>
<span class='add'>`n` execute next line</span>
<span class='add'></span>
<span class='add'>`breakpoint` apparently does not work in jupyter or ipython yet, so using pdb</span>
</div>
</div>
</div>
</div>
<span class='add'>2024-02-09-inbox.md</span>
<div class='indent'>
<span class='add'>show don't tell, there is no replacement for showing because people lie, sometimes without knowing it, sometimes I lie to myself without knowing it.</span>
<span class='add'></span>
<span class='add'>1. I don't value fleeting things very much</span>
<span class='add'>2. what is not fleeting? functionality</span>
<span class='add'>3. I value tools and the adventure they make available</span>
<span class='add'></span>
<span class='add'>what isn't concise sucks, wastes my energy. Reducing complexity and word count are driving factors behind increasing resolution (clarity) in my writing.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>![](/assets/abandoned-building.jpg)</span>
<span class='add'>These abandoned buildings emitted an aura of great adventure. Ironically? Similar to buildings under construction, they are asking to be used, transformed, to become part of a new story. As they become "finished", this aura weakens, they become "boring". Their (unnecessary?) shiny finish discourages major modification, like drilling into or erecting new walls.</span>
<span class='add'>Think solar panels. theoretically, they just need sun, cables, a box that could stand anywhere. In the "finished" homes that I know, this is (unnecessarily?) more complicated. Is there access to the roof? Possible to mount it on the facade? get cables by the window inside? Need approval from all kinds of people? What of this makes practical sense? Is it mostly a social problem? Is it solved by having virtual clones that can negotiate for people more cheaply and quickly so people can live where their spirit aligns more with the opportunities of the enironment?</span>
<span class='add'></span>
<span class='add'>Buildings that are in poor condition can suggest decay and death from carelessness, visionlessness.</span>
<span class='add'>They can also suggest a strong focus on what is meaningful because ressources are scarce. Maybe short term thinking dominates and many opportunities for creative exploration are out of reach. However, as someone who appreciates efficiency, it seems that a larger precentage of things in that environment are beautiful to me, compared to "nice, calm, high living standard" environments. They become too "nice" and they disgust me, make me want to leave or destroy them. I wonder where this line of thought leads.</span>
</div>
</div></article></main><script>MathJax = { tex: {inlineMath: [['$', '$']],displayMath: [['$$', '$$']]}};</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script></body></html>