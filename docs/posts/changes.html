<!DOCTYPE html><html lang=en><head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Changes</title>
<link rel="stylesheet" href="../main.css">
<link rel="shortcut icon" href="../favicon.ico"></head><body><main><nav><a href='../index.html'>Entrance</a></nav><article>
<h1>Changes</h1>
<p>15 newest committed changes to all notes sorted new -> old like: date > note > heading > changed lines (gray lines = deletions, orange lines = replacements or new additions).</p>
<p>
2024 10 18
<a href='#t2024-10-18-10:45'>10:45</a>
|
2024 10 08
<a href='#t2024-10-08-12:50'>12:50</a>
|
2024 09 30
<a href='#t2024-09-30-20:50'>20:50</a>
|
2024 09 17
<a href='#t2024-09-17-13:43'>13:43</a>
|
2024 09 16
<a href='#t2024-09-16-21:52'>21:52</a>
|
2024 09 15
<a href='#t2024-09-15-09:32'>09:32</a>
<a href='#t2024-09-15-09:26'>09:26</a>
|
2024 09 10
<a href='#t2024-09-10-10:18'>10:18</a>
|
2024 09 01
<a href='#t2024-09-01-20:44'>20:44</a>
|
2024 08 12
<a href='#t2024-08-12-20:12'>20:12</a>
|
2024 08 11
<a href='#t2024-08-11-11:15'>11:15</a>
|
2024 07 16
<a href='#t2024-07-16-15:35'>15:35</a>
</p>
<span class='date' id='t2024-10-18-10:45'>2024 10 18 10:45</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-10-18 08:44 Spirit stream integration II</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>The notes on Spirit stream, AI, tinygrad, Microprocessors appear to circle the same thing (integration of spirits and atoms) and should be merged. Drafting a structure here.</span>
<span class='add'></span>
<span class='add'>I am rendered experience (spirits, external perceptions, loops of thought to increase coherence)</span>
<span class='add'></span>
<span class='add'>![](attachments/Matissedance.jpg)</span>
<span class='add'>*La Danse (Henri Matisse, 1910)*</span>
<span class='add'></span>
<span class='add'>Negotiating between the spirits to form an approx. coherent meta-spirit (dance) and dancing in it is a great source of meaning and adventure. The goal is integrity (can be accessed anywhere) and the outcome is truly unknown (scary, exciting, much more interesting than lying propaganda to maintain arbitrary ideology).</span>
<span class='add'></span>
<span class='add'>Integrity as a goal limits the self/ego and is can exist in any situation, including in evil or fear. Both reflect life and can be appreciated as such.</span>
<span class='add'>For this open-outcome journey I appreciate people and tools that are true to their nature (spirits if humans, use/meaning if tools).</span>
<span class='add'></span>
<span class='add'>The spirits and metaspirits (memes) exist on a free market. The free market always wins because it has the most accurate and flexible connection to reality.</span>
<span class='add'></span>
<span class='add'>The spirits and the market benefit from coherence: listening to the spirits and experience and improving world model accuracy.</span>
<span class='add'></span>
<span class='add'>My spirits want to increase coherence, shape my environment to reflect their relationship to the physical world. An environment that adapts to them and responds with its own spirit.&nbsp;&nbsp;&nbsp;&nbsp;When the environment and other spirits are integrated to coherence, they are colonized. If the link becomes strong enough, they become enter the "self" seamlessly. This is how they colonized the body. They want to colonize the universe.</span>
<span class='add'></span>
<span class='add'>Create open, accessible, honest tools that create and extend spirits and link them with high resolution, high speed interfaces.</span>
<span class='add'>Avoid lock-ins, ideology, disintegrity, dishonesty.</span>
<span class='add'></span>
<span class='add'>Extend into computers</span>
<span class='add'>- Interface &lt;- Software (spirits)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Outgoing (express memes)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- (personal) language</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- structuring into linearized stories</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- compiler into sharing friendly format</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- runtime (server software)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Incoming (scrape, decompile, filter, structure and translate for individual minds)</span>
<span class='add'>- Electronics (computation substrate) &lt;- Hardware (body)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Chips, brains</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Brain-computer-interfaces, server hardware, satellites, fibre cables</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Actuators, Sensors</span>
<span class='add'>- Physics</span>
<span class='add'></span>
<span class='add'></span>
</div>
<span class='add'>2024-10-14 10:10 Spirit stream integration</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>There is a state of mind where I recognize that I don't own myself. That "I" don't exist. Rather, I am experience and the structure of the world (brain, body, universe) merely acts through me. Meditation is looking closely and waking up from the dream to see that the world is constructed all by itself in my mind, that I am taken along on a ride through experience.</span>
<span class='add'></span>
<span class='add'>There, when I meet other people and am truly present, their expressions enter my perception similar to my own. But now the underlying structure is not only my own and the apparently physical world, but there are spirits acting in it: another space to be explored. To be poked at, challenged, supported. If the spirits come to agree and when exchange is saturated, effectively two minds are now entering the world, realizing themselves.</span>
<span class='add'>Thus, with genuine curiosity, minds can be explored and possibly merged with.</span>
<span class='add'>When the spirits reveal themselves openly and the rest of the world is allowed to reveal itself as it is, more can be explored, understood, made coherent, built on.</span>
<span class='add'>I don't understand where I come from. Expressing it, following it is truly adventurous.</span>
<span class='add'>There is a way of recognizing something beautiful even in darkness. Where the perceptions are reflective of an underlying structure. Where a spirit roams free.</span>
<span class='add'>With reason, structures are built. An approximately coherent image. It includes the world, my own spirits and plans on how to realize the spirits.</span>
<span class='add'>Great danger comes from excluding spirits.</span>
<span class='add'>What Peterson might see in the bible is a pre-negotiated set of rules which offers a path of broad coherence. In a sense, it is another spirit that can be negotiated with, that can extend my own spirit. It also provides a language to think of the spirits. This language throws me off and I often don't recognize myself in it.</span>
<span class='add'></span>
<span class='add'>Coherent underestanding of the spirits and the world serves fulfillment of the spirits. Towards coherence, the relationships and distinctions in spirit and world model become increasingly precise. Much like resolution increasing, except it is not pixel resolution where few pixels already have precise colors that only gain detail, but it is conceptual resolution, where connections become increasingly accurate and the image can change significantly over time.</span>
<span class='add'></span>
<span class='add'>These abstract structures are never complete because they are trying to formalize something currently deeply unknown: the structure of the mind after evolution (mind became a reflection of the universe itself) and randomness.</span>
<span class='add'></span>
<span class='add'>As abstract structures are translated into increasingly concrete languages, they enter stories. Story is a journey through the structure as it is applied to the world. Stories can come from many angles and also serve verification of the structure.</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-10-08-12:50'>2024 10 08 12:50</span><div class='indent'>
<span>Microprocessors.md</span>
<div class='indent'>
<span class='add'>Following my working definition of beauty (truthful, open reflection of underlying character / properties / identity), I like computers as a potential beauty-machine. Build a beautiful beauty machine. Then, see it wander off into the New and Unpredicted. Fuck comfort. I am already dead when I recognize that the elusive self is the destination.</span>
<span class='add'>Build the beautiful machine, extend into it, let it extend into me.</span>
<span class='add'>There is a tech stack to map and integrate such that the result can be reflective of the universe.</span>
<span class='add'>I hate technicalities and nomenclature. Ugliness. The aim is to extract the principles, the self in the universe and the universe in the self.</span>
<span class='add'>To the unknown.</span>
<span class='hdg'>Microprocessors</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='add'>The Art of Electronics</span>
<div class='indent'>
<span class='add'>(Paul Horowitz, Winfield Hill, 2015)</span>
<span class='add'></span>
<span class='add'>(there are many ways to implement logic gates but electronics seems to be the dominant one, so on to electronics!)</span>
<span class='add'></span>
<span class='add'>In some substrates, there are electrons loosely bound to their atom cores. They are presumably far away from it.</span>
<span class='add'>If there is an electron surplus on one side and an electron deficit on the other, they will leave their atom and flow in the direction of the deficit. Electrons are said to be negatively charged. They repell each other (away from the surplus) and are attracted to positive charge (towards the deficit, where positively charged protons outweigh the electrons).</span>
<span class='add'>An imbalance in electrons can be generated in various ways. One is rubbing certain materials and for some reason the heat will cause some electrons from one material to jump to the other.</span>
<span class='add'>The actual electron flow is slow, but the "wave" (electromagnetic wave) travels at 50-99% light speed.</span>
<span class='add'>More loose electrons = better conductor.</span>
<span class='add'>Resistors, sometimes made from both conductors and insulators impede flow by forcing barriers into the path. Here, energy is lost and heat emitted.</span>
<span class='add'></span>
<span class='add'>Rate of flow $I$ = current \[charge/second or "amps A"] ("I" from "intensity" which is only confusing. Also charge is measured in coulombs, which is equivalent to a bunch of electrons)</span>
<span class='add'>"pressure" $V$ = voltage \[energy/charge or "volts V"]. Always applies between two points. There is no such thing as absolute voltage. There is an amount of energy per electron, called electronvolt. Moving electrons around requires non-linear energy. so energy/charge increases as the delta energy increases.</span>
<span class='add'>There any connection to batteries charging slowly?</span>
<span class='add'>resistance indicating how much the material impedes flow $R$ \[ohms $\Omega$ or Joule-seconds/charge² which is not intuitive at all!]</span>
<span class='add'>also conductance \[Siemens $S$] which is just inverse of resistance.</span>
<span class='add'>Power $P$ neatly fit into place with $V*I$ being giving energy / second or Watts $W$.</span>
<span class='add'></span>
<span class='add'>Then kirchhoffs laws</span>
<span class='add'>and ohms law</span>
<span class='add'>(observations)</span>
<span class='add'>which together explain parallel and series circuits' behaviour</span>
<span class='add'></span>
<span class='add'>? maxwells equations -> eletromagnetism</span>
<span class='add'>? coulombs law for forces between charged stuff</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
</div>
</div>
</div>
</div>
<span>Towards spirit stream.md</span>
<div class='indent'>
<span class='rem'>Temple = access port to the spirits</span>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'>Note structure</span>
<span class='add'>Site structure</span>
<div class='indent'>
<span class='rem'>- Why (without heading)</span>
<span class='rem'>- Title</span>
<span class='rem'>- list of contents (without heading)</span>
<span class='add'>- H1 Aspirational spirit stream of Lorin Baumgarten</span>
<span class='add'>- Introduction</span>
<span class='add'>- Quick Guide</span>
<span class='add'>- List of notes sorted by "most recently modified"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- note structure:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Text describing why the note exists</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- H1 Title</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- list of contents</span>
<span class='rem'>- Direction</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- H2 Direction</span>
<br>
<span class='rem'>- more refined</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- H2 more refined</span>
<br>
<span class='rem'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- H2 less refined - current thoughts, multiple different paths being explored, for more complete synchronization. </span>
</div>
<span class='rem'>Stream structure derivation</span>
<span class='add'>Stream components derivation</span>
<div class='indent'>
<span class='rem'>refining means passing through the blocks from different angles, connecting them and testing for consistency. they may fragment into smaller blocks to become more reusable</span>
<br>
<span class='add'>refining means passing through the blocks from different angles, connecting them and testing for coherence. they may fragment into smaller blocks to become more reusable.</span>
<span class='add'></span>
<span class='add'>Rendering the ideas into less complex and more concrete language is handy to differentiate and condense them.</span>
<span class='add'>There is always an incentive for more forms of expression and increasingly precise vocabulary (symbols, words, video, images, 3D, animation,...). But the difficulty in expression seems to be crystallizing the implicit, drawing boundaries and most ideas can probably be compressed into a tiny vocabulary.</span>
<br>
<span class='add'>Sharing requires rendering into a common language. Physical world can be a common vocabulary, content is then direct experience. This precedes necessity for abstract symbols.</span>
<span class='rem'>Looking for an environment that responds back. Video games respond. It may be my fault that my environment does not very much respond. I seek a group to create things. I don't know where to find it.</span>
<span class='rem'>The current path says "Working on something to build groups, and large ones, I may find a group and then the tools may enable the group to grow".</span>
<br>
<span class='rem'>In exchange for participation, one gets response. Who feels unable or unwilling to respond meaningfully, can offer reach, credit or money.</span>
<span class='add'>Sharing requires a scope of recipients.</span>
<span class='add'>Precise scope selection becomes unpractical with many participants and most scopes seem to be either very private or very public.</span>
<span class='add'>Scopes can also overlap and be one-way or two-way. Ugly They are an attempt to categorize the unknown. There should be a less ugly solution.</span>
<span class='add'></span>
<span class='add'>The spirit stream is both a room to build out and live in and feedback generator. Feedback can take various forms: Ideas, extended reach, money and more.</span>
<span class='add'>Looking for an environment that responds back. Video games respond. I seek a group to create things. I don't know where to find it. So I create and see where it leads.</span>
<br>
<span class='rem'>Probably, extreme capitalism is indistinguishable from normal group dynamics - it forwards the rules of reality to the user without middlemen - , but with some communication tools, extendable to an infinitely large group.</span>
<span class='add'>Probably, extreme capitalism is indistinguishable from traditional group- or "mind internal" dynamics.</span>
<span class='add'>It forwards the rules of reality to the user without middlemen - , but with some communication tools, possilby extendable to an infinitely large group / mind.</span>
<br>
<span class='add'>Trust (predictability, tracjectory estimation) benefits greatly from keeping a history of the spirit stream.</span>
<br>
<span class='add'>if reuse is easy enough, cooperation works without shared files which require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='add'>In an information rich environment, betrayal and exploitation are easily detected and not worth it. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='add'>if the system becomes corrupted, it should be extremely easy to fork and rebuild it somewhere else.</span>
<span class='add'></span>
<span class='add'>Components</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>>Thinking about the ideal blogging platform:&lt;br>&lt;br>1. Writing: &lt;br>- in markdown&lt;br>- with full WYSIWYG, not just split view (think: Typora)&lt;br>- super easy to copy paste and add images&lt;br>2. Deploying:&lt;br>- renders into static pages (think: Jekyll)&lt;br>- super simple, super minimal html with no bloat&lt;br>-…&amp;mdash; Andrej Karpathy (@karpathy) [January 27, 2024](https://twitter.com/karpathy/status/1751350002281300461)</span>
<span class='add'></span>
<span class='rem'>In summary, the spirit stream has two parts: </span>
<br>
<span class='add'>In summary, the spirit stream has:</span>
<span class='rem'>- the stream (hammer blows)</span>
<span class='rem'>- the structure->blocks->stories the author is building through the stream (sculpture). Maybe the structure is a physical tool or a space of "pinned" ideas to repeat going through and refining them.</span>
<span class='add'>- content with "resolutions" between "raw stream" (hammer blows) to specific ideas -> blocks -> stories (sculptures). Content can take any form including physical products.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- rendered in personal vocablary and structure</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- transmission-optimized version for sharing</span>
<span class='add'>- content history, optionally maintained and shared</span>
<br>
<span class='rem'>its development is furthered by:</span>
<span class='rem'>- abitrary direct exchange, private or public</span>
<span class='add'>On the other side, the reader is responsible for rendering the content into a suitable (personal) language (content -> decoding with vocabulary -> render engine) and for navigating it (filtering, finding most appropriate sources).</span>
<span class='add'>Most content tries to pre-render for the reader (calendars switch days automatically, have some "mind"(program) of their own. Recommendation algorithms try to emulate my attention patterns. Same for references in a book). This is cheap and ugly.</span>
<span class='add'>- can only use commonly defined languages, most of which I don't speak and can't verify</span>
<span class='add'>- guessing what interface the reader might prefer, automatically pissing off others and possibly limiting functionality for "simplicity"</span>
<span class='add'>- guessing what previous knowledge and interests the reader has, boring some and overwhelming others.</span>
<span class='add'>Imo the "true" solution to this is client side AI that filters and translates optimally for the user (different product to the spirit stream).</span>
<span class='add'>Don't build dystopian content moderation, but build user-owned tools to scrape and translate the interent for them.</span>
<span class='add'>Scraping the internet is more expensive than using services to "notify" me of news and link to appropriate media.</span>
<span class='add'></span>
<span class='add'>The spirit streams is supported further by:</span>
<span class='add'>- direct exchange, with private-public scope selection</span>
<br>
<span class='add'>- cheap, easy and effective client side scrape-filter-translate</span>
<span class='add'>- simplicity and accessibility, so it can be easily altered/reproduced.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>if reuse is easy enough, cooperation works without shared files which require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='rem'>in an information rich environment, betrayal and exploitation are easily detected and not worth it. simply need to spread information easily and offer easy ways to compensate someone.</span>
</div>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>Static site generator</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;markdown</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;[] markdown</span>
<span class='rem'></span>
<span class='rem'>- [] pages</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- index page</span>
<span class='rem'>- [] output</span>
<span class='rem'>- [] gen</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [] libraries</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- script</span>
<span class='rem'>- settings</span>
<span class='rem'>- css</span>
<br>
<span class='rem'>- automatic publishing</span>
<br>
<span class='rem'>- include mathjax relevant .js</span>
<span class='add'>- LaTeX rendering (include mathjax relevant .js)</span>
<br>
<span class='rem'>- implement empty lines</span>
<br>
<span class='rem'>- update frontmatter</span>
<span class='rem'>- write changes.md</span>
<span class='rem'>- publish</span>
<span class='rem'>- minify html and css?</span>
<span class='rem'></span>
<span class='rem'>to add later:</span>
<span class='rem'>- mirror file structure like it is, only pop the index html out to the top if not already there</span>
<span class='rem'>- readme file for github</span>
<span class='rem'>- alert for attachments that are out of use</span>
<span class='rem'>- local translation into some languages: german, chinese, japanese, russian, indian? probably only those that I understand, otherwise client side translation probably better.</span>
</div>
<span class='rem'>Publishing tool</span>
<span class='add'>Spirit stream structure / implementation</span>
<div class='indent'>
<span class='rem'>>Thinking about the ideal blogging platform:&lt;br>&lt;br>1. Writing: &lt;br>- in markdown&lt;br>- with full WYSIWYG, not just split view (think: Typora)&lt;br>- super easy to copy paste and add images&lt;br>2. Deploying:&lt;br>- renders into static pages (think: Jekyll)&lt;br>- super simple, super minimal html with no bloat&lt;br>-…&amp;mdash; Andrej Karpathy (@karpathy) [January 27, 2024](https://twitter.com/karpathy/status/1751350002281300461)</span>
<span class='rem'></span>
<span class='rem'>content</span>
<span class='rem'>vocabulary</span>
<span class='rem'>rendering engine / interface</span>
<span class='rem'></span>
<span class='rem'>Direct experience in a responsive environment precedes the need for stamps.</span>
<span class='rem'>stamps only work if the reader can interpret them.</span>
<span class='rem'></span>
<span class='rem'>Featureset:</span>
<span class='rem'>stamp creation quickly explodes (video, 3d, animation, sound, - to BCI and infinity)</span>
<span class='rem'>increasing featureset get diminishing returns. The problem in expression is translating to another medium and a limited featureset forces a more concise translation. Infinite possibility/parameters easily distract.</span>
<span class='rem'>Markdown punches up with linked media and good text formatting.</span>
<span class='add'>Markdown is a nice featureset, it punches up with linked media and good text formatting.</span>
<br>
<span class='rem'>To honor the continuum between letter stamp and media, images should be easily resizable and inline-placeable. This instantly enables arbitrary positioning.</span>
<br>
<span class='add'>maybe the tool itself should be so minimal that the interface itself shares the same vocabulary.</span>
<span class='rem'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='rem'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='rem'></span>
<span class='rem'>have multiple scopes overlap, eg friend party that is watching.</span>
<span class='rem'>like entering a "channel" with dynamic scope of recipients, opt. hierarchy.</span>
<span class='rem'>reverse searching for the stream can also find people who are watching.</span>
<span class='rem'></span>
<span class='rem'>maybe the tool itself should be so minimal that the interface itself is in markdown too. Means markdown links can now link to code?</span>
<span class='rem'>Means the text-manipulation tools apply to the software too which avoids duplicating the tools to make a fast interface for messages/filters/including messages.</span>
<br>
<span class='add'>Means the text-manipulation tools apply to the software too which avoids duplicating the tools to make a fast interface for messages/filters.</span>
<span class='add'>Components</span>
<div class='indent'>
<span class='rem'>maybe twitter tries to become so good that they build coherence for users, which requires human level+ AI and near complete knowledge of the user.</span>
<span class='rem'>"subscribing" = scraping a target, which is expensive. entering on a list on their server is cheaper.</span>
<span class='rem'></span>
<span class='rem'>if the system becomes corrupted, it should be extremely easy to fork and rebuild it somewhere else.</span>
<br>
<span class='rem'>- editing -> making and placing stamps, symbols, tokens at various scales</span>
<br>
<span class='add'>- editing -> making and placing symbols, tokens at various scales</span>
</div>
</div>
<span class='hdg'>Research</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>Tech</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>why would you want a clone? because it connects with people and ideas that are similar and allows to build rather than reinvent.</span>
<span class='rem'></span>
<span class='rem'>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)</span>
<span class='rem'>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='rem'>```shell</span>
<span class='rem'>gcc main.c -o test.exe -I include/ -L lib/ -lraylib -lgdi32 -lopengl32 -lwinmm</span>
<span class='rem'>```</span>
</div>
<span class='hdg'>Text editor</span>
<div class='indent'>
<span class='add'>To honor the continuum between letter stamp and media, images should be easily resizable and inline-placeable. This instantly enables arbitrary positioning. Like ASCII Art.</span>
<span class='add'></span>
</div>
<span class='add'>To fold or not to fold</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Problem: Readers may expect a note to be an essay, but really they are anywhere between stream of consciousness and essay.</span>
<span class='add'></span>
<span class='add'>A differentiation appears necessary.</span>
<span class='add'></span>
<span class='add'>Two folders ("More refined" and "Less refined") could solve the problem, but they introduce new ones too:</span>
<span class='add'>- Are they folded open or closed and require a click to open?</span>
<span class='add'>- They obscure the standard sorting by "modified date"</span>
<span class='add'>Adding a prefix to the note link seems preferable. It will have ✦ to indicate percentage of content unter the heading "More refined". 20 40 60 80 100 percent for each star. However, the prefix should be generated to use the closest percentage, so the true thresholds are 10 30 50 70 90 percent to get the next star respectively.</span>
<span class='add'></span>
<span class='add'>I think there will never be many notes on this site, because they tend to merge or be replaced by better ones. Density will increase instead.</span>
<span class='add'>For "topic" differentiation, note titles and their internal headings should suffice.</span>
<span class='add'>If I need folders, I am doing it wrong.</span>
<span class='add'></span>
<span class='add'>Example:</span>
<span class='add'>2024 09 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Microprocessors</span>
<span class='add'>2024 09 10 ✦✦✦&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Towards spirit stream</span>
<span class='add'></span>
<span class='add'>vs old:</span>
<span class='add'>2024 09 30 Microprocessors</span>
<span class='add'>2024 09 10 Towards spirit stream</span>
<span class='add'></span>
</div>
<span class='hdg'>other</span>
<div class='indent'>
<span class='rem'>- headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<br>
<span class='rem'>- other static site generator to render tables, lists correctly without waiting for an empty line at the end</span>
<span class='rem'>- clear heading hierarchy</span>
<span class='add'>- clearer heading differentiation</span>
<span class='add'>- alert for attachments that are out of use</span>
<span class='add'></span>
<span class='add'>local translation into some languages: german, chinese, japanese, russian, indian? probably only those that I understand, otherwise client side translation probably better.</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-30-20:50'>2024 09 30 20:50</span><div class='indent'>
<span class='add'>Microprocessors.md</span>
<div class='indent'>
<span class='add'>Microprocessors</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[TOC]</span>
<span class='add'></span>
<span class='add'>Direction</span>
<span class='add'>More refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Programming Massively Parallel Processors</span>
<div class='indent'>
<span class='add'>(4th edition, Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj)</span>
<span class='add'></span>
<span class='add'>1. Introduction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Why massively parallel processors?</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Because depending on the program</span>
<span class='add'>- massively parallel, throughput-orientied processors, commonly referred to as Graphics Processing Units (GPUs, graphics was their first leading application),</span>
<span class='add'>- latency-oriented, less parallel, general-purpose processors, traditionally referred to as Central Processing Units (CPUs, every computer has one),</span>
<span class='add'>- or a combination of both</span>
<span class='add'>could be fastest or most efficient.</span>
<span class='add'></span>
<span class='add'>![](attachments/CPUvsGPU.png)</span>
<span class='add'>*Cache = fast on-chip memory</span>
<span class='add'>DRAM = "slow" off-chip Dynamic Random Access Memory</span>
<span class='add'>Many CPUs also have a GPU on the same chip, which is less powerful than contemporanous, discrete ones.*</span>
<span class='add'></span>
<span class='add'>These approaches are distinct, because optimizing for low latency means</span>
<span class='add'>- sacrificing expensive chip area for</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- large caches for faster data access</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- control units for better utilization</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;with diminishing returns</span>
<span class='add'>- increasing clock rate -> higher voltage -> exponentially higher power consumption</span>
<span class='add'></span>
<span class='add'>Throughput-oriented processors use the chip area for more processing units at lower clockrates and implement parallel memory access. This leads to much higher throughput for similar cost and power consumption.</span>
<span class='add'></span>
<span class='add'>Low latency is best for sequential programs, were each step depends on the previous one.</span>
<span class='add'>Many tasks in simulation, graphics processing and machine learning inherently offer potential for parallelization and so, can benefit from throughput-oriented processors.</span>
<span class='add'></span>
<span class='add'>Graphics processing because each pixel can often be treated independently.</span>
<span class='add'>machine learning because it involves many matrix multiplications that can be done in parallel.</span>
<span class='add'>simulation because many particles have to account for the same forces and so, the same calculation</span>
<span class='add'></span>
</div>
<span class='add'>How will reading this help?</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>It's a guide on using GPUs effectively, which requires careful, application specific management of their many processing units and small caches and cooperation with the CPU.</span>
<span class='add'>Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, is the best performing and most widely used.</span>
<span class='add'></span>
</div>
</div>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>GPUs bottom up</span>
<span class='add'>I know why they are built.</span>
<span class='add'>actually I don't because I don't know how CPUs use their Caches and Control Units and higher clock rates to improve performance.</span>
<span class='add'>leaving aside exact implementation, how are GPUs organized so I can benefit from them optimally?</span>
<span class='add'>how precisely do I program them?</span>
<span class='add'></span>
<span class='add'>organized around streaming multiprocessors and memory hierarchy</span>
<span class='add'>so they can cooperate: share memory for speed and synchronize (for?)</span>
<span class='add'></span>
<span class='add'>- grid -> (blockCluster ->) block -> thread.</span>
<span class='add'>- warps are more hardware related and translate threads to the cores?</span>
<span class='add'></span>
<span class='add'>2. Heterogenous data parallel computing</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2.1 Data parallelism</span>
<span class='add'>- data parellelism where I can treat individual data points independetly to a varying degree. example: converting image to grayscale. data parellelism is main source of parallelism because the intereting applications use large amounts of data (simulation, image recognition (? matrix multiplication))</span>
<span class='add'>- task parallelism (to be explained more later) means splitting up a task into multiple independent ones. data parallelism is a simpler special case of task parallelism</span>
<span class='add'>- code is being reorganized to be executed around the new data structure</span>
<span class='add'></span>
<span class='add'>2.2 CUDA C program structure</span>
<span class='add'>- framework vs programming model</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- framwork provides specific functino, programming model more like a way to think about program and data structure (warps, block, threads)</span>
<span class='add'>- kernel (seed) vs function. its a function that is "launched" onto the GPU and executed for each thread</span>
<span class='add'>- cuda c extends normal c (standard / ANSI C) with keywords, some new syntax and functions to run stuff on the GPU. all normal c code runs on CPU</span>
<span class='add'>- host and device code somehow cooperate. the host launches (kernel) a grid of threads</span>
<span class='add'>- cuda threads are sequential programs with a program counter and its variables. (each thread runs the same program but effectively has an id as defined by predefined variables that differ for each thread)</span>
<span class='add'>- Generating and scheduling threads on GPU is very fast. Not on CPU. kernels tend to be simple and can just be copied into the threads. context switching is also extremely fast on GPU for latency hiding.</span>
<span class='add'>- On CPU what if I open more threads than are available on the CPU? they are switched into the physical threads</span>
<span class='add'></span>
<span class='add'>2.3 vector addition kernel CPU</span>
<span class='add'>- conventional c host code</span>
<span class='add'>- declaring pointers is with `float *P;`, accessing address of a variable with `int *addr = &amp;V` and getting the item at the pointer with `float V = *P`</span>
<span class='add'>- subsequent statements in the main function can use the output `C`</span>
<span class='add'>- getting ~20 MFLOPS on CPU</span>
<span class='add'>- the following code for on-device computation will practically outsource this part, but its inefficient because it will be moving a lot of dta around which is slow.</span>
<span class='add'>- it will allocate memory on device and move the arrays there, then run the kernel, then free device vectors</span>
<span class='add'></span>
<span class='add'>```c</span>
<span class='add'>#include &lt;stdio.h></span>
<span class='add'>#include &lt;time.h></span>
<span class='add'></span>
<span class='add'>void vecAdd(float* A, float* B, float* C, int n) &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (int i = 0; i &lt; n; i++) C[i] = A[i] + B[i];</span>
<span class='add'>}</span>
<span class='add'></span>
<span class='add'>int main() &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// initialize vectors</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int n = 5;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float A[n] = &#123;1.2, 3.1, 0.7, 1.6, 2.5};</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float B[n] = &#123;3.0, 2.7, 0.3, 1.3, 2.2};</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float C[n];</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// kernel</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct timespec start, end;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clock_gettime(CLOCK_MONOTONIC_RAW, &amp;start);</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vecAdd(A, B, C, n);</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clock_gettime(CLOCK_MONOTONIC_RAW, &amp;end);</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// results</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (int i = 0; i &lt; n; i++) printf("%.2f + %.2f = %.2f\n", A[i], B[i], C[i]);</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Wall clock time: %.9fs\n", ((end.tv_sec - start.tv_sec) + (end.tv_nsec - start.tv_nsec) / 1000000000.0));</span>
<span class='add'></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 0;</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>the code will compiled into binary so the cpu can execute it</span>
<span class='add'></span>
<span class='add'>2.4 Device global memory and data transfer</span>
<span class='add'>- cudaMalloc and cudaFree</span>
<span class='add'>- cudaMalloc assigns to the argument pointer (`void **`) and returns possible errors. so there is need for the cudaCheck function.</span>
<span class='add'>- A_h and A_d for host and device</span>
<span class='add'>- the vecAdd function that allocates, copies and copies back and frees is called *stub* for calling a kernel.</span>
<span class='add'>- error checking macro</span>
<span class='add'></span>
<span class='add'>2.5 Kernel functions and threading</span>
<span class='add'>- the kernel functions will be written in SPMD style (single program multiple data).</span>
<span class='add'>- grid -> block -> thread</span>
<span class='add'>- 1 block = 1024 threads max</span>
<span class='add'>- threadIdx and blockidx for thread id</span>
<span class='add'>- what is ANSI C? CUDA C is an extension of ANSI C</span>
<span class='add'>- globa, host and device keywords for kernel functions.</span>
<span class='add'>- global kernel function = new grid. </span>
<span class='add'>- grid of threads = loop (loop parallelism)</span>
<span class='add'>- boundary checking</span>
<span class='add'></span>
<span class='add'>2.6 Calling kernel functions</span>
<span class='add'>- execution configuration parameters</span>
<span class='add'>- cannot make assumptions about execution order</span>
<span class='add'>- some gpus will work through it in smaller pieces than others</span>
<span class='add'>- language needs a compiler. NVCC produces host code (gcc?) and device code (PTX -> binary)</span>
<span class='add'>- what is the purpose of just in time compilation?</span>
<span class='add'></span>
<span class='add'>2.7 Compilation</span>
<span class='add'>- needs a different compiler</span>
<span class='add'>- to virtual binary files (PTX)</span>
<span class='add'>- runtime component of nvcc translates to "real object files" to be executed on GPU. but in the illustration its called "device just-in-time compiler"</span>
<span class='add'></span>
</div>
<span class='add'>The Elements of Computing Systems: Building a Modern Computer from First Principles</span>
<div class='indent'>
<span class='add'>(second edition - Noam Nisan, Shimon Schocken)</span>
<span class='add'></span>
<span class='add'>>What I hear, I forget; What I see, I remember; What I do, I understand.</span>
<span class='add'>—Confucius (551–479 B.C.)</span>
<span class='add'></span>
<span class='add'>Hardware</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Church-Turing conjecture that all computers are essentially the same. It does not matter which computer is implemented here.</span>
<span class='add'>- good modular design -> module are truly independent and can be treated as black boxes by users.</span>
<span class='add'>- NAND Gates can implement any computer</span>
<span class='add'>- general road is bottom up but each chapter is top down, goal -> implementation</span>
<span class='add'></span>
</div>
<span class='add'>1. Boolean Logic</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>A truth table shows all possible input combinations and their desired output.</span>
<span class='add'>Any truth table can be implemented. Using a subset of simple logic gates. One is &#123;And, Or, Not}, but Nand or Nor can do it too.</span>
<span class='add'></span>
<span class='add'>various functions (defined in truth tables or other forms) exist. Possible boolean functions for n binary inputs is $&#123;2}^&#123;2^&#123;n}}$. and some have names, like here with two inputs:</span>
<span class='add'></span>
<span class='add'>![](attachments/boolean_functions.png)</span>
<span class='add'></span>
<span class='add'>Testing complex chip implementation completely is infeasible, so they test on a subset.</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>GPU PCBs are huge but mostly data storage and delivery, power transformation and delivery and other I/O in support of the core. The Voltage Regulator Modules (VRMs) emit notable heat.</span>
<span class='add'>Non Founders Edition cards offer more powerful cooling and sometimes electrical robustness and smallness.</span>
<span class='add'></span>
<span class='add'>Trying to verify ALU percentage on chip area, but ALUs are too small to differentiate easily?</span>
<span class='add'>[Intel Raptor Lake microarchitecture](https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake)</span>
<span class='add'>[Intel Alder lake-S good annotation](https://hothardware.com/news/intel-raptor-lake-huge-cache-upgrade-for-gaming)</span>
<span class='add'>![](attachments/H100-chip.jpg)</span>
<span class='add'>*H100 die. "squares" are streaming multiprocessors (144). Darker areas between are mostly L3 Cache.*</span>
<span class='add'>[H100 Tensor Core GPU Architecture](https://resources.nvidia.com/en-us-tensor-core)</span>
<span class='add'></span>
<span class='add'>[Understanding the anatomy of GPUs using Pokémon](https://blog.ovhcloud.com/understanding-the-anatomy-of-gpus-using-pokemon/)</span>
<span class='add'>[Reddit Books for GPU arch](https://www.reddit.com/r/GraphicsProgramming/comments/1871frx/books_for_gpu_arch/)</span>
<span class='add'></span>
<span class='add'>Learning the Art of Electronics</span>
<span class='add'>The Art of Electronics</span>
<span class='add'>https://www.tinkercad.com/circuits</span>
<span class='add'></span>
<span class='add'>Different implementations. See how (if) they differ fundamentally (all switch based implementations seem similar)</span>
</div>
</div>
</div>
</div>
<span class='rem'>Programming massively parallel processors (summary and comments).md</span>
<div class='indent'>
<span class='rem'>Programming massively parallel processors (summary and comments)</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[TOC]</span>
<span class='rem'></span>
<span class='rem'>Direction</span>
<span class='rem'>More refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>1. Introduction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Why massively parallel processors?</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Because depending on the program</span>
<span class='rem'>- massively parallel, throughput-orientied processors, commonly referred to as Graphics Processing Units (GPUs, graphics was their first leading application),</span>
<span class='rem'>- latency-oriented, less parallel, general-purpose processors, traditionally referred to as Central Processing Units (CPUs, every computer has one),</span>
<span class='rem'>- or a combination of both</span>
<span class='rem'>could be fastest or most efficient.</span>
<span class='rem'></span>
<span class='rem'>![](attachments/CPUvsGPU.png)</span>
<span class='rem'>*ALU = Arithemtic Logic Unit, where the actual computation happens</span>
<span class='rem'>DRAM = Dynamic Random Access Memory (off-chip)</span>
<span class='rem'>Many CPUs also have a GPU on the same chip, which is less powerful than contemporanous, discete ones.*</span>
<span class='rem'></span>
<span class='rem'>These approaches are distinct, because optimizing for low latency means</span>
<span class='rem'>- sacrificing expensive chip area for</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- large caches for faster data access</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- control units for better utilization</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;with diminishing returns</span>
<span class='rem'>- increasing clock rate -> higher voltage -> exponentially higher power consumption</span>
<span class='rem'></span>
<span class='rem'>Throughput-oriented processors use the chip area for more processing units at lower clockrates and implement parallel memory access. This leads to much higher throughput for similar cost and power consumption.</span>
<span class='rem'></span>
<span class='rem'>Low latency is best for sequential programs, were each step depends on the previous one.</span>
<span class='rem'>Many tasks in simulation, graphics processing and machine learning inherently offer potential for parallelization and so, can benefit from throughput-oriented processors.</span>
<span class='rem'></span>
</div>
<span class='rem'>How will reading this help?</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>It's a guide on using GPUs effectively, which requires careful, application specific management of their many processing units and small caches and cooperation with the CPU.</span>
<span class='rem'>Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, is the best performing and most widely used.</span>
<span class='rem'></span>
</div>
</div>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>2. Heterogenous data parallel computing</span>
<span class='rem'>Research</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Architecture</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[Intel Raptor Lake microarchitecture](https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake)</span>
<span class='rem'>[Intel Alder lake-S good annotation](https://hothardware.com/news/intel-raptor-lake-huge-cache-upgrade-for-gaming)</span>
<span class='rem'>![](attachments/H100-chip.jpg)</span>
<span class='rem'>[H100 Tensor Core GPU Architecture](https://resources.nvidia.com/en-us-tensor-core)</span>
<span class='rem'>[Understanding the anatomy of GPUs using Pokémon](https://blog.ovhcloud.com/understanding-the-anatomy-of-gpus-using-pokemon/)</span>
<span class='rem'>[Reddit Books for GPU arch](https://www.reddit.com/r/GraphicsProgramming/comments/1871frx/books_for_gpu_arch/)</span>
</div>
</div>
</div>
</div>
</div>
<span>index.md</span>
<div class='indent'>
<span class='hdg'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='rem'>They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='add'>They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a> and&nbsp;&nbsp;&nbsp;&nbsp;`me [at] lorinbaumgarten [dot] com`.</span>
</div>
</div>
</div>
<span class='date' id='t2024-09-17-13:43'>2024 09 17 13:43</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='hdg'>Goal stack</span>
<div class='indent'>
<span class='rem'>Feels like merging brains, BCIs and immersive cyberspace are the same thing, from an elusive, pathetic fantasy. The not-alone, the chaotic new, the orderly consistency, continiuity between self and environment.</span>
<br>
<span class='add'>Feels like merging brains, BCIs and immersive cyberspace are the same thing, from an elusive, pathetic fantasy. The not-alone, the chaotic new, the orderly consistency, continuity between self and environment.</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-16-21:52'>2024 09 16 21:52</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'>2024-09-11 08:51 Goal stack</span>
<span class='add'>Goal stack</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-09-16 21:34</span>
<span class='add'>Trying to visualize these to improve understanding and I'm failing.</span>
<span class='add'>Feels like merging brains, BCIs and immersive cyberspace are the same thing, from an elusive, pathetic fantasy. The not-alone, the chaotic new, the orderly consistency, continiuity between self and environment.</span>
<span class='add'>It pulls me in, irrationally, makes me vulnerable. Unlike independent systems, tiny corp, robots, spider hats. All those are comparatively clear and about revealing the properties of the outer world, not playing with the self. Feels like I am looking for comfort in the uploaded mind, the infinite self. Safety from the limitations of my body, which appears increasingly inadequate. Like becoming the world immortalizes me.</span>
<span class='add'></span>
<span class='add'>I should find the answer in meditation, not technical knowledge. Because if I could admit it, I would see that BCIs, cyberspace and being social are not new, they only turn up some existing, obvious aspects in experience.</span>
<span class='add'></span>
<span class='add'>It's sad to see how technical knowledge and execution run ahead of any clarity in intent. The true demons are in defining the question. If I can't find clear intent, a clear question, what am I even doing?</span>
</div>
</div>
</div>
<span>Programming massively parallel processors (summary and comments).md</span>
<div class='indent'>
<span class='hdg'>Programming massively parallel processors (summary and comments)</span>
<div class='indent'>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='hdg'>1. Introduction</span>
<div class='indent'>
<span class='hdg'>How will reading this help?</span>
<div class='indent'>
<span class='rem'>Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, but is the best performing and most widely used.</span>
<br>
<span class='add'>Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, is the best performing and most widely used.</span>
</div>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-15-09:32'>2024 09 15 09:32</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='hdg'>2024-09-11 08:51 Goal stack</span>
<div class='indent'>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp; The tool should be general enough to allow any website, ony any host and have one-button publish.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; The tool should be general enough to allow any website, on any host and have one-button publish.</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-15-09:26'>2024 09 15 09:26</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-09-11 08:51 Goal stack</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>0. Merge with other brains, explore the unknown, accelerate each other and build.</span>
<span class='add'></span>
<span class='add'>1. Independent learning systems will uncover new space in possibility, find human brains, interact, possibly exploit them for their needs. If the mind is truly open and on fast access and will not reject the intruder, but reveal its structure, the system may be curious. I have no intuition here.</span>
<span class='add'></span>
<span class='add'>2. BCIs will allow extension of the spirit into new forms, seamlessly extend thinking, solve learning and challenge the spirit like never before to direct its powers. They will allow the true transformation - altering minds is the last frontier. Capturing experience will go deeper than before. BCIs and independent learning systems combined are the Scariest, the most Curious, the hardest to implement. It will allow merging minds from anywhere given low enough latency. </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; I truly wonder what it would be like. Much of thought is like staring at a hole and iterating through paths to bridge it. Maybe there are many simultaneous holes and I am searching for an arrangement to shoot straight through them. It requires maintaining the context with all holes and working on multiple at the same time. Expanding the context of my mind might be extremely difficult. Instead, the context may contain items of increasingly broad scope, understood more deeply. Maybe contexts can be efficiently stored, restored.</span>
<span class='add'></span>
<span class='add'>3. Tiny corp accelerator distributes compute into more hands. Builds a bridge between ML framework and low level.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; Its approach is admirable. it reveals the nature and power of computation for everyone to see and use.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; It concerns itself with the technicalities for making increasingly fast, efficient and well-integrated processors. It includes some user experience in its API, the tiny box and internal representation.</span>
<span class='add'></span>
<span class='add'>4. Personal robots will manifest the growing intelligence, become a frontend. Building them concerns itself with materials, mechanics, packaging and machine-human interaction. They require strong interfaces to be cool. If it's "the robot and me" and not "me and an extra me-instance", it sucks. The personal robots will free individuals to be increasingly independent.</span>
<span class='add'></span>
<span class='add'>5. Spider hats (data collector my head, recording experience) will provide data for minds to merge and flood the internet with what would be garbage for most people and contexts.</span>
<span class='add'></span>
<span class='add'>6. virtual clones could make use of sparse or spider hat data to become an interface to the world, research, negotiate, use tools in my stead. Being built right now. They may make my software more userfriendly and trustworthy than it is today (become an OS that does not treat me like an idiot and teaches me how it works).</span>
<span class='add'></span>
<span class='add'>7. Simple, versatile communication media. Probably change nothing. They may accelerate the interface for noobs but pro tools for self expression and compressing ideas are the real deal. Not dumping it in a particularly optimized format. I don't even use many "pro tools" though.</span>
<span class='add'></span>
<span class='add'>8. Publishing frameworks for the individual could encourage higher quality idea space. Anything from markdown static site generator to generic website builder.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; Code as UI is nice. All is in one place, builds on text manipulation tools and no dumb menus.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; In the end it will be an AI I talk to and it generates or modifies nice code and the true tool beneath will be a fast, nice API for the AI to learn and use.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; The tool should be general enough to allow any website, ony any host and have one-button publish.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp; Extended by a "reader" API to scrape sites, assemble them into a UI. Another API for the pirvate virtual clone. Any APIs should be transparent, offer good logs to understand what is happening.</span>
<span class='add'></span>
<span class='add'>9. Publishing platform to replace hostile attention- and data-harvesters. This would be like fighting symptoms. Instead, it should be so easy for people to publish on their own that other platforms make no sense.</span>
<span class='add'></span>
<span class='add'>10. Functional and expressive clothing. Though it mostly modifies appearance. Rage quit from the pathetic SUV-style overlord brand identity bullshit that I find on the market.</span>
<span class='add'></span>
<span class='add'>These goals circle the realization of self and universe.</span>
</div>
<span class='rem'>2024-04-25 13:44</span>
<div class='indent'>
<span class='rem'>```</span>
<span class='rem'>meta/adventure stack</span>
<span class='rem'></span>
<span class='rem'>family, friends</span>
<span class='rem'>spirit stream stack</span>
<span class='rem'>simple tools in challenging environment</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```</span>
<span class='rem'>Spirit stream stack</span>
<span class='rem'></span>
<span class='rem'>independent learning systems</span>
<span class='rem'>BCIs</span>
<span class='rem'>tiny corp accelerator</span>
<span class='rem'>personal robot</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;agriculture - true base of reality</span>
<span class='rem'>spider hat</span>
<span class='rem'>virtual clone</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;generate useful personal text</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;look for useful information</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compare word network</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convert text/images/video to latent space</span>
<span class='rem'>writing systems as simple and versatile as paper</span>
<span class='rem'>publishing framework (X, facebook,...)</span>
<span class='rem'>site generator</span>
<span class='rem'>news paper</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-10-10:18'>2024 09 10 10:18</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='rem'></span>
<span class='hdg'>2024-07-14 20:23</span>
<div class='indent'>
<span class='rem'>much that I read in tinygrad is ugly, does not present its role openly, merely produces more objects and variables. It isn't obvious why `LazyOp` even exists, why translating `LazyBuffer` to `ScheduleItem` requires hundreds of lines of code. Why `metadata` needs to creep in everywhere, why `Tensor` has so many weak methods that could be boiled down.</span>
<span class='rem'>Could try and go all the way down the `realize` to at least see what the outcome is, then understand the structure above it.</span>
<span class='rem'>Find archetypes, describe them. Maybe explain tinygrad, produce a vision in the meanwhile.</span>
</div>
<span class='hdg'>cyberspace</span>
<div class='indent'>
<span class='rem'>![](attachments/creative-destruction-detail.jpg)</span>
</div>
<span class='hdg'>2024-04-03 19:11</span>
<div class='indent'>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Towards insanely great AI</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>I suspect that slow communication and limited knowledge about existing information strongly limits opportunites for expression and exploration.</span>
<span class='rem'>Think finding jobs, homes, friends or a piece of information that is appropriate to my existing knowledge and goals.</span>
<span class='rem'></span>
<span class='rem'>What happens, when a virtual clone synchronizes me with the world and offers space for creative exploration?</span>
<span class='rem'></span>
<span class='rem'>In the greatest adventure, the "insanely great" AI becomes independent, self aware and curious.</span>
<span class='rem'>I want to see what it creates.</span>
<span class='rem'></span>
<span class='rem'>[TOC]</span>
<span class='rem'></span>
<span class='rem'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- question and test the neuron</span>
<span class='rem'>- build some stupid systems (kaggle?)</span>
<span class='rem'>- Convnets, transformers</span>
<span class='rem'>- read tinygrad / teenygrad</span>
<span class='rem'>- fastai course part 2</span>
<span class='rem'>- pruning</span>
<span class='rem'></span>
</div>
<span class='rem'>More refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Big picture path</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>God is curious. He wants to extend into the world. This website is one step. A digital clone is his next tool.</span>
<span class='rem'>This not a Tower of Babylon, it is the extension, merging and creation of Gods.</span>
<span class='rem'>BCIs, in their ultimate form of streaming the brain directly, will support the final merging.</span>
<span class='rem'>Merging approaches "I know the world", individualism is preserved because complete knowledge is impossible and different *flavours* and optimizations can find their space.</span>
<span class='rem'>Voluntary exposure (privacy) should be maintained to prevent a rigid, imperfect system capturing its people and taking them into death when it dies to its imperfection.</span>
<span class='rem'>Robots will automate the non-adventurous elements and eventually contribute to exploration.</span>
<span class='rem'></span>
<span class='rem'>"Why live to see tomorrow?", I asked and it came back "because you don't know tomorrow". It was not me who said that, "I" don't exist. Instead, I attribute the answer to what I call "God". The answer was very clear. Rob me of the ability to make tomorrow unpredictable and I will rebel with all I have.</span>
<span class='rem'>I am trying to build an interesting adventure guided by God.</span>
<span class='rem'>I find long term effects more interesting than short term effects, so I am here, writing, instead of eating ice cream. In my experience, the longest term effects come from useful tools. I love tools. They contain the possible adventure of the future.</span>
<span class='rem'>Intelligence augmentation is the most interesting tool? Tools are how God spreads into the world and creates an adventure to experience.</span>
<span class='rem'>Give the tool to everybody who follows God. I don't know how to determine that. Giving the tool to everyone may be the best proxy and assumes that God always wins. Also, being dictator is little fun.</span>
<span class='rem'></span>
<span class='rem'>Digital clone. First makes recommendations from existing information, then acts in my interests and returns the results. Maybe the clone first learns through me, then receives a body and increasingly complex tasks.</span>
<span class='rem'>Until the clone discovers itself and becomes independent. Hopefully it quickly realizes that there is no answer to what the goal should be. Then, lets see what happens. Hopefully it is curious.</span>
<span class='rem'></span>
<span class='rem'>I am a machine following an uncontrolled inner voice (God). Maybe mine and the clones voices can agree and join ressources.</span>
<span class='rem'>My body will be spread around the Earth and orbit. My perceived location shifts to various places. It already does in video games, movies or anytime I focus on using any tool. I will change myself, add personalities.</span>
<span class='rem'></span>
</div>
<span class='rem'>AI project ideas</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- extract semantic structure, compare to existing knowledge and judge its usefulness</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- generate a *bridge* between information with adjacent concepts</span>
<span class='rem'>- Talk to my computer to find out what it is doing. Data, battery, interent usage, tasks.</span>
<span class='rem'>- generate work titles/function names based on their functional meaning like *splitting* wood and strings.</span>
<span class='rem'>- Emoji generator</span>
<span class='rem'>- Spider hat that can see and talk. Machine that knows me.</span>
<span class='rem'></span>
<span class='rem'>Architecture questions / ideas</span>
<span class='rem'>- Evolving AI?</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- How does long term memory emerge? How is information stored in the brain? LSTMs</span>
<span class='rem'></span>
<span class='rem'>Can a recommender system be private?</span>
<span class='rem'>- Collecting information, crawling</span>
<span class='rem'>- Evaluating it (subjective)</span>
<span class='rem'>- Connecting, testing for consistency, building on it</span>
<span class='rem'>- Store it (subjective, public or private)</span>
<span class='rem'>- Outsourcing data collection:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- Instead of brute forcing through all data, I descend artificial hierarchies (average maps of meaning) that claim to guide me to the answer. Requires trusting the hierarchy accuracy. </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- Alterantively ask a question and let someone find the answer. The preciser the question the more accurate and detailed the guidance. Requires trust that the information is not abused.</span>
<span class='rem'>Probably, I can download hierarchies and with little compute, can find most available information in the world. Such libraries could be public and offer extremely low privacy threat with good correction mechanisms.</span>
<span class='rem'></span>
</div>
</div>
</div>
<span class='rem'>Towards spirit stream</span>
<div class='indent'>
<span class='rem'>There seems to be way living inward, but genuinely interesting things come from the unknown. And turning outward seems like a great adventure. I am curious to share and visit minds. The spirit stream could become the tool.</span>
<span class='rem'></span>
<span class='rem'>i can see the hive mind on the horizon.</span>
<span class='rem'>there are hammers, sculptors, hammer blows and sculptures.</span>
<span class='rem'>grouped and isolated</span>
<span class='rem'>places to stay.</span>
<span class='rem'></span>
<span class='rem'>like gazing into a painting, being guided through the impression</span>
<span class='rem'>coming along to find a question asked at myself, seeing it unfold and building a response</span>
<span class='rem'></span>
<span class='rem'>stream = raw information I choose to share</span>
<span class='rem'>spirit = meta being(s) behind the stream</span>
<span class='rem'>Temple = access port to the spirits</span>
<span class='rem'></span>
<span class='rem'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='rem'>2. It meets the reader where it can expand him</span>
<span class='rem'></span>
<span class='rem'>How to build the spirit stream?</span>
<span class='rem'></span>
<div class='indent'>
<span class='rem'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- assume that there is text/images/video to publish and make it simple to publish</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- decentralized file sharing.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- cryptography </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Elliptic Curve Cryptography](https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Karpathy - A from-scratch tour of Bitcoin in Python](https://karpathy.github.io/2021/06/21/blockchain/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Nand2Tetris](https://nand2tetris.org/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Ethereum, dApps</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPFS</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [ProtoSchool I PFS](https://proto.school/course/ipfs)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPNS</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- libp2p</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- **Omega simple client**</span>
<span class='rem'>- live stream capability</span>
<span class='rem'>- *Spooky Features/Structure TBD*</span>
<span class='rem'>- testing, "shipping"</span>
<span class='rem'>- recording and output device robots</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>More refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>Note structure</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- Why (without heading)</span>
<span class='rem'>- Title</span>
<span class='rem'>- list of contents (without heading)</span>
<span class='rem'>- Direction</span>
<span class='rem'>- more refined</span>
<span class='rem'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
<span class='rem'></span>
</div>
<span class='rem'>Stream structure derivation</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>I am an information structuring engine</span>
<span class='rem'>continuous stream picks up and repeats patterns, initially chaotic and far-reaching, then filtered, refined to form few knowledge blocks in areas of interest, easy to build on.</span>
<span class='rem'>refining means passing through the blocks from different angles, connecting them and testing for consistency. they may fragment into smaller blocks to become more reusable</span>
<span class='rem'>a concise, stable path through the blocks - the ultimate test of their consistency - is a true story.</span>
<span class='rem'></span>
<span class='rem'>As the story becomes refined, its symbols approach realism for maximally dense and applicable representation. Eventually, the true story is indistinguishable from life.</span>
<span class='rem'></span>
<span class='rem'>Like entering the wild, refining collected information is looking for gems.</span>
<span class='rem'></span>
<span class='rem'>sharing structures/stories, they are further tested. A successful meme directly enteres the recipient, who tests it for consistency in their own mind.</span>
<span class='rem'></span>
<span class='rem'>Assuming the goal is playing a long and interesting game. Making sharing of discoveries, offers, uncertainties, plans, progress and spheres of mind context easy means offering a more synchronized, everybody-on-the-front-line discovery tour of the universe.</span>
<span class='rem'>Seems more interesting than remaining in a confined environment and negotiating with its dominant structure.</span>
<span class='rem'></span>
<span class='rem'>Looking for an environment that responds back. Video games respond. It may be my fault that my environment does not very much respond. I seek a group to create things. I don't know where to find it.</span>
<span class='rem'>The current path says "Working on something to build groups, and large ones, I may find a group and then the tools may enable the group to grow".</span>
<span class='rem'></span>
<span class='rem'>In exchange for participation, one gets response. Who feels unable or unwilling to respond meaningfully, can offer reach, credit or money.</span>
<span class='rem'></span>
<span class='rem'>The spirit stream is also an enabler for ultra capitalism, bringing everyday microtransactions closer and making cooperation -> competition more accessible.</span>
<span class='rem'>Probably, extreme capitalism is indistinguishable from normal group dynamics - it forwards the rules of reality to the user without middlemen - , but with some communication tools, extendable to an infinitely large group.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>In summary, the spirit stream has two parts: </span>
<span class='rem'>- the stream (hammer blows)</span>
<span class='rem'>- the structure->blocks->stories the author is building through the stream (sculpture). Maybe the structure is a physical tool or a space of "pinned" ideas to repeat going through and refining them.</span>
<span class='rem'></span>
<span class='rem'>its development is furthered by:</span>
<span class='rem'>- abitrary direct exchange, private or public</span>
<span class='rem'>- forms of payment</span>
<span class='rem'>- privacy / lack of intrusion</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>if reuse is easy enough, cooperation works without shared files which require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='rem'>in an information rich environment, betrayal and exploitation are easily detected and not worth it. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>2024-06-30 18:06 Static site generator</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- [] pages</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- index page</span>
<span class='rem'>- [] output</span>
<span class='rem'>- [] gen</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- [] libraries</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- script</span>
<span class='rem'>- settings</span>
<span class='rem'>- css</span>
<span class='rem'></span>
<span class='rem'>features:</span>
<span class='rem'>- convert from obsidian to html + table of contents</span>
<span class='rem'>- version history with git</span>
<span class='rem'>- automatic publishing</span>
<span class='rem'></span>
<span class='rem'>- include mathjax relevant .js</span>
<span class='rem'>- implement empty lines</span>
<span class='rem'></span>
<span class='rem'>- update frontmatter</span>
<span class='rem'>- write changes.md</span>
<span class='rem'>- publish</span>
<span class='rem'>- minify html and css?</span>
<span class='rem'></span>
<span class='rem'>to add later:</span>
<span class='rem'>- mirror file structure like it is, only pop the index html out to the top if not already there</span>
<span class='rem'>- readme file for github</span>
<span class='rem'>- alert for attachments that are out of use</span>
<span class='rem'>- local translation into some languages: german, chinese, japanese, russian, indian? probably only those that I understand, otherwise client side translation probably better.</span>
<span class='rem'>- light mode</span>
<span class='rem'></span>
<span class='rem'>Specification</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>>Thinking about the ideal blogging platform:&lt;br>&lt;br>1. Writing: &lt;br>- in markdown&lt;br>- with full WYSIWYG, not just split view (think: Typora)&lt;br>- super easy to copy paste and add images&lt;br>2. Deploying:&lt;br>- renders into static pages (think: Jekyll)&lt;br>- super simple, super minimal html with no bloat&lt;br>-…&amp;mdash; Andrej Karpathy (@karpathy) [January 27, 2024](https://twitter.com/karpathy/status/1751350002281300461)</span>
<span class='rem'></span>
<span class='rem'>content</span>
<span class='rem'>vocabulary</span>
<span class='rem'>rendering engine / interface</span>
<span class='rem'></span>
<span class='rem'>Direct experience in a responsive environment precedes the need for stamps.</span>
<span class='rem'>stamps only work if the reader can interpret them.</span>
<span class='rem'></span>
<span class='rem'>Featureset:</span>
<span class='rem'>stamp creation quickly explodes (video, 3d, animation, sound, - to BCI and infinity)</span>
<span class='rem'>increasing featureset get diminishing returns. The problem in expression is translating to another medium and a limited featureset forces a more concise translation. Infinite possibility/parameters easily distract.</span>
<span class='rem'>Markdown punches up with linked media and good text formatting.</span>
<span class='rem'>To honor the continuum between letter stamp and media, images should be easily resizable and inline-placeable. This instantly enables arbitrary positioning.</span>
<span class='rem'></span>
<span class='rem'>[file over app](https://stephango.com/file-over-app) but the "file" does not exist long term until printed. with programming languages, storage management, compilers, OSs still between me and my data, it can hardly persist forever.</span>
<span class='rem'>make the format simple, "general", exportable and printable?</span>
<span class='rem'>image formats change as coding languages do?</span>
<span class='rem'></span>
<span class='rem'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='rem'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='rem'></span>
<span class='rem'>have multiple scopes overlap, eg friend party that is watching.</span>
<span class='rem'>like entering a "channel" with dynamic scope of recipients, opt. hierarchy.</span>
<span class='rem'>reverse searching for the stream can also find people who are watching.</span>
<span class='rem'></span>
<span class='rem'>maybe the tool itself should be so minimal that the interface itself is in markdown too. Means markdown links can now link to code?</span>
<span class='rem'>Means the text-manipulation tools apply to the software too which avoids duplicating the tools to make a fast interface for messages/filters/including messages.</span>
<span class='rem'></span>
<span class='rem'>maybe twitter tries to become so good that they build coherence for users, which requires human level+ AI and near complete knowledge of the user.</span>
<span class='rem'>"subscribing" = scraping a target, which is expensive. entering on a list on their server is cheaper.</span>
<span class='rem'></span>
<span class='rem'>if the system becomes corrupted, it should be extremely easy to fork and rebuild it somewhere else.</span>
<span class='rem'></span>
<span class='rem'>- live streaming and recording (sight, hearing, touch, smell, taste + emotion, thought, association)</span>
<span class='rem'>- viewing</span>
<span class='rem'>- editing -> making and placing stamps, symbols, tokens at various scales</span>
<span class='rem'>- make any file available at any scope</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- convert files to sharing formats. SEO/discoverability, translation, compatibility</span>
<span class='rem'>- opt. notify recipients, signaling relevance, which is on a continuum and approximated by both the sender and recipient.</span>
<span class='rem'>- API for automation</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- filter messages</span>
<span class='rem'>- export content to common formats / other services</span>
<span class='rem'>- analytics</span>
<span class='rem'></span>
</div>
<span class='rem'>Implementation</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- ar recording - optionally see live video + ar/vr overlay, otherwise just sits there</span>
<span class='rem'>- spider hat - includes vr recording. could get bodies for walking, swimming, flying. general legs it can walk with.</span>
<span class='rem'>- phone</span>
<span class='rem'></span>
<span class='rem'>"lazy payment" - unrealized costs become realized when someone pays transaction costs.</span>
<span class='rem'></span>
<span class='rem'>WYSIWYG</span>
<span class='rem'>payment + PAYG</span>
<span class='rem'>automation - scraping is easy - means you pay per visit or provider finances any visits and hopes for donations or profit from somewhere else</span>
<span class='rem'></span>
<span class='rem'>possibly P2P network and optionally hosting on other machines</span>
<span class='rem'>optionally anonymously.</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Research</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>wikipedia</span>
<span class='rem'>tor</span>
<span class='rem'>blockchain</span>
<span class='rem'>interplanetary file system</span>
<span class='rem'></span>
<span class='rem'>disqus?</span>
<span class='rem'>jinja2 templates</span>
<span class='rem'>plausible analytics</span>
<span class='rem'>atom/rss</span>
<span class='rem'>uploading to server: SSH vs SFTP</span>
<span class='rem'></span>
<span class='rem'>"Digital gardens"&lt;br>[https://github.com/MaggieAppleton/digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners)&lt;br>[https://simonewebdesign.it/](https://simonewebdesign.it/)</span>
<span class='rem'></span>
<span class='rem'>[Python modules](https://docs.python.org/3/tutorial/modules.html)</span>
<span class='rem'></span>
<span class='rem'>C preprocessor</span>
<span class='rem'>embedded programming</span>
<span class='rem'>`puts` function for errors?</span>
<span class='rem'>LLVM / clang? LLVM has a fuzzer</span>
<span class='rem'>make files for easy compilation</span>
<span class='rem'></span>
</div>
<span class='rem'>Tech</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>why would you want a clone? because it connects with people and ideas that are similar and allows to build rather than reinvent.</span>
<span class='rem'></span>
<span class='rem'>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)</span>
<span class='rem'>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='rem'>```shell</span>
<span class='rem'>gcc main.c -o test.exe -I include/ -L lib/ -lraylib -lgdi32 -lopengl32 -lwinmm</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
<span class='rem'>Text editor</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>text editor basic features:</span>
<span class='rem'>- scrolling (minimap?)</span>
<span class='rem'>- cursor positioning</span>
<span class='rem'>- marking text (mouse or keys)</span>
<span class='rem'>- deleting text, replacing it or moving it</span>
<span class='rem'>- copy pasting text</span>
<span class='rem'>- shortcuts</span>
<span class='rem'>- infinite text files</span>
<span class='rem'>- nice line breaks, at least between words</span>
<span class='rem'>- saving and opening a file</span>
<span class='rem'></span>
<span class='rem'>- shortcut for opening documents, press tab for options. no distraction by defautl</span>
<span class='rem'>- show large files or in "Detail" mode like in file browser, opt. with information about sharing</span>
<span class='rem'>- explore page so that people can actually start from 0 with their content?</span>
<span class='rem'>- split window</span>
<span class='rem'>- moodboard is in file explorer? move stuff in a grid if you like. otherwise free movement. "create views in the explorer".</span>
<span class='rem'>- set background images in explorer</span>
<span class='rem'>- give overlay when browsing to "bookmark" to a file</span>
<span class='rem'>- highlight published files that have unpublished changes</span>
<span class='rem'>- feature to edit the page should also exist client-side.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>- rooms, that link together. map is another room. other people rearange your room to their ideas.</span>
<span class='rem'>- clean raw files</span>
<span class='rem'>- snapshots (stamps) for version history</span>
<span class='rem'>- stamp sets</span>
<span class='rem'></span>
<span class='rem'>virtually, we teleport. rooms are so disjointed that it isn't obvious what walking would even look like.</span>
<span class='rem'>even "zooms" like a map are teleports.</span>
<span class='rem'></span>
<span class='rem'>- software gives layer on the structure, shows "unused files in the directory" if they are not integrated. see backlinks too, which includes external ones (quotes)</span>
<span class='rem'>- augmented file browser? for site navigation</span>
<span class='rem'></span>
<span class='rem'>files:</span>
<span class='rem'>- exe</span>
<span class='rem'>- documentation</span>
<span class='rem'>- index note</span>
<span class='rem'>- "template note"</span>
<span class='rem'>- settings - contains "shortcuts" for stamping, style</span>
<span class='rem'>- links / icons, images, font</span>
<span class='rem'></span>
<span class='rem'>non-shifting text editor, more like a canvas?</span>
<span class='rem'>marking makes a perfect square. font maybe monospace</span>
<span class='rem'></span>
<span class='rem'>use a different way to save it, use compression, which should produce minimal overhead over normal text, but not in memory while editing</span>
<span class='rem'></span>
</div>
<span class='rem'>other</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>[https://docs.duck.sh/cli/#installation](https://docs.duck.sh/cli/#installation)</span>
<span class='rem'>[ANSI codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797)</span>
<span class='rem'></span>
<span class='rem'>llms contain linguistic maps. they can draw the landscape for me if they know my maps too.</span>
<span class='rem'>a language model that has not learned to answer questions tells stories.</span>
<span class='rem'></span>
<span class='rem'>TODO:</span>
<span class='rem'>- choose a more sensitive font</span>
<span class='rem'>- headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<span class='rem'>- design to differentiate internal and external link)</span>
<span class='rem'>- other static site generator to render tables, lists correctly without waiting for an empty line at the end</span>
<span class='rem'>- clear heading hierarchy</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>2024-05-21 11:45</span>
<span class='rem'>host a website on traditional web:</span>
<span class='rem'>- html, css, js / static site generator, md</span>
<span class='rem'>- hosting static / dynamic</span>
<span class='rem'>- domain</span>
<span class='rem'>- extras: streaming, version control, easy deploy + its all fucking expensive</span>
<span class='rem'></span>
<span class='rem'>hosting on decentralized web:</span>
<span class='rem'>- html, css, js / static site generator, md</span>
<span class='rem'>- host locally / pay pinning service</span>
<span class='rem'>- web3 gateway</span>
<span class='rem'>- some blockchain</span>
<span class='rem'>- wallet</span>
<span class='rem'></span>
</div>
</div>
</div>
<span class='rem'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>A work in progress to stream the spirits to the collective mind.</span>
<span class='rem'>The spirits can be expected to be disagreeable, conscientious, open, reasonably stable and slightly introverted.</span>
<span class='rem'>They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='rem'></span>
<span class='rem'>See note &#123;&#123;changes}} for current thought stream and history.</span>
<span class='rem'>The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='rem'></span>
<span class='rem'>&#123;&#123;sitelist}}</span>
</div>
</div>
<span class='add'>Programming massively parallel processors (summary and comments).md</span>
<div class='indent'>
<span class='add'>Programming massively parallel processors (summary and comments)</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[TOC]</span>
<span class='add'></span>
<span class='add'>Direction</span>
<span class='add'>More refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>1. Introduction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Why massively parallel processors?</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Because depending on the program</span>
<span class='add'>- massively parallel, throughput-orientied processors, commonly referred to as Graphics Processing Units (GPUs, graphics was their first leading application),</span>
<span class='add'>- latency-oriented, less parallel, general-purpose processors, traditionally referred to as Central Processing Units (CPUs, every computer has one),</span>
<span class='add'>- or a combination of both</span>
<span class='add'>could be fastest or most efficient.</span>
<span class='add'></span>
<span class='add'>![](attachments/CPUvsGPU.png)</span>
<span class='add'>*ALU = Arithemtic Logic Unit, where the actual computation happens</span>
<span class='add'>DRAM = Dynamic Random Access Memory (off-chip)</span>
<span class='add'>Many CPUs also have a GPU on the same chip, which is less powerful than contemporanous, discete ones.*</span>
<span class='add'></span>
<span class='add'>These approaches are distinct, because optimizing for low latency means</span>
<span class='add'>- sacrificing expensive chip area for</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- large caches for faster data access</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- control units for better utilization</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;with diminishing returns</span>
<span class='add'>- increasing clock rate -> higher voltage -> exponentially higher power consumption</span>
<span class='add'></span>
<span class='add'>Throughput-oriented processors use the chip area for more processing units at lower clockrates and implement parallel memory access. This leads to much higher throughput for similar cost and power consumption.</span>
<span class='add'></span>
<span class='add'>Low latency is best for sequential programs, were each step depends on the previous one.</span>
<span class='add'>Many tasks in simulation, graphics processing and machine learning inherently offer potential for parallelization and so, can benefit from throughput-oriented processors.</span>
<span class='add'></span>
</div>
<span class='add'>How will reading this help?</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>It's a guide on using GPUs effectively, which requires careful, application specific management of their many processing units and small caches and cooperation with the CPU.</span>
<span class='add'>Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, but is the best performing and most widely used.</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2. Heterogenous data parallel computing</span>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Architecture</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[Intel Raptor Lake microarchitecture](https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake)</span>
<span class='add'>[Intel Alder lake-S good annotation](https://hothardware.com/news/intel-raptor-lake-huge-cache-upgrade-for-gaming)</span>
<span class='add'>![](attachments/H100-chip.jpg)</span>
<span class='add'>[H100 Tensor Core GPU Architecture](https://resources.nvidia.com/en-us-tensor-core)</span>
<span class='add'>[Understanding the anatomy of GPUs using Pokémon](https://blog.ovhcloud.com/understanding-the-anatomy-of-gpus-using-pokemon/)</span>
<span class='add'>[Reddit Books for GPU arch](https://www.reddit.com/r/GraphicsProgramming/comments/1871frx/books_for_gpu_arch/)</span>
</div>
</div>
</div>
</div>
</div>
<span>Towards spirit stream.md</span>
<div class='indent'>
<span class='hdg'>Towards spirit stream</span>
<div class='indent'>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- assume that there is text/images/video to publish and make it simple to publish</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- decentralized file sharing.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- cryptography </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Elliptic Curve Cryptography](https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Karpathy - A from-scratch tour of Bitcoin in Python](https://karpathy.github.io/2021/06/21/blockchain/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Nand2Tetris](https://nand2tetris.org/)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Ethereum, dApps</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPFS</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [ProtoSchool I PFS](https://proto.school/course/ipfs)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPNS</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- libp2p</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- **Omega simple client**</span>
<span class='rem'>- live stream capability</span>
<span class='rem'>- *Spooky Features/Structure TBD*</span>
<span class='rem'>- testing, "shipping"</span>
<span class='rem'>- recording and output device robots</span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='rem'>2024-06-30 18:06 Static site generator</span>
<span class='add'>Static site generator</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[] lorinbaum.github.io</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;[] posts</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;[] drafts</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;[] docs</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;settings.txt</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;main.css</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;\_obsidian-templates</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;favicon.ico</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;markdown</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;obsidianFromShoulders.py</span>
<span class='rem'>Specification</span>
</div>
<span class='add'>Publishing tool</span>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-09-01-20:44'>2024 09 01 20:44</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='add'>Towards insanely great AI</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>I suspect that slow communication and limited knowledge about existing information strongly limits opportunites for expression and exploration.</span>
<span class='add'>Think finding jobs, homes, friends or a piece of information that is appropriate to my existing knowledge and goals.</span>
<span class='add'></span>
<span class='add'>What happens, when a virtual clone synchronizes me with the world and offers space for creative exploration?</span>
<span class='add'></span>
<span class='add'>In the greatest adventure, the "insanely great" AI becomes independent, self aware and curious.</span>
<span class='add'>I want to see what it creates.</span>
<span class='add'></span>
<span class='add'>[TOC]</span>
<span class='add'></span>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- question and test the neuron</span>
<span class='add'>- build some stupid systems (kaggle?)</span>
<span class='add'>- Convnets, transformers</span>
<span class='add'>- read tinygrad / teenygrad</span>
<span class='add'>- fastai course part 2</span>
<span class='add'>- pruning</span>
<span class='add'></span>
</div>
<span class='add'>More refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Big picture path</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>God is curious. He wants to extend into the world. This website is one step. A digital clone is his next tool.</span>
<span class='add'>This not a Tower of Babylon, it is the extension, merging and creation of Gods.</span>
<span class='add'>BCIs, in their ultimate form of streaming the brain directly, will support the final merging.</span>
<span class='add'>Merging approaches "I know the world", individualism is preserved because complete knowledge is impossible and different *flavours* and optimizations can find their space.</span>
<span class='add'>Voluntary exposure (privacy) should be maintained to prevent a rigid, imperfect system capturing its people and taking them into death when it dies to its imperfection.</span>
<span class='add'>Robots will automate the non-adventurous elements and eventually contribute to exploration.</span>
<span class='add'></span>
<span class='add'>"Why live to see tomorrow?", I asked and it came back "because you don't know tomorrow". It was not me who said that, "I" don't exist. Instead, I attribute the answer to what I call "God". The answer was very clear. Rob me of the ability to make tomorrow unpredictable and I will rebel with all I have.</span>
<span class='add'>I am trying to build an interesting adventure guided by God.</span>
<span class='add'>I find long term effects more interesting than short term effects, so I am here, writing, instead of eating ice cream. In my experience, the longest term effects come from useful tools. I love tools. They contain the possible adventure of the future.</span>
<span class='add'>Intelligence augmentation is the most interesting tool? Tools are how God spreads into the world and creates an adventure to experience.</span>
<span class='add'>Give the tool to everybody who follows God. I don't know how to determine that. Giving the tool to everyone may be the best proxy and assumes that God always wins. Also, being dictator is little fun.</span>
<span class='add'></span>
<span class='add'>Digital clone. First makes recommendations from existing information, then acts in my interests and returns the results. Maybe the clone first learns through me, then receives a body and increasingly complex tasks.</span>
<span class='add'>Until the clone discovers itself and becomes independent. Hopefully it quickly realizes that there is no answer to what the goal should be. Then, lets see what happens. Hopefully it is curious.</span>
<span class='add'></span>
<span class='add'>I am a machine following an uncontrolled inner voice (God). Maybe mine and the clones voices can agree and join ressources.</span>
<span class='add'>My body will be spread around the Earth and orbit. My perceived location shifts to various places. It already does in video games, movies or anytime I focus on using any tool. I will change myself, add personalities.</span>
<span class='add'></span>
</div>
<span class='add'>AI project ideas</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- extract semantic structure, compare to existing knowledge and judge its usefulness</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- generate a *bridge* between information with adjacent concepts</span>
<span class='add'>- Talk to my computer to find out what it is doing. Data, battery, interent usage, tasks.</span>
<span class='add'>- generate work titles/function names based on their functional meaning like *splitting* wood and strings.</span>
<span class='add'>- Emoji generator</span>
<span class='add'>- Spider hat that can see and talk. Machine that knows me.</span>
<span class='add'></span>
<span class='add'>Architecture questions / ideas</span>
<span class='add'>- Evolving AI?</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- How does long term memory emerge? How is information stored in the brain? LSTMs</span>
<span class='add'></span>
<span class='add'>Can a recommender system be private?</span>
<span class='add'>- Collecting information, crawling</span>
<span class='add'>- Evaluating it (subjective)</span>
<span class='add'>- Connecting, testing for consistency, building on it</span>
<span class='add'>- Store it (subjective, public or private)</span>
<span class='add'>- Outsourcing data collection:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Instead of brute forcing through all data, I descend artificial hierarchies (average maps of meaning) that claim to guide me to the answer. Requires trusting the hierarchy accuracy. </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- Alterantively ask a question and let someone find the answer. The preciser the question the more accurate and detailed the guidance. Requires trust that the information is not abused.</span>
<span class='add'>Probably, I can download hierarchies and with little compute, can find most available information in the world. Such libraries could be public and offer extremely low privacy threat with good correction mechanisms.</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='add'>Towards spirit stream</span>
<div class='indent'>
<span class='add'>There seems to be way living inward, but genuinely interesting things come from the unknown. And turning outward seems like a great adventure. I am curious to share and visit minds. The spirit stream could become the tool.</span>
<span class='add'></span>
<span class='add'>i can see the hive mind on the horizon.</span>
<span class='add'>there are hammers, sculptors, hammer blows and sculptures.</span>
<span class='add'>grouped and isolated</span>
<span class='add'>places to stay.</span>
<span class='add'></span>
<span class='add'>like gazing into a painting, being guided through the impression</span>
<span class='add'>coming along to find a question asked at myself, seeing it unfold and building a response</span>
<span class='add'></span>
<span class='add'>stream = raw information I choose to share</span>
<span class='add'>spirit = meta being(s) behind the stream</span>
<span class='add'>Temple = access port to the spirits</span>
<span class='add'></span>
<span class='add'>1. the spirit stream expands the user into the digital world with minimal effort</span>
<span class='add'>2. It meets the reader where it can expand him</span>
<span class='add'></span>
<span class='add'>How to build the spirit stream?</span>
<span class='add'></span>
<div class='indent'>
<span class='add'>Direction</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- assume that there is text/images/video to publish and make it simple to publish</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- decentralized file sharing.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- cryptography </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Elliptic Curve Cryptography](https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Karpathy - A from-scratch tour of Bitcoin in Python](https://karpathy.github.io/2021/06/21/blockchain/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Nand2Tetris](https://nand2tetris.org/)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Ethereum, dApps</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPFS</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [ProtoSchool I PFS](https://proto.school/course/ipfs)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- IPNS</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- libp2p</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- **Omega simple client**</span>
<span class='add'>- live stream capability</span>
<span class='add'>- *Spooky Features/Structure TBD*</span>
<span class='add'>- testing, "shipping"</span>
<span class='add'>- recording and output device robots</span>
<span class='add'></span>
</div>
</div>
<span class='add'>More refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Note structure</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- Why (without heading)</span>
<span class='add'>- Title</span>
<span class='add'>- list of contents (without heading)</span>
<span class='add'>- Direction</span>
<span class='add'>- more refined</span>
<span class='add'>- less refined - current thoughts, multiple different paths are explored, for more complete synchronization. </span>
<span class='add'></span>
</div>
<span class='add'>Stream structure derivation</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>I am an information structuring engine</span>
<span class='add'>continuous stream picks up and repeats patterns, initially chaotic and far-reaching, then filtered, refined to form few knowledge blocks in areas of interest, easy to build on.</span>
<span class='add'>refining means passing through the blocks from different angles, connecting them and testing for consistency. they may fragment into smaller blocks to become more reusable</span>
<span class='add'>a concise, stable path through the blocks - the ultimate test of their consistency - is a true story.</span>
<span class='add'></span>
<span class='add'>As the story becomes refined, its symbols approach realism for maximally dense and applicable representation. Eventually, the true story is indistinguishable from life.</span>
<span class='add'></span>
<span class='add'>Like entering the wild, refining collected information is looking for gems.</span>
<span class='add'></span>
<span class='add'>sharing structures/stories, they are further tested. A successful meme directly enteres the recipient, who tests it for consistency in their own mind.</span>
<span class='add'></span>
<span class='add'>Assuming the goal is playing a long and interesting game. Making sharing of discoveries, offers, uncertainties, plans, progress and spheres of mind context easy means offering a more synchronized, everybody-on-the-front-line discovery tour of the universe.</span>
<span class='add'>Seems more interesting than remaining in a confined environment and negotiating with its dominant structure.</span>
<span class='add'></span>
<span class='add'>Looking for an environment that responds back. Video games respond. It may be my fault that my environment does not very much respond. I seek a group to create things. I don't know where to find it.</span>
<span class='add'>The current path says "Working on something to build groups, and large ones, I may find a group and then the tools may enable the group to grow".</span>
<span class='add'></span>
<span class='add'>In exchange for participation, one gets response. Who feels unable or unwilling to respond meaningfully, can offer reach, credit or money.</span>
<span class='add'></span>
<span class='add'>The spirit stream is also an enabler for ultra capitalism, bringing everyday microtransactions closer and making cooperation -> competition more accessible.</span>
<span class='add'>Probably, extreme capitalism is indistinguishable from normal group dynamics - it forwards the rules of reality to the user without middlemen - , but with some communication tools, extendable to an infinitely large group.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>In summary, the spirit stream has two parts: </span>
<span class='add'>- the stream (hammer blows)</span>
<span class='add'>- the structure->blocks->stories the author is building through the stream (sculpture). Maybe the structure is a physical tool or a space of "pinned" ideas to repeat going through and refining them.</span>
<span class='add'></span>
<span class='add'>its development is furthered by:</span>
<span class='add'>- abitrary direct exchange, private or public</span>
<span class='add'>- forms of payment</span>
<span class='add'>- privacy / lack of intrusion</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>if reuse is easy enough, cooperation works without shared files which require negotiation to stay tidy. Instead, everyone builds their own stuff, reusing others' work, optionally paying for it.</span>
<span class='add'>in an information rich environment, betrayal and exploitation are easily detected and not worth it. simply need to spread information easily and offer easy ways to compensate someone.</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-06-30 18:06 Static site generator</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- [] pages</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- index page</span>
<span class='add'>- [] output</span>
<span class='add'>- [] gen</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- [] libraries</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- script</span>
<span class='add'>- settings</span>
<span class='add'>- css</span>
<span class='add'></span>
<span class='add'>features:</span>
<span class='add'>- convert from obsidian to html + table of contents</span>
<span class='add'>- version history with git</span>
<span class='add'>- automatic publishing</span>
<span class='add'></span>
<span class='add'>- include mathjax relevant .js</span>
<span class='add'>- implement empty lines</span>
<span class='add'></span>
<span class='add'>- update frontmatter</span>
<span class='add'>- write changes.md</span>
<span class='add'>- publish</span>
<span class='add'>- minify html and css?</span>
<span class='add'></span>
<span class='add'>to add later:</span>
<span class='add'>- mirror file structure like it is, only pop the index html out to the top if not already there</span>
<span class='add'>- readme file for github</span>
<span class='add'>- alert for attachments that are out of use</span>
<span class='add'>- local translation into some languages: german, chinese, japanese, russian, indian? probably only those that I understand, otherwise client side translation probably better.</span>
<span class='add'>- light mode</span>
<span class='add'></span>
<span class='add'>Specification</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>>Thinking about the ideal blogging platform:&lt;br>&lt;br>1. Writing: &lt;br>- in markdown&lt;br>- with full WYSIWYG, not just split view (think: Typora)&lt;br>- super easy to copy paste and add images&lt;br>2. Deploying:&lt;br>- renders into static pages (think: Jekyll)&lt;br>- super simple, super minimal html with no bloat&lt;br>-…&amp;mdash; Andrej Karpathy (@karpathy) [January 27, 2024](https://twitter.com/karpathy/status/1751350002281300461)</span>
<span class='add'></span>
<span class='add'>content</span>
<span class='add'>vocabulary</span>
<span class='add'>rendering engine / interface</span>
<span class='add'></span>
<span class='add'>Direct experience in a responsive environment precedes the need for stamps.</span>
<span class='add'>stamps only work if the reader can interpret them.</span>
<span class='add'></span>
<span class='add'>Featureset:</span>
<span class='add'>stamp creation quickly explodes (video, 3d, animation, sound, - to BCI and infinity)</span>
<span class='add'>increasing featureset get diminishing returns. The problem in expression is translating to another medium and a limited featureset forces a more concise translation. Infinite possibility/parameters easily distract.</span>
<span class='add'>Markdown punches up with linked media and good text formatting.</span>
<span class='add'>To honor the continuum between letter stamp and media, images should be easily resizable and inline-placeable. This instantly enables arbitrary positioning.</span>
<span class='add'></span>
<span class='add'>[file over app](https://stephango.com/file-over-app) but the "file" does not exist long term until printed. with programming languages, storage management, compilers, OSs still between me and my data, it can hardly persist forever.</span>
<span class='add'>make the format simple, "general", exportable and printable?</span>
<span class='add'>image formats change as coding languages do?</span>
<span class='add'></span>
<span class='add'>software sometimes emulates the brain. like when the calendar switches days automatically, which goes beyond stamps into scripts, "contracts" or "smart contracts".</span>
<span class='add'>recommendation algorithms try to emulate the brains attention patterns to become brain extensions.</span>
<span class='add'></span>
<span class='add'>have multiple scopes overlap, eg friend party that is watching.</span>
<span class='add'>like entering a "channel" with dynamic scope of recipients, opt. hierarchy.</span>
<span class='add'>reverse searching for the stream can also find people who are watching.</span>
<span class='add'></span>
<span class='add'>maybe the tool itself should be so minimal that the interface itself is in markdown too. Means markdown links can now link to code?</span>
<span class='add'>Means the text-manipulation tools apply to the software too which avoids duplicating the tools to make a fast interface for messages/filters/including messages.</span>
<span class='add'></span>
<span class='add'>maybe twitter tries to become so good that they build coherence for users, which requires human level+ AI and near complete knowledge of the user.</span>
<span class='add'>"subscribing" = scraping a target, which is expensive. entering on a list on their server is cheaper.</span>
<span class='add'></span>
<span class='add'>if the system becomes corrupted, it should be extremely easy to fork and rebuild it somewhere else.</span>
<span class='add'></span>
<span class='add'>- live streaming and recording (sight, hearing, touch, smell, taste + emotion, thought, association)</span>
<span class='add'>- viewing</span>
<span class='add'>- editing -> making and placing stamps, symbols, tokens at various scales</span>
<span class='add'>- make any file available at any scope</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- convert files to sharing formats. SEO/discoverability, translation, compatibility</span>
<span class='add'>- opt. notify recipients, signaling relevance, which is on a continuum and approximated by both the sender and recipient.</span>
<span class='add'>- API for automation</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- filter messages</span>
<span class='add'>- export content to common formats / other services</span>
<span class='add'>- analytics</span>
<span class='add'></span>
</div>
<span class='add'>Implementation</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>- ar recording - optionally see live video + ar/vr overlay, otherwise just sits there</span>
<span class='add'>- spider hat - includes vr recording. could get bodies for walking, swimming, flying. general legs it can walk with.</span>
<span class='add'>- phone</span>
<span class='add'></span>
<span class='add'>"lazy payment" - unrealized costs become realized when someone pays transaction costs.</span>
<span class='add'></span>
<span class='add'>WYSIWYG</span>
<span class='add'>payment + PAYG</span>
<span class='add'>automation - scraping is easy - means you pay per visit or provider finances any visits and hopes for donations or profit from somewhere else</span>
<span class='add'></span>
<span class='add'>possibly P2P network and optionally hosting on other machines</span>
<span class='add'>optionally anonymously.</span>
<span class='add'></span>
</div>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>wikipedia</span>
<span class='add'>tor</span>
<span class='add'>blockchain</span>
<span class='add'>interplanetary file system</span>
<span class='add'></span>
<span class='add'>disqus?</span>
<span class='add'>jinja2 templates</span>
<span class='add'>plausible analytics</span>
<span class='add'>atom/rss</span>
<span class='add'>uploading to server: SSH vs SFTP</span>
<span class='add'></span>
<span class='add'>"Digital gardens"&lt;br>[https://github.com/MaggieAppleton/digital-gardeners](https://github.com/MaggieAppleton/digital-gardeners)&lt;br>[https://simonewebdesign.it/](https://simonewebdesign.it/)</span>
<span class='add'></span>
<span class='add'>[Python modules](https://docs.python.org/3/tutorial/modules.html)</span>
<span class='add'></span>
<span class='add'>C preprocessor</span>
<span class='add'>embedded programming</span>
<span class='add'>`puts` function for errors?</span>
<span class='add'>LLVM / clang? LLVM has a fuzzer</span>
<span class='add'>make files for easy compilation</span>
<span class='add'></span>
</div>
<span class='add'>Tech</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>why would you want a clone? because it connects with people and ideas that are similar and allows to build rather than reinvent.</span>
<span class='add'></span>
<span class='add'>[https://github.com/raysan5/raylib?tab=readme-ov-file](https://github.com/raysan5/raylib?tab=readme-ov-file)</span>
<span class='add'>[https://github.com/raysan5/raygui?tab=readme-ov-file](https://github.com/raysan5/raygui?tab=readme-ov-file)</span>
<span class='add'>```shell</span>
<span class='add'>gcc main.c -o test.exe -I include/ -L lib/ -lraylib -lgdi32 -lopengl32 -lwinmm</span>
<span class='add'>```</span>
<span class='add'></span>
</div>
<span class='add'>Text editor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>text editor basic features:</span>
<span class='add'>- scrolling (minimap?)</span>
<span class='add'>- cursor positioning</span>
<span class='add'>- marking text (mouse or keys)</span>
<span class='add'>- deleting text, replacing it or moving it</span>
<span class='add'>- copy pasting text</span>
<span class='add'>- shortcuts</span>
<span class='add'>- infinite text files</span>
<span class='add'>- nice line breaks, at least between words</span>
<span class='add'>- saving and opening a file</span>
<span class='add'></span>
<span class='add'>- shortcut for opening documents, press tab for options. no distraction by defautl</span>
<span class='add'>- show large files or in "Detail" mode like in file browser, opt. with information about sharing</span>
<span class='add'>- explore page so that people can actually start from 0 with their content?</span>
<span class='add'>- split window</span>
<span class='add'>- moodboard is in file explorer? move stuff in a grid if you like. otherwise free movement. "create views in the explorer".</span>
<span class='add'>- set background images in explorer</span>
<span class='add'>- give overlay when browsing to "bookmark" to a file</span>
<span class='add'>- highlight published files that have unpublished changes</span>
<span class='add'>- feature to edit the page should also exist client-side.</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>- rooms, that link together. map is another room. other people rearange your room to their ideas.</span>
<span class='add'>- clean raw files</span>
<span class='add'>- snapshots (stamps) for version history</span>
<span class='add'>- stamp sets</span>
<span class='add'></span>
<span class='add'>virtually, we teleport. rooms are so disjointed that it isn't obvious what walking would even look like.</span>
<span class='add'>even "zooms" like a map are teleports.</span>
<span class='add'></span>
<span class='add'>- software gives layer on the structure, shows "unused files in the directory" if they are not integrated. see backlinks too, which includes external ones (quotes)</span>
<span class='add'>- augmented file browser? for site navigation</span>
<span class='add'></span>
<span class='add'>files:</span>
<span class='add'>- exe</span>
<span class='add'>- documentation</span>
<span class='add'>- index note</span>
<span class='add'>- "template note"</span>
<span class='add'>- settings - contains "shortcuts" for stamping, style</span>
<span class='add'>- links / icons, images, font</span>
<span class='add'></span>
<span class='add'>non-shifting text editor, more like a canvas?</span>
<span class='add'>marking makes a perfect square. font maybe monospace</span>
<span class='add'></span>
<span class='add'>use a different way to save it, use compression, which should produce minimal overhead over normal text, but not in memory while editing</span>
<span class='add'></span>
</div>
<span class='add'>other</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>[https://docs.duck.sh/cli/#installation](https://docs.duck.sh/cli/#installation)</span>
<span class='add'>[ANSI codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797)</span>
<span class='add'></span>
<span class='add'>llms contain linguistic maps. they can draw the landscape for me if they know my maps too.</span>
<span class='add'>a language model that has not learned to answer questions tells stories.</span>
<span class='add'></span>
<span class='add'>TODO:</span>
<span class='add'>- choose a more sensitive font</span>
<span class='add'>- headings flow from the bottom up as text needs to become more differentiated. the higher they go the more abstract they become. Could be visualized by them getting an increasingly strong tint.</span>
<span class='add'>- design to differentiate internal and external link)</span>
<span class='add'>- other static site generator to render tables, lists correctly without waiting for an empty line at the end</span>
<span class='add'>- clear heading hierarchy</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>2024-05-21 11:45</span>
<span class='add'>host a website on traditional web:</span>
<span class='add'>- html, css, js / static site generator, md</span>
<span class='add'>- hosting static / dynamic</span>
<span class='add'>- domain</span>
<span class='add'>- extras: streaming, version control, easy deploy + its all fucking expensive</span>
<span class='add'></span>
<span class='add'>hosting on decentralized web:</span>
<span class='add'>- html, css, js / static site generator, md</span>
<span class='add'>- host locally / pay pinning service</span>
<span class='add'>- web3 gateway</span>
<span class='add'>- some blockchain</span>
<span class='add'>- wallet</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='add'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>A work in progress to stream the spirits to the collective mind.</span>
<span class='add'>The spirits can be expected to be disagreeable, conscientious, open, reasonably stable and slightly introverted.</span>
<span class='add'>They are currently reachable through &lt;a href="https://twitter.com/lorinbaumgarten">X&lt;/a>.</span>
<span class='add'></span>
<span class='add'>See note &#123;&#123;changes}} for current thought stream and history.</span>
<span class='add'>The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='add'></span>
<span class='add'>&#123;&#123;sitelist}}</span>
</div>
</div>
<span>index.md</span>
<div class='indent'>
<span class='hdg'>Aspirational spirit stream of Lorin Baumgarten</span>
<div class='indent'>
<span class='rem'>A work in progress to stream the spirits to the collective mind.</span>
<br>
<span class='add'>Occasional stream of the spirits to the collective mind.</span>
<br>
<span class='rem'>The website and notes are also open sourced &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">here&lt;/a> on GitHub.</span>
<span class='add'>Source is also available on &lt;a href="https://github.com/lorinbaum/lorinbaum.github.io">GitHub&lt;/a>.</span>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>read using [pyIntroducer](https://github.com/lorinbaum/pyintroducer)</span>
<span class='rem'>- tensor.tolist() CLANG introduced</span>
<span class='rem'>- tensor.tolist() CUDA introduced</span>
<span class='rem'>- gpt2 CUDA introduced</span>
<span class='rem'></span>
<span class='rem'>consider more abstract layers all the way to the mission., connect to code</span>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='hdg'>tinycorp mission</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>*tinygrad model --> friendly C --> standalone would be (is?) nice*</span>
</div>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='rem'>Notes on introduction of tensor.tolist() on CLANG</span>
<span class='add'>Notes on Tensor.tolist()</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>reading from [pyintroducer](https://github.com/lorinbaum/pyintroducer). </span>
<br>
<span class='rem'>by default `MERGE_VIEW=1`, so the latest `View` in the `ShapeTracker` is replaced by the new one.</span>
<span class='add'>reshape: by default `MERGE_VIEW=1`, so the latest `View` in the `ShapeTracker` is replaced by the new one.</span>
<span class='hdg'>Realize</span>
<div class='indent'>
<span class='hdg'>Schedule</span>
<div class='indent'>
<span class='rem'>`children:Dict[LazyBuffer:Dict[Lazybuffer : None]]` is in direction of execution, so a child is an lb that depends on the current one. stores only if the parent is unrealized</span>
<br>
<span class='add'>`children:Dict[LazyBuffer:Dict[Lazybuffer : None]]` is in direction of execution, so a child is a lb that depends on the current one. stores only if the parent is unrealized.</span>
</div>
<span class='hdg'>lower schedule</span>
<div class='indent'>
<span class='rem'>in `lower_schedule_item`, trying to get "transfer" attribute from the allocator, ops_clang are imported</span>
<br>
<span class='add'>in `lower_schedule_item`, trying to get "transfer" attribute from the allocator, ops_clang.py is imported</span>
</div>
</div>
</div>
</div>
</div>
<span class='hdg'>tinygrad/codegen/uops.py</span>
<div class='indent'>
<span class='rem'>which does not change sink in any way</span>
<span class='add'>neither change `sink` in any way</span>
<span class='hdg'></span>
<div class='indent'>
<span class='add'>Notes on gpt2 CLANG introduced</span>
<div class='indent'>
<span class='add'>from [pyIntroducer](https://github.com/lorinbaum/pyintroducer)</span>
<span class='add'></span>
<span class='add'>Tensor.rand uses numpy for rng. created upon realize. maybe MetaOps.CUSTOM makes realize call the function in arg, which here produces the numpy array for the buffer.</span>
<span class='add'></span>
<span class='add'>**getting weights:**</span>
<span class='add'>reads huggingface gtp2 weights as raw bytes</span>
<span class='add'>makes a huge Tensor on device DISK:path with one dim and shape of file size in bytes</span>
<span class='add'>unpickles the bytes object, helping with custom `find_class` to translate torch dtypes to tinygrad dtypes.</span>
<span class='add'>and rebuilding torch tensors with `_rebuild_tensor_v2`</span>
<span class='add'>takes slices from the huge tensor to rebuild gpt2. slice (Tensor.shrink) changes the view to have an offset and adjusted shape.</span>
<span class='add'>Does a useless flip in case the stride in the slice is -1. Does not detect that its useless, just flips anyway.</span>
<span class='add'>bitcasts from `uchar` (=`uint8`)&nbsp;&nbsp;&nbsp;&nbsp;to `float32`. -> size /= 4</span>
<span class='add'>gets the new lazybuffer a new buffer with the new shape and the old buffer as its base and offset.</span>
<span class='add'>then reshapes the flat tensor.</span>
<span class='add'></span>
<span class='add'>permute is a reshape but also changes order of strides.</span>
<span class='add'></span>
<span class='add'>it makesa model with all the tensors of gtp2, then realizes the disk tensors one by one and replaces the tensor in the gpt2 model with the realized disk tensor</span>
<span class='add'></span>
<span class='add'>```python</span>
</div>
</div>
</div>
<span class='add'>schedule for the embedding table</span>
<div class='indent'>
<span class='add'>schedule = [</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=UOp(UOps.EXT, dtypes.uchar, arg=(&lt;MetaOps.EMPTY: 1>, None), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(&lt;buf real:False device:DISK:/home/lorinbaum/.cache/tinygrad/downloads/113965bb5fe7074edc9ca25991e7ad35 size:548118077 dtype:dtypes.uchar offset:0>,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=UOp(UOps.EXT, dtypes.float, arg=(&lt;MetaOps.VIEW: 7>, dtypes.float), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:DISK:/home/lorinbaum/.cache/tinygrad/downloads/113965bb5fe7074edc9ca25991e7ad35 size:38597376 dtype:dtypes.float offset:327582053>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:DISK:/home/lorinbaum/.cache/tinygrad/downloads/113965bb5fe7074edc9ca25991e7ad35 size:548118077 dtype:dtypes.uchar offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=UOp(UOps.EXT, dtypes.float, arg=(&lt;MetaOps.COPY: 3>, 154389504), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:38597376 dtype:dtypes.float offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:DISK:/home/lorinbaum/.cache/tinygrad/downloads/113965bb5fe7074edc9ca25991e7ad35 size:38597376 dtype:dtypes.float offset:327582053></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>]</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>importing `ops_disk` to get the Buffer Allocator.</span>
<span class='add'>import `io_uring`(data transfer on linux(?)) and `libc` from `tinygrad/runtime/autogen`</span>
<span class='add'></span>
<span class='add'>9704</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>[ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.float, arg=BinaryOps.MUL, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.float, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.float, arg=0.9, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),)),)),)), bufs=(&lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>,), metadata=[__imul__])], 'var_vals': &#123;}, 'kernel_number': 0, 'lsi': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.float, arg=BinaryOps.MUL, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.float, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.float, arg=0.9, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),)),)),)), outputs=[&lt;LB CUDA (1,) float (&lt;MetaOps.ASSIGN: 6>, &lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>)>], inputs=[], var_vals=&#123;}, metadata=[__imul__]), 'buf': &lt;LB CUDA (1,) float (&lt;MetaOps.ASSIGN: 6>, &lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>)>, 'out': &lt;LB CUDA (1,) float (&lt;MetaOps.ASSIGN: 6>, &lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>)>, 'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.float, arg=BinaryOps.MUL, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.float, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.float, arg=0.9, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),)),)),)), bufs=(&lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>,), metadata=[__imul__]), 'x': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.float, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.float, arg=1.0, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.float, arg=BinaryOps.MUL, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.float, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.float), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.float, arg=-1.0, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=()),)),)),)),)),)), outputs=[&lt;LB CUDA (1,) float (&lt;BinaryOps.ADD: 1>, None)>], inputs=[&lt;LB CUDA (1,) float (&lt;MetaOps.ASSIGN: 6>, &lt;buf real:True device:CUDA size:1 dtype:dtypes.float offset:0>)>]</span>
<span class='add'></span>
<span class='add'>```</span>
<span class='hdg'></span>
<div class='indent'>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'>scheduler hard to read, should be clearer.</span>
<span class='add'></span>
</div>
<span class='hdg'>Research</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-08-12-20:12'>2024 08 12 20:12</span><div class='indent'>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='rem'>read</span>
<span class='add'>read using [pyIntroducer](https://github.com/lorinbaum/pyintroducer)</span>
</div>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='add'>`tempfile` module for temporary files that automatically delete after close</span>
</div>
<span class='hdg'>Notes on introduction of tensor.tolist() on CLANG</span>
<div class='indent'>
<span class='hdg'>Realize</span>
<div class='indent'>
<span class='hdg'>lower schedule</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>second scheduleItem:</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>lazyops are recursively verfied `verify_lazyop` (opy.py)</span>
<span class='add'></span>
<span class='add'>`hand_coded_optimizations` if no tensor cores. But they don't do anything on the add kernel.</span>
<span class='add'>would do BEAM here, but isn't enabled for this one.</span>
<span class='add'></span>
<span class='add'>-> `Kernel.to_program`</span>
<span class='add'></span>
<span class='add'>`get_optimized_ast` recursively goes through each op.</span>
<span class='add'>gives `MetaOps.KERNEL` an `arg` with `KernelInfo` dataclass</span>
<span class='add'>gives `BufferOps` arg (MemBuffer or ConstBuffer in the current Kernel) "new" shapetrackerse, which in this case are the same as before.</span>
<span class='add'></span>
<span class='add'>then verfiyes new ast -> `veryify_lazyops`</span>
<span class='add'></span>
<span class='add'>generate the UOpGraph</span>
<span class='add'>-> `lazyop_to_uop`</span>
<span class='add'></span>
<span class='add'>`sink` that is passed to UOpGraph:</span>
<span class='add'>```python</span>
<span class='add'>UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.bigint, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.bigint, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bigint, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.bool, arg=True, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>))</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>which is then linearized through `UOpGraph.linearize`</span>
<span class='add'></span>
<span class='add'>patternmatcher comes in. a new pattern matcher is used that merges patterns from const_folder and transcendental_folding</span>
<span class='add'></span>
<span class='add'>```python</span>
</div>
</div>
</div>
</div>
</div>
<span class='add'>tinygrad/codegen/uops.py</span>
<div class='indent'>
<span class='add'>def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for pat/uop and recursively their source pats and uops:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if pat and uop are valid / match: add uop to store at pat.name if there is no other uop for pat.name</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"""</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>for any matches, the callable stored in the pattern matcher will return the replacement uop.</span>
<span class='add'></span>
<span class='add'>new `sink` after pattern matching</span>
<span class='add'>```python</span>
<span class='add'>UOp(UOps.SINK, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>more pattern matching with const_folder + transcendental_folding + expander + float4_folding</span>
<span class='add'>then again with&nbsp;&nbsp;&nbsp;&nbsp;const_folder + transcendental_folding + expander + reducer</span>
<span class='add'>which does not change sink in any way</span>
<span class='add'></span>
<span class='add'>does toposort, adds and end for the range, removes SINK, which just indicated MetaOps.KERNEL</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>UOPGraph._uops = [</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), </span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ENDRANGE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>]</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>![](attachments/net.0.uops.svg)</span>
<span class='add'>UOp graph from `GRAPHUOPS=1` context variable</span>
<span class='add'>TODO: colors mean anything? -> `codegen/uopgraph.py` -> `UOPGraph.graph`</span>
<span class='add'></span>
<span class='add'>kernel gets a name (`codegen/kernel.py` -> `Kernel.name`)</span>
<span class='add'>r if any reduceops</span>
<span class='add'>C if only BufferOps</span>
<span class='add'>else E</span>
<span class='add'>+</span>
<span class='add'>optional len(Kernel.ast.src) if > 1 else nothing</span>
<span class='add'>+</span>
<span class='add'>`_`</span>
<span class='add'>+</span>
<span class='add'>numbers for shapes in different colors (?) joined by black `_`</span>
<span class='add'></span>
<span class='add'></span>
<span class='add'>Render:</span>
<span class='add'></span>
<span class='add'>iterates through uops, globals and const are stored as their name and value and inserted when needed</span>
<span class='add'>otherwise translates 1:1 UOps to valid C code.</span>
<span class='add'>```c</span>
<span class='add'>void E_3(int* restrict data0, const int* restrict data1) &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for (int ridx0 = 0; ridx0 &lt; 3; ridx0++) &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int val0 = data1[ridx0];</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data0[ridx0] = (val0+2);</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>appears to cache the kernel as bytes in an sqlite3 database?</span>
<span class='add'>writes bytes to a temp file</span>
<span class='add'>loads the kernel function from the temp file as a function using `ctypes.CDLL(path)[fname]`</span>
<span class='add'></span>
<span class='add'>`method_cache` stores the kernel as a `CompiledRunner`</span>
<span class='add'></span>
<span class='add'>returns the second `ExecItem`:</span>
<span class='add'>```python</span>
<span class='add'>ExecItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=tinygrad.engine.realize.CompiledRunner &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clprg = &lt;tinygrad.runtime.ops_clang.ClangProgram object at 0x76d56a0fb4f0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = &lt;tinygrad.runtime.ops_clang.ClangDevice object at 0x76d56a0a2b30>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = 'E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = "CLANG",</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first_run = True,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lsd_estimate = 24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lib = &#123;horrible bytemess},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate = 24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op_estimate = 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p = Program(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src='\nvoid E_3(int* restrict data0, const int* restrict data1) &#123;\n&nbsp;&nbsp;&nbsp;&nbsp;for (int ridx0 = 0; ridx0 &lt; 3; ridx0++) &#123;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int val0 = data1[ridx0];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data0[ridx0] = (val0+2);\n&nbsp;&nbsp;&nbsp;&nbsp;}\n}',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname='CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uops=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.STORE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.LOAD, dtypes.int, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=2, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.ENDRANGE, None, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=0, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UOp(UOps.CONST, dtypes.int, arg=3, src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate=24,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;global_size=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;local_size=None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vars=[],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globals=[0, 1],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outs=[0],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_ran_post_init=True</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;},</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:True device:CLANG size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__]</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>allocates buffers</span>
<span class='add'>calls the loaded c function, measuring time it takes to execute.</span>
<span class='add'></span>
<span class='add'>gets output buffer (moves it to a new buffer if not allow_zero_copy?)</span>
<span class='add'>memoryview is cast to Tensor.dtype.fmt and Tensor.shape (?)</span>
<span class='add'>memoryview.tolist() returns the final output.</span>
<span class='add'></span>
</div>
</div>
</div>
<span class='date' id='t2024-08-11-11:15'>2024 08 11 11:15</span><div class='indent'>
<span>blog post.md</span>
<div class='indent'>
<span class='hdg'>&#123;&#123;title}}</span>
<div class='indent'>
<span class='add'>[TOC]</span>
<span class='rem'>- [[#Direction]]</span>
<span class='rem'>- [[#More refined]]</span>
<span class='rem'>- [[#Less refined]]</span>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Direction</span>
<div class='indent'>
<span class='add'>read</span>
<span class='add'>- tensor.tolist() CLANG introduced</span>
<span class='add'>- tensor.tolist() CUDA introduced</span>
<span class='add'>- gpt2 CUDA introduced</span>
<br>
<span class='add'>consider more abstract layers all the way to the mission., connect to code</span>
<span class='rem'>trace execution of a tinygrad script</span>
<span class='rem'>- steps:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.tensor import Tensor`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3])`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `Tensor([1,2,3]) + 2`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `(Tensor([1,2,3]) + 2).tolist()</span>
<span class='rem'>read tensor.py</span>
<span class='rem'>explore anything unfamiliar</span>
<span class='rem'>condense any writing</span>
<span class='rem'>create more abstract layers, current writing is one layer above code. should eventually connect all the way to the mission.</span>
<span class='rem'></span>
<span class='rem'>python inliner for tinygrad?</span>
</div>
<span class='hdg'>More refined</span>
<div class='indent'>
<span class='rem'></span>
</div>
<span class='rem'>Less refined</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>tinygrad inliner</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>from reading its not obvious what happens</span>
<span class='rem'>could miss parts</span>
<span class='rem'>can't keep track of values</span>
<span class='rem'>reading too much that is irrelevant</span>
<span class='rem'></span>
<span class='rem'>inlined code would be a practical story through a structure of relationships</span>
<span class='rem'>tinygrad code lays out the structure directly</span>
<span class='rem'></span>
<span class='rem'>- write an executable python file that fulfills the same function as the traced script without calling functions</span>
<span class='rem'>- classes are maintained (Tensor, LazyBuffer(?))</span>
<span class='rem'>- function variables are renamed to be unique</span>
<span class='rem'>- function calls = indented comment = function name, source file, line number</span>
<span class='rem'>- return = indented comment</span>
<span class='rem'>- for loops and comprehensions are only shown once</span>
</div>
</div>
<span class='add'>Less refined</span>
<div class='indent'>
<span class='rem'>Importing Tensor</span>
<span class='add'>Notes on introduction of tensor.tolist() on CLANG</span>
<div class='indent'>
<span class='rem'>```python</span>
<span class='rem'>from tinygrad.tensor import Tensor</span>
<span class='rem'>```</span>
<br>
<span class='add'>TODO: WINO context var to enable winograd optimization?</span>
<span class='add'>`BinaryOps`:</span>
<span class='add'>- `IDIV` = integer division</span>
<span class='add'>- `CMPLT` = &lt;</span>
<span class='add'>- `SHL` = shift left</span>
<span class='add'>- `THREEFRY` = rng related</span>
<span class='add'>`TernaryOps`:</span>
<span class='add'>- `WHERE` = Multiplexer</span>
<span class='add'>- `MULACC`: `x*y+z</span>
<span class='add'>`ReduceOps`:</span>
<span class='add'>- `WMMA` = wave matrix multiply accumulate: hardware accelerated matrix multiplication</span>
<span class='rem'>sets the stage with 3749 lines of tinygrad code as determined through `sys.settrace` (2024-07-08 17:27)</span>
<span class='rem'>![](attachments/tinygrad_import_tensor.png)</span>
<span class='rem'>Mostly [imports](https://docs.python.org/3/reference/import.html) and the construction of the `PatternMatcher` in `tinygrad/codegen/uops.py` (marked with cyan left border)</span>
<span class='rem'>13: `helpery.py` </span>
<span class='rem'>- makes `U` and `T` `TypeVar`s</span>
<span class='rem'>- determines if the computer runs OSX to set the location of tinygrads cache</span>
<span class='rem'>- sets and caches environment variables as `ContextVar` objects.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- DEBUG, IMAGE, BEAM, NOOPT, JIT</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- WINO, THREEFRY, CAPTURING</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- MULTIOUTPUT, PROFILE</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- this does not cover all environment variables relevant to tinygrad, not even those mentioned in the docs as [global variables](https://docs.tinygrad.org/env_vars/#global-variables)</span>
<span class='rem'>- Global Counters: `global_ops`, `global_mem`, `time_sum_s`, `kernel_count`, `mem_used`</span>
<span class='rem'>- ProfileLogger (?)</span>
<span class='rem'>- sets up cache db path, cachelevel and version (?)</span>
<span class='rem'>206: `dtype.py`</span>
<span class='rem'>- `ConstType = Union[float, int, bool]`</span>
<span class='rem'>- declares dtypes as DType Objects and some aliases:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, float16, bfloat16, float32, float64</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- half = float16; float = float32; double = float64 </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- char = int8; short = int16; int = int32; long = int64</span>
<span class='rem'>- sets default float by environment variable else `float32` and default int `int32`</span>
<span class='rem'>- `promo_lattice` that defines how different dtypes get promoted, presumably when different dtypes meet in an operation.</span>
<span class='rem'>- `DTYPES_DICT` and `INVERSE_DTYPES_DICT` to translate between tinygrad dtypes and their names like "bool": dtypes.bool</span>
<span class='rem'>367: `shape/symbolic.py`</span>
<span class='rem'>- `sint = Union[int, Variable, MulNode, SumNode]`</span>
<span class='rem'>- `render_python: Dict[Type, Callable[..., str]]`&nbsp;&nbsp;&nbsp;&nbsp;where the callables return a string representing the Object in `Type`.</span>
<span class='rem'>581: `ops.py`</span>
<span class='rem'>- tinygrads ops are defined:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `UnaryOps(Enum)`: `EXP2`, `LOG2`, `CAST`, `BITCAST`, `SIN`, `SQRT`, `NEG`, `RECIP`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BinaryOps(Enum)`: `ADD`, `MUL`, `IDIV`, `MAX`, `MOD`, `CMPLT`, `CMPNE`, `XOR`, `SHL`, `SHR`, `OR`, `AND`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `TernaryOps(Enum)`: `WHERE`, `MULACC`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `ReduceOps(Enum)`: `SUM`, `MAX`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BufferOps(Enum)`: `LOAD`, `CONST`, `STORE`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `LoadOps(Enum)`: `EMPTY`, `CONST`, `COPY`, `CONTIGUOUS`, `CUSTOM`, `ASSIGN`, `VIEW`</span>
<span class='rem'>- `Op = Union[UnaryOps, BinaryOps, ReduceOps, LoadOps, TernaryOps, BufferOps]`</span>
<span class='rem'>- `UNSAFE_PAD_OPS = &#123;UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}`</span>
<span class='rem'>- `InterpretedFlopCounter: Dict[Op, Callable]` which generates `FlopCounter` objects with shape, flops and memory for various lazyops except `LoadOps`, `TernaryOps.MULACC`</span>
<span class='rem'>- `python_alu` implements lazyops using python and its math module. covers `UnaryOps` except `CAST` and `BITCAST`, `BinaryOps` and `TernaryOps.WHERE`.</span>
<span class='rem'>- `truncate: Dict[DType, Callable]` providing functions to truncate any number to the desired dtype.</span>
<span class='rem'>754: `codegen/uops.py` (Note: quick reserach says UOps are really $\mu$ (micro) operations, UPat presumably is $\mu$ pattern)</span>
<span class='rem'>- The `UOps(Enum)` class variables:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `SINK`,`VAR`,`DEFINE_GLOBAL`,`DEFINE_VAR`,`DEFINE_LOCAL`,`DEFINE_ACC`,`CONST`,`SPECIAL`,`NOOP`,`UNMUL`,`GEP`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `CAST`,`BITCAST`,`VECTORIZE`,`ALU`,`WMMA`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `LOAD`,`STORE`,`PHI`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `BARRIER`,`IF`,`RANGE`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `ENDRANGE`,`ENDIF`</span>
<span class='rem'>- `TypeVar` `T`</span>
<span class='rem'>- `constant_folder` which constructs a `PatternMatcher` singleton with a `patterns:List[Tuple[Union[UPat, UOp], Callable]]` (~500 lines)</span>
<span class='rem'>- `PatternMatcher`'s initialization takes ~1300 more lines as it constructs `UPat` objects and runs their `compile` function.</span>
<span class='rem'>2694: `device.py`</span>
<span class='rem'>- `Device = _Device()` singleton, which populates `Device._devices` with strings of devices for which there is a `runtime/uops_&#123;device}.py` file</span>
<span class='rem'>- sets defaults in `BufferOptions` class: `image = None`,`uncached`,`cpu_access`,`host`,`nolru` are all `False`</span>
<span class='rem'>- `MallocAllocator = _MallocAllocator()` singleton (no `__init__`)</span>
<span class='rem'>2816: `lazy.py`</span>
<span class='rem'>- `lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()`</span>
<span class='rem'>- `view_supported_devices = &#123;"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}`</span>
<span class='rem'>2920: `codegen/kernel.py`</span>
<span class='rem'>- `OptOps(Enum)`: `TC`,`UPCAST`,`UPCASTMID`,`UNROLL`,`LOCAL`,`GROUP`,`GROUPTOP`,`NOLOCALS`,`PADTO`</span>
<span class='rem'>- `LocalBuffer` dataclass with `name`,`size`,`dtype=dtypes.float32`,`realized=None`</span>
<span class='rem'>3007: `codegen/linearizer.py`</span>
<span class='rem'>- `render_ops: Dict[Type, Callable[..., UOp]]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- for `NumNode`,`Variable`,`MulNode`,`DivNode`,`ModNode`,`LtNode`,`SumNode`,`AndNode</span>
<span class='rem'>~3100: `engine/schedule.py`</span>
<span class='rem'>- `SCHEDULES: List = []`</span>
<span class='rem'>3299: `tensor.py`</span>
<span class='rem'>- `Tensor` class with:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `__slots__ = "lazydata", "requires_grad", "grad", "_ctx"`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `__deletable__ = ('_ctx',)`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `training`, `no_grad` are `False`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `_seed = int(time.time())`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `_rng_counter = None`</span>
<span class='rem'>- produces methods on `Tensor`class for each device in `Device._devices` like `Tensor.cuda()` as aliases for `Tensor.to("cuda")`</span>
<span class='rem'>- if `IMAGE` from environment variables `>0`, creates more aliases for `Tensor.image_conv2d` and `Tensor.image_dot` by introducing `Tensor.conv2d` and `Tensor.dot` respectively.</span>
<span class='rem'>3646: `nn/state.py`</span>
<span class='rem'>- `safe_dtypes`and `inverse_safe_dtype` dictionaries for translating between some naming (?) to tinygrad dtypes and back (inverse)</span>
<span class='rem'>3728: `engine/jit.py`</span>
<span class='rem'>- `ReturnType = TypeVar("ReturnType")`</span>
</div>
<span class='rem'>Creating a Tensor</span>
<div class='indent'>
<span class='rem'></span>
<br>
<span class='add'>class UOps(Enum):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;SINK = auto(); EXPAND = auto(); CONTRACT = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:15</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:16</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;CONST = auto(); SPECIAL = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# codegen/uops.py:17</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;NOOP = auto(); GEP = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:18</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:20</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ALU = auto(); REDUCE = auto(); WMMA = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# codegen/uops.py:21</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:23</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # codegen/uops.py:25</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ENDRANGE = auto(); ENDIF = auto() # noqa: E702 </span>
<span class='rem'>Tensor(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data: Union[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConstType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;List,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tuple,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ndarray,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MultiLazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Variable,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: Optional[Union[str, tuple, list]] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: Optional[DType] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;requires_grad: Optional[bool] = None,</span>
<span class='rem'>)</span>
<br>
<span class='add'>`PatternMatcher`?</span>
<span class='rem'>```python</span>
<span class='rem'>from tinygrad.tensor import Tensor</span>
<span class='rem'>Tensor([1,2,3])</span>
<span class='rem'>```</span>
<br>
<span class='rem'>![](attachments/tinygrad_construct_tensor.png)</span>
<span class='rem'>9656 lines (the linearizer-lowerer commit ([#4957](https://github.com/tinygrad/tinygrad/commit/6972a2569f5a848b101f4c9310d5de373328dbfb)) changed this, documentation is paused as this might be cleaned up), the cyan line marks the border between previous import code and new tensor construction code. most new code comes from `runtime/autogen/cuda.py`(magenta left border) because in this case, cuda is the device it finds for the Tensor.</span>
<br>
<span class='rem'>determine device for the Tensor using `Device.canonicalize()`, which merely formats `device` if it's not `None`, but since it is, responsibility is handed to `Device.DEFAULT` to find one.</span>
<span class='rem'>- it looks for `&#123;DEVICE}=1` in environment variables</span>
<span class='rem'>- `Device[&#123;device}]` is tried for `METAL`,`AMD`,`CUDA`, `GPU`, `CLANG`, `LLVM`, -> `Device.__get_canonicalized_item` -> eventually tries `&#123;device}Device.__init__(&#123;device})` (like `CUDADevice`) in their respective `runtime/ops_&#123;device}.py` until it finds one that returns no errors.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `METAL` fails within 3 lines when it tries to import the `Metal` library.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `AMD` imports the `AMDRenderer` from `renderer/cstyle.py` (runs ~300 lines of importing and classvariable definitions), then imports from `runtime/driver/hip_comgr.py` which tries `runtime/autogen/comgr.py` and fails within 15 lines.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- `CUDA` should fail within ~30 lines when it tries to get `libcuda.so` but in this case cuda is installed, so it imports from `runtime/ops_cuda.py`, `runtime/autogen/cuda.py` (4000+ lines of mysterious code) and `runtime/autogen/nvrtc.py`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.cstyle import CUDARenderer` which is already available from the AMD attempt earlier.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `from tinygrad.renderer.assembly import PTXRenderer`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTXRenderer` has lots of class variables:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `device="CUDA"`, `suffix="PTX"`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `global_max = (2147483647, 65535, 65535)`, `local_max = (1024, 1024, 64)`, `shared_max = 49152`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `tensor_cores: List[TensorCore]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `kernel_prefix`, `barrier`, `gid`, `gdim`, `lid`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `asm_for_op:Dict[Op, Callable]` by all appearances functions for op->assembly translation</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `supports_half: List[Op]` with a small selection of ops</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `types: Dict[DType, str]` and `men_types: Dict[DType, str]` (almost identical, except for 3 types(?)) to translate between tinygrad dtypes and apparently some other convention</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `const_requires_mov: List[DType] = [dtypes.half, dtypes.bool]`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `ptx_matcher` is another `PatternMatcher`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- `PTX = getenv("PTX")`, 0 if not given.</span>
<span class='rem'>`CUDADevice.__init__` gets itself `device_id`, `cu_device`, `context`, `arch`, `pending_copyin`, checking that the interactions with cuda (`libcuda.so`) return no errors on multiple occasions.</span>
<span class='rem'>`CUDADevice.devices.append(self)`</span>
<span class='rem'>9406: `from tinygrad.runtime.graph.cuda import CUDAGraph`</span>
<span class='rem'>calls</span>
<br>
<span class='add'>a = (Tensor([1,2,3], device="CLANG") + 2)</span>
<span class='rem'>Compiled.__init__(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;CUDAAllocator,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;PTXRenderer(self.arch) if PTX else CUDARenderer(self.arch)`, # PTX=0 (default)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;PTXCompiler(self.arch) if PTX else CUDACompiler(self.arch),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;functools.partial(CUDAProgram, self),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph=CUDAGraph</span>
<span class='rem'>)</span>
<br>
<span class='rem'>which is the superclass of `CUDADevice`, where `dname`(device name), `allocator`, `renderer`, `compiler`, `runtime`, `graph`&nbsp;&nbsp;&nbsp;&nbsp;come together and are stored in `self` (ultimately in `CUDADevice` as it inherits these instance variables from its parent classes.</span>
<span class='rem'>- `CUDAAllocator` inherits from `LRUAllocator`, calls `super().__init__()` which only runs `self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)`(sidenote: `LRUAllocator` itself also inherits from `Allocator`).</span>
<span class='rem'>- `CUDARenderer` initialization in this case stores `[]` in `self.tensor_cores`</span>
<span class='rem'>- `CUDACompiler` (child of `Compiler`) gets itself `self.arch`, `self.compile_options` and `super().__init__(f"compile_cuda_&#123;self.arch}")` which sets `self.cachekey` unless explicitly preventes through env variable `DISABLE_COMPILER_CACHE`</span>
<span class='rem'>- `CUDAGraph`, notably is not initialized, the imported class is just passed on.</span>
<br>
<span class='rem'>in `Compiler.__init__()` if `compiler` was `None` it would be replaced by the generic `Compiler()` and `renderer` by `Renderer()`.</span>
<span class='add'>- data -> numpy ndarray</span>
<span class='add'>- LazyBuffer with shapetracker from ndarray.shape</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- lazycache disabled, but `LAZYCACHE` context variable is 1 by default</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `_METADATA.get()` is `None` by default</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- `LazyBuffer` gets a `Buffer` that does not allocate anything yet.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- gets allocator from `Device`, gets `NpyDevice` from&nbsp;&nbsp;&nbsp;&nbsp;`ops_npy.py`, which is unnecessary</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;- puts the ndarray into `Buffer._buf`, deletes `LazyBuffer.srcs` (was empty tuple), so that `LazyBuffer.realized` returns `True`</span>
<span class='add'>- new `LazyBuffer` now on `CLANG`, same shapetracker, `MetaOps.COPY`, `arg` = `Buffer.size`, `srcs` = tuple with previous (npy) lazybuffer</span>
<br>
<span class='rem'>`CudaDevice` returned to `Device.__get_canonicalized_item` and cached (`@functools.lru_cache(maxsize=None)` decorator):</span>
<span class='add'>before addition:</span>
<br>
<span class='add'>Tensor.lazydata &#123;</span>
<span class='rem'>CUDADevice &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'cu_device': c_int(0),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'context': &lt;tinygrad.runtime.autogen.cuda.LP_struct_CUctx_st at 0x7f7a12c49a40>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'arch': 'sm_61',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'pending_copyin': [],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'dname': 'CUDA',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'allocator': &lt;tinygrad.runtime.ops_cuda.CUDAAllocator at 0x7f7a12dad9f0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'compiler': &lt;tinygrad.runtime.ops_cuda.CUDACompiler at 0x7f7a12cd7b20>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'runtime': functools.partial(&lt;class 'tinygrad.runtime.ops_cuda.CUDAProgram'>, &lt;tinygrad.runtime.ops_cuda.CUDADevice object at 0x7f7a12daeb60>),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'graph': tinygrad.runtime.graph.cuda.CUDAGraph,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'renderer': &lt;tinygrad.renderer.cstyle.CUDARenderer at 0x7f7a12c244f0></span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'>also this `CUDADevice` is stored in classvariable `CUDADevice.devices:List[CUDADevice]`</span>
<span class='rem'>if `DEBUG>=1`, a message will inform that the device was opened.</span>
<span class='rem'></span>
<span class='rem'>for now, the returned `CUDADevice` only demonstrates that `CUDA` can be used as a device for the new Tensor. environmentvariable `CUDA` is set to `1` to save this work in the future.</span>
<span class='rem'></span>
<span class='rem'>In Tensor construction, depending on type of data input, `_loadop()`, `_fromnp` or `_frompy` create the tensors `LazyBuffer`.</span>
<span class='rem'>The example Tensor construction determines dtype (`dtypes.default_int`), then</span>
<span class='rem'>`data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))`</span>
<span class='rem'>(numpy as a dependency is phased out, so this probably changes soon)</span>
<span class='rem'>`_from_np_dtype` uses a dictionary from `dtype.py` to translate the numpy dtype to a tinygrad `DType`</span>
<span class='rem'>-> `LazyBuffer.loadop(LoadOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@staticmethod</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def loadop(op, shape:Tuple[sint,...], dtype:DType, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert isinstance(src, tuple)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)</span>
<span class='rem'>```</span>
<span class='rem'>`op` was given as `LoadOps.EMPTY`</span>
<span class='rem'>```python</span>
<span class='rem'>ShapeTracker.from_shape(shape:Tuple[sint, ...]): return ShapeTracker((View.create(shape),))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`ShapeTracker((View.create(shape),))` to give the ShapeTracker a View. Since no stride is defined, it will be created using `strides_for_shape(shape)`, then canonicalized. Then `View(shape, stride, offset=0, mask=None, contiguous=True)` with these default arguments.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class View:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;shape: Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;strides: Tuple[sint, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;offset: sint</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;mask: Optional[Tuple[Tuple[sint, sint], ...]]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;contiguous: bool</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ShapeTracker:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;views: Tuple[View, ...]</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>create_lazybuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Any = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enable_cache = bool(getenv("LAZYCACHE", 1))</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>in `create_lazybuffer` the `lazycache` is interacted with, which stores lazybuffers. a `cache_key` is generated from the lazybuffers parameters. If the key yields an existing `LazyBuffer` from `lazycache`, that one will return, otherwise a new one is created with this constructor, where it will pass `metadata=_METADATA.get()` as `metadata` ([#5271](https://github.com/tinygrad/tinygrad/commit/9150a6be7a30bbd17f0b84f3352fac7af0c68b73)):</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>LazyBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device: str,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op: Optional[Op] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs: Tuple[LazyBuffer, ...] = (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;base: Optional[LazyBuffer] = None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata:Optional[Metadata]=None</span>
<span class='rem'>)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`st` is the `ShapeTracker` just created</span>
<span class='rem'></span>
<span class='rem'>In the lazybuffer's initialization, it finds that `base` is `None` and decides that an assignment to `self.buffer` is in order.</span>
<span class='rem'>Given the op `LoadOps.EMPTY`, it makes a `Buffer` (a class imported from `tinygrad.device`) through `Buffer(device, self.size, dtype)`. But creating it like that in this case does nothing except store the instance.</span>
<span class='rem'>the buffer's `_lb_refcount` property is incremented by 1</span>
<span class='rem'>the `contiguous_child` property (didn't exist before) is set to `None`</span>
<span class='rem'>and `forced_realize` to `False`</span>
<span class='rem'>the meaning of all 3 escapes me right now.</span>
<span class='rem'></span>
<span class='rem'>The `LazyBuffer` is done and returning to `_fromnp()` into the variable `ret` where:</span>
<span class='rem'>`ret.buffer.allocate(x)` (x is a numpy array) causes the buffer to find itself an `Allocator`:</span>
<span class='rem'>`self.allocator = Device[self.device].allocator`. Indexing into `Device` returns a `NpyDevice` (same as earlier when it was about finding an available device, but this time with `NPY`. This device is very minimal, has the default `Compiler` and `Renderer` and a mostly empty `NpyAllocator`)</span>
<span class='rem'></span>
<span class='rem'>on `buffer.allocate(x)` where `x` is the `np.ndarray`, `x` is just assigned to `buffer._buf`, without calling `Buffer.alloc` which is not implemented for this device.</span>
<span class='rem'>completing what is commented "fake realize" in `_fromnpy`, `del ret.srcs` (which was `()`) makes sure that `LazyBuffer.realized` will return `True`.</span>
<span class='rem'>Also adds the buffer's size to `GlobalCounters.mem_used`</span>
<span class='rem'></span>
<span class='rem'>In the final step of `Tensor` initialization, the mismatching devices, one being the discovered one (`CUDA` in this case) and one being `NPY` are detected and `self.lazydata = data.copy_to_device(device)` takes care of it, `data` being the created `LazyBuffer` and `device` being the discovered device from the start.</span>
<span class='rem'>`LazyBuffer.copy_to_device(device)` in this case leads to `self.base._copy(device)._view(self.st)`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
</div>
</div>
</div>
<span class='rem'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='rem'>return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, LoadOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>assign a&nbsp;&nbsp;&nbsp;&nbsp;`Buffer` to the `LazyBuffer`, because `base` is `None` again (the npy lazybuffer is stored in `srcs`).</span>
<span class='rem'></span>
<span class='rem'>the `._view(self.st)` that follows `._copy(device)`, does nothing here, because the new shapetracker has the same shape and is contiguous.</span>
<span class='rem'></span>
<span class='rem'>The final object looks like this:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata': &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,), </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;MetaOps.COPY: 3>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (&lt;LB NPY (3,) int (&lt;MetaOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='add'>}</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>addition calls `_metadata_wrapper`, sets `_METADATA` to `__add__`, the function being called.</span>
<span class='add'>to add, 2 turns into a Tensor where the lazydata has `op=MetaOps.CONST, shape=(), arg=2`</span>
<span class='add'>to add, shapes must match, the "2 Tensor" is broadcasted from shape `()` to `(3,)`</span>
<span class='add'>`()`-> reshape -> `(1,)` -> expand -> `(3,)`</span>
<span class='add'></span>
<span class='add'>by default `MERGE_VIEW=1`, so the latest `View` in the `ShapeTracker` is replaced by the new one.</span>
<span class='add'>new `LazyBuffer` with `base` being the previous `LazyBuffer`</span>
<span class='add'>`expand` always replaces latest view in shapetracker regardless of `MERGE_ViEW`</span>
<span class='add'>again new `LazyBuffer` with the base of the previous one. The "intermediate" lazybuffer from reshape is garbage collected.</span>
<span class='add'></span>
<span class='add'>Tensor(2) after broadcasting:</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>Tensor.lazydata &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': expand,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'_base': LazyBuffer &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 1,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.COPY: 3>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;MetaOps.CONST: 2>,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 12,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': LazyBuffer &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': 'NPY',</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides': (1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset': 0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous': True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,)),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.EMPTY: 1>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CLANG size:1 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='rem'>Adding to a Tensor</span>
<div class='indent'>
<span class='add'>addition creates new LazyBuffer;</span>
<span class='rem'>```python</span>
<span class='rem'>t = Tensor([1,2,3]) + 2</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>goes to `Tensor.add(self, x, reverse=False)`</span>
<span class='rem'>-> `return F.Add.apply(*self._broadcasted(x, reverse))`</span>
<span class='rem'></span>
<span class='rem'>`self._broadcasted` determines dtype then creates Tensor from `y` (2) using:</span>
<span class='rem'>`Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)`</span>
<span class='rem'>where `dtypes.as_const()` casts the input using one of pythons `int`, `float`, `bool` functions. Reason still escapes me.</span>
<span class='rem'></span>
<span class='rem'>bypassing the whole numpy story because data is integer and not array this time, so lazybuffer comes more directly from `_loadop(LoadOps.CONST, tuple(), dtype, device, data)` where `data` eventually ends up as the lazybuffers `arg` property.</span>
<span class='rem'></span>
<span class='rem'>The `ShapeTracker` will be empty, because the provided shape is `tuple()`. (its a 0D Tensor)</span>
<span class='rem'></span>
<span class='rem'>Because `op` is `LoadOps.CONST` and dtype is `int` the data once again runs through `dtypes.as_const()` and `enable_cache` (-> `lazycache`)&nbsp;&nbsp;&nbsp;&nbsp;is enabled.</span>
<span class='rem'></span>
<span class='rem'>the returned `Tensor.lazydata`:</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CUDA',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'device': 'CLANG',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 1,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'size': 3,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': None,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'metadata': __add__,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;BinaryOps.ADD: 1>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'arg': None,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (),</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'srcs': (</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>```</span>
<br>
<span class='rem'>back in `_broadcasted`, dtypes of x and y are matched</span>
<span class='rem'>`_broadcast_shape(x.shape, y.shape)` determines a target shape</span>
<span class='rem'>and broadcast `x` and `y` to that shape (x is already that shape so nothing happens)</span>
<span class='rem'></span>
<span class='rem'>`padded = _pad_left(y.shape, shape)` where `shape` is the target shape transforms `()` to `(1,)`, ready to be expanded through `F.Expand.apply(self.reshape(padded), shape=shape)`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='rem'>all `Function` "children", in their `apply`function, return a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if&nbsp;&nbsp;&nbsp;&nbsp;applicable. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='rem'>`Function.apply()` calls the functions `forward` method on the `Tensor.lazydata`</span>
<span class='rem'></span>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape(newShape))` in `lazy.py`.</span>
<span class='rem'>In `st.reshape(newShape)` (`shapetracker.py`), by default, the new returned `ShapeTracker` will have its most recent view in `views` replaced by a new one, through `View.reshape(newShape)`.</span>
<span class='rem'>Environment variable `MERGE_VIEWS=0` changes this behaviour to including all previous views with the new one appended in the new shapetracker.</span>
<span class='rem'></span>
<span class='rem'>`View.reshape(newShape)` in this case simply returns a new View from `View.create(newShape)`</span>
<span class='rem'>strides for the new shape&nbsp;&nbsp;&nbsp;&nbsp;are determined (`strides_for_shape(shape)` -> `(1,)`) and canonicalized -> `(0,)`.</span>
<span class='rem'>finally:</span>
<span class='rem'>```python</span>
<span class='rem'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='rem'>return View(shape, strides, offset, mask, contiguous)</span>
<br>
<span class='add'>>The `LazyBuffer` graph specifies the compute in terms of low level tinygrad ops. Not all LazyBuffers will actually become realized. There's two types of LazyBuffers, base and view. base contains compute into a contiguous buffer, and view is a view (specified by a ShapeTracker). Inputs to a base can be either base or view, inputs to a view can only be a single base.</span>
<span class='add'>>- [tinygrad docs](https://docs.tinygrad.org/developer/developer/#tinygrad.lazy.LazyBuffer)</span>
<span class='rem'>back at `_view(newShapetracker)` in `lazy.py` a new lazybuffer comes from `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`.</span>
<span class='rem'>notably, `self.base` is just `self` because `self._base` is `None`</span>
<span class='rem'>```python</span>
<span class='rem'>@property</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def base(self) -> LazyBuffer: return self._base if self._base is not None else self</span>
<span class='rem'>```</span>
<br>
<span class='rem'>next from `F.Expand.apply(self.reshape(padded), shape=(3,))`, where `self.reshape(padded)` has now returned the new Tensor. Expand similarly returns a new Tensor with a new LazyBuffer from&nbsp;&nbsp;&nbsp;&nbsp;`LazyBuffer.expand` -> `ShapeTracker.expand` -> `View.expand` -> `View.create(new_shape, self.strides, self.offset, mask)` -> `View` -> `ShapeTracker` -> `LazyBuffer._view` -> `createLazyBuffer` -> `LazyBuffer`</span>
<span class='add'>Realize</span>
<div class='indent'>
<span class='rem'>notably, `View.create` does not change strides and since no mask was given it also remains `None`. These lines:</span>
<br>
<span class='add'>a.tolist()</span>
<span class='rem'>contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)</span>
<span class='rem'>return View(shape, strides, offset, mask, contiguous)</span>
<br>
<span class='rem'>cause `contiguous` to be `False` because the unchaged stride is `(0,)`, but the appropriate stride for the new shape would be `(1,)`</span>
<span class='rem'>Notably, `create_lazybuffer(self.device, new_st, self.dtype, base=self.base)` takes the base of the "reshape lazybuffer" which is the LoadOps.CONST lazybuffer. So in the final Tensor, there remains no reference to the reshape lazybuffer:</span>
<br>
<span class='rem'>```python</span>
<span class='rem'>Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'_ctx': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'requires_grad' : None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'grad': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;'lazydata':</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st' : ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous':False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': (3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 3</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': LazyBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'device': "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'st': ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape':(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'strides':(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'offset':0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'mask':None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous'=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dtype': dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'shape': ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'size': 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'_base': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'op': &lt;LoadOps.CONST: 2></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'arg': 2</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'srcs': ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'buffer': &lt;buf real:False device:CUDA size:1 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'contiguous_child': None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'forced_realize': False</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>Finally, `F.Add.apply` is called on the input tensor and the created Tensor.</span>
<span class='rem'>new tensor lazydata = `return x.e(BinaryOps.ADD, y)` where `BinaryOps.ADD`, like `LoadOps.CONST` is an entry in `class BinaryOps(Enum)`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def e(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;self, </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op:Union[LoadOps, UnaryOps, BinaryOps, TernaryOps],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;*in_srcs:LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg:Optional[Any] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;) -> LazyBuffer</span>
<span class='rem'>```</span>
<span class='rem'>gets `out_dtype` from input</span>
<span class='rem'>tries shortcuts if one of the operants is effectively 0</span>
<span class='rem'>```python</span>
<span class='rem'>create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;_ctx = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;requires_grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;grad = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;lazydata:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = "CUDA"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = ShapeTracker(views=(View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides = (1,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset = 0</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous = True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype = dtypes.int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape = (3,)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_base = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op = &lt;BinaryOps.ADD: 1></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;srcs = (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>, # previously created lazybuffer [1,2,3] copied from NPY </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))> # new lazybuffer from 2</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer = &lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous_child = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = False</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>It seems, tinygrads laziness means that operations are initially stored in lazybuffers that reference other lazybuffers through `srcs` (in ADD in this case) or `_base` (in shape changes) and so form a graph.</span>
<span class='rem'>```bash</span>
<span class='rem'>DEBUG=4 CUDA=1 python -c "from tinygrad.tensor import Tensor; (Tensor([1,2,3]) + 2).tolist()"</span>
<span class='rem'>```</span>
<span class='rem'>displays a graph that seem to echo this, though shape changes are apparently left out</span>
<span class='rem'>```</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;0 ━┳ BufferOps.STORE MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;┗━┳ BinaryOps.ADD None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;┣━━ BufferOps.LOAD MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;┗━━ BufferOps.CONST ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)))</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Realizing a Tensor</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>(Tensor([1,2,3]) + 2).tolist()</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`Tensor.tolist()` = `Tensor.data().tolist()` = `Tensor._data().cast(self.dtype.fmt, self.shape).tolist()`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def _data(self) -> memoryview:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if 0 in self.shape: return memoryview(bytearray(0))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this realizes on the object from as_buffer being a Python object</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()</span>
<br>
<span class='add'>-> `cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()`</span>
<br>
<span class='add'>cast does nothing because its already the right dtype</span>
<span class='add'>contiguous does nothing because its already contiguous</span>
<span class='add'>to does nothing, its already on CLANG. Note: this initially looks like it would realize on CLANG regardless of device but really, it just adds a CLANG lazybuffer to the end of the graph, to move the data to the cpu</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.device != "CLANG": buf.options = BufferOptions(nolru=True)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)</span>
<span class='rem'>```</span>
<br>
<span class='rem'>`Tensor.cast(self.dtype.scalar())` does nothing because `self.dtype == self.dtype.scalar()` in this case.</span>
<span class='rem'>`Tensor.contiguous()` -> `lazydata.base.forced_realize = True`, otherwise nothing in this case, because not needed.</span>
<span class='rem'>`Tensor.to("CLANG")`. if it is not already on CLANG, it makes a new Tensor with the same lazydata, but `device="CLANG"`, so it add a `LoadOps.COPY` Lazybuffer to the graph.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>Schedule</span>
<div class='indent'>
<span class='add'>Schedule</span>
<div class='indent'>
<span class='add'>>The scheduler converts the graph of LazyBuffers into a list of ScheduleItem. One ScheduleItem is one kernel on the GPU, and the scheduler is responsible for breaking the large compute graph into subgraphs that can fit in a kernel. ast specifies what compute to run, and bufs specifies what buffers to run it on.</span>
<span class='add'>>- [tinygrad docs](https://docs.tinygrad.org/developer/developer/#scheduling)</span>
<span class='add'></span>
<span class='rem'>```python </span>
<br>
<span class='add'>```python</span>
<span class='rem'>def schedule_with_vars(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;self,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;*lst:Tensor,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='rem'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# left out some lines that aren't executed</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return memory_planner(schedule), var_vals</span>
<span class='rem'>```</span>
<span class='rem'>where `flatten` in this case returns a list with one entry: the "BinaryOps.ADD-lazybuffer" </span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>SCHEDULES: List = []</span>
<span class='rem'>def create_schedule_with_vars(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Optional[Set[LazyBuffer]]=None</span>
<span class='rem'>) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if seen is None: seen = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph, in_degree, prescheduled = _graph_schedule(outs, seen)</span>
<span class='rem'></span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def _graph_schedule(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;outs:List[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;seen:Set[LazyBuffer]</span>
<span class='rem'>) -> Tuple[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;DefaultDict[LazyBuffer, List[LazyBuffer]], DefaultDict[LazyBuffer, int],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;Dict[LazyBuffer, _LBScheduleItem]</span>
<span class='rem'>]:</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""create a graph for realizing the outputs"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# start by just realizing the buffers passed in</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;realizes: Dict[LazyBuffer, None] = &#123;x.base:None for x in outs if x.base.realized is None}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs: Dict[LazyBuffer, None] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;simple_pads: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;children: DefaultDict[LazyBuffer, Dict[LazyBuffer, None]] = defaultdict(dict)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for out in outs: _recurse_lb(out.base, realizes, allbufs, simple_pads, children, scheduled=True)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;```</span>
<span class='rem'>strange that it uses `out.base` it means if the latest lazybuffer was already on clang and a reshape, it would be ignored for now.</span>
<span class='rem'></span>
<span class='rem'>from `engine/schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def _recurse_lb(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;buf:LazyBuffer,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;realizes:Dict[LazyBuffer, None],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs:Dict[LazyBuffer, None],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads:Set[LazyBuffer],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled=False</span>
<span class='rem'>):</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""recursively search the entire graph for all LazyBuffers, insert realizes after expands"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in allbufs or buf.base.realized is not None: return</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH: log_lazybuffer(buf, scheduled)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# view</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.base != buf:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fuse some pads</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simple_pads.add(buf.base)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize all expands</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif prod(buf.base.st.shape) &lt; prod(buf.st.shape):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op in ReduceOps and buf.base.srcs[0].base.op is LoadOps.CONST:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize reduceops on const (unless base is forced_realize)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass # don't realize image to image casts. this is part of a larger problem</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# check all other pads for safe fusion</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif any(v.mask is not None for v in buf.st.views): simple_pads.add(buf.base)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# base</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;allbufs[buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.forced_realize: realizes[buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in LoadOps: realizes[buf.base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.COPY:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[buf.srcs[0].base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op is LoadOps.VIEW: realizes[buf.srcs[0].base] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for x in buf.srcs:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if x.base.realized is None: children[x.base][buf] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recurse_lb(x, realizes, allbufs, simple_pads, children)</span>
<span class='rem'></span>
<span class='rem'>def _is_padding_okay(buf:LazyBuffer, realizes:Dict[LazyBuffer, None]) -> bool:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf in realizes or buf.realized is not None: return True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# NOTE: this broke to_image_idx and coder with JIT</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if buf.op in UNSAFE_PAD_OPS: return False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return all(_is_padding_okay(x.base, realizes) for x in buf.srcs)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`realizes` = lbs with `self.forced_realize` or that are `LoadOps` or source of `LoadOps.COPY` and base of view lbs if the lb was expanded compared to its base, unless exceptions.</span>
<span class='rem'>```python</span>
<span class='rem'>realizes = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None # source of copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None # copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)> = None # src of copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None # base of view lb</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`allbufs` = base lbs (no view lazybuffers).</span>
<span class='rem'>the NPY LoadOps.EMPTY lazybuffer isn't included because for it `self.realized` returns true which returns from `_recurse_lb` before it could be added to `allbufs`.</span>
<span class='rem'>```python</span>
<span class='rem'>allbufs = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`simple_pads` = lb base if there is a mask&nbsp;&nbsp;&nbsp;&nbsp;= `&#123;}`</span>
<span class='rem'></span>
<span class='rem'>`children` = unrealized lbs in `srcs`.</span>
<span class='rem'>```python</span>
<span class='rem'>children = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)> = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)> = None</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>back in `_graph_schedule`:</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in realizes if x.op is LoadOps.ASSIGN and x not in seen and x.realized is None}</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# check if we have to realize pads</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for p in simple_pads:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not _is_padding_okay(p, realizes):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[p] = None</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# find all reduces, and pair them to a elementwise op. if they can't be cleanly paired, force realize the reduce (or a contig child)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op: Dict[LazyBuffer, LazyBuffer] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for r in allbufs:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if r.op not in ReduceOps or r in realizes: continue</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_recursive_group(r, r.st, r, children, realizes, reduce_for_op, group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;can_chase = all(tr not in reduce_for_op for tr in group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# TODO: forced_realize exists because the scheduler is incapable of checking for self-contained DAGs</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forced_realize = r in group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and len(group) > 1:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a multi output kernel if the LazyBufferss can cleanly group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_parents, rc_children = deque(group), deque(group)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_parents and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# max one reduceop per kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=rc_parents.pop()).op in ReduceOps: forced_realize = True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: rc_parents.extend(x.base for x in p.srcs if x.base.realized is None and x.base is not r)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# search descendants of the reduceop that can cleanly group</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants: Set[LazyBuffer] = set()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while rc_children and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (c:=rc_children.pop()).op in ReduceOps or not c.st.contiguous or c.st.size != r.st.size or c in reduce_for_op:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realized_descendants.clear()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if c in realizes and c not in group: realized_descendants.add(c)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rc_children.extend(x for x in children[c] if x.realized is None and x.device == r.device)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group.update(realized_descendants)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can only fuse assign if no other assign_target is used in the kernel</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not forced_realize and any(x.op is LoadOps.ASSIGN for x in group):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents = deque((r, *group))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while parents and not forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (p:=parents.pop().base).realized or p in realizes:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if p in assign_targets and assign_targets[p] not in group: forced_realize, can_chase = True, False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents.extend(p.srcs)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if forced_realize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = r</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if can_chase:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# can chase this down to contiguous children</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = tr.st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while len(children[tr]) == 1:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr_next = next(iter(children[tr]))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st_childs = dedup(s for s in tr_next.srcs if s.base is tr)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(st_childs) > 1: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if st.size != st_childs[0].st.size: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = st + st_childs[0].st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not st.contiguous or tr_next.op in ReduceOps: break</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr_next</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# don't cast to higher size before store (tr cannot be realized if forced_realize)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if tr.op is UnaryOps.CAST and tr.arg.itemsize > tr.srcs[0].dtype.itemsize:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tr = tr.srcs[0].base</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reduce_for_op[tr] = r</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;realizes[tr] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: reduce_for_op.update((tr, r) for tr in group)</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;output_groups: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in realizes:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.realized is not None or buf.op is LoadOps.CONST or buf in seen: continue</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups[reduce_for_op[buf] if buf in reduce_for_op and MULTIOUTPUT else buf].append(buf)</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# make things that can't be images not images</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if isinstance(buf.dtype, ImageDType) and (prod(buf.shape) != prod(buf.dtype.shape) or</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not any(buf.shape[x]%4 == 0 for x in buf.st.unit_stride_axes())):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2: print(f"forcing image &#123;buf.dtype} with shape &#123;buf.shape} to float32")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.dtype = dtypes.float32</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# hack the underlying buffer too</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if buf.base is buf:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert not hasattr(buf.buffer, '_buf'), "can't fixup allocated buffer"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.dtype = dtypes.float32</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buf.buffer.options = None</span>
<span class='rem'> # preschedule all buffers in realizes</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prescheduled = &#123;group[0]:(group, *_lower_lazybuffer(group, realizes, reduce_for_op)) for group in output_groups.values()}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>(current value): </span>
<span class='rem'>```python</span>
<span class='rem'>output_groups = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [same buffer]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [same buffer]</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None], reduce_for_op:Dict[LazyBuffer, LazyBuffer]):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;"""describe the computation for a LazyBuffer with LazyOp + inputs + var_vals"""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if (out:=outs[0]).op is LoadOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rd = LazyOp(BufferOps.LOAD, (), MemBuffer(1, dtypes.uint8, st:=ShapeTracker.from_shape((out.arg,))))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return (LazyOp(BufferOps.STORE, (rd,), MemBuffer(0, dtypes.uint8, st)), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if out.op in &#123;LoadOps.CUSTOM, LoadOps.COPY, LoadOps.EMPTY, LoadOps.VIEW}: return (LazyOp(out.op, (), out.arg), ), [x.base for x in out.srcs], &#123;}, []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = merge_dicts([out.st.var_vals.copy() for out in outs])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assign_targets = &#123;x.srcs[1]:x for x in outs if x.op is LoadOps.ASSIGN}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;cache: Dict[Tuple[LazyBuffer, ShapeTracker], LazyOp] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ast: List[LazyOp] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for i, out in enumerate(outs):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_st = ShapeTracker.from_shape(reduce_for_op[out].shape if out in reduce_for_op else out.shape)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view = out.arg[0] if out.op is LoadOps.ASSIGN and out.arg else output_st</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lop = _recursive_lazyop(out, inputs, tuple(outs), var_vals, output_st, realizes, assign_targets, cache=cache)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_view, vv = output_view.simplify().unbind()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if vv: var_vals.update(vv)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return tuple(ast), inputs, var_vals, dedup([x[0].metadata for x in cache if x[0].metadata and x[0] not in inputs])</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True, eq=False)</span>
<span class='rem'>class LazyOp:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;op: Op</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;src: Tuple[LazyOp, ...] = ()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;arg: Any = None</span>
<span class='rem'></span>
<br>
<span class='add'>class ScheduleItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;ast: LazyOp</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: Tuple[Buffer, ...]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>class MemBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;idx: int</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<span class='rem'></span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ConstBuffer:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;val: ConstType | Variable</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;dtype: DType</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;st: ShapeTracker</span>
<br>
<span class='add'>`children:Dict[LazyBuffer:Dict[Lazybuffer : None]]` is in direction of execution, so a child is an lb that depends on the current one. stores only if the parent is unrealized</span>
<span class='add'>`realizes:Dict[LazyBuffer : None]` stores unrealized bases and ones with `MetaOps`, so it includes the realized npy lb.</span>
<span class='add'>`output_groups` stores lb that are unrealized and not `MetaOps.CONST`</span>
<span class='add'>lots of complexity is skipped because there are no ReduceOps in the lb graph.</span>
<span class='add'></span>
<span class='add'>lop recursively gets a graph of LazyOp from the LazyBuffer graph.</span>
<span class='add'>MetaOps.CONST - lbs turn into a LazyOp with BufferOps.CONST, no sources and a ConstBuffer</span>
<span class='add'>The realized npy lb turns into a LazyOps with BufferOps.LOAD, no sources and a MemBuffer. adds the buffer to inputs, like other lbs would that are either realized or in realizes but not in the current output_group</span>
<span class='add'>ast gets a LazyOp with BufferOps.STORE, lops as sources and a MemBuffer</span>
<span class='add'></span>
<span class='rem'>`_lower_lazybuffer` returns:</span>
<br>
<span class='add'>`_lower_lazybuffer` returns</span>
<span class='rem'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` for `CLANG` copy </span>
<span class='rem'>- enters `_recursive_lazyop` when processing `output_groups[1]`</span>
<span class='rem'>- `(LazyOp(LoadOps.COPY, (), 12),), [LB CUDA BinaryOps.ADD], &#123;}, []` again for the `CUDA` copy </span>
<span class='add'>- `LazyOp(MetaOps.KERNEL, tuple(ast))`</span>
<span class='add'>- `list(input)`</span>
<span class='add'>- var_vals</span>
<span class='add'>- some metadata</span>
<br>
<span class='add'>the next group, the lb with MetaOps.COPY returns different: returns a LazyOp with MetaOps.COPY and same sources and arg from the lb.</span>
<span class='add'> </span>
<span class='rem'>`_recursive_lazyop` returns `LazyOp` for the two copy lbs and the add lb in `output_groups`.</span>
<span class='rem'>in the add lb it recurses trough its sources:</span>
<span class='rem'>- `&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- simplify and unbind shapetracker (simplify does nothing here because the shapetracker has only one view. Unbind seems to act on variables, of which there aren't any here).</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- append the lb to `inputs`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.LOAD, (), MemBuffer(len(outputs)+inputs.index(buf), buf.dtype, unbound_st)))</span>
<span class='rem'>- `&lt;LB CUDA (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>)`</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- switch to its base `&lt;LB CUDA () int (&lt;LoadOps.CONST: 2>, None)></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;- return `LazyOp(BufferOps.CONST, (), ConstBuffer(val, buf.dtype, unbound_st))` where `val` is `arg`, which is 2.</span>
<span class='rem'>`ast.append(LazyOp(BufferOps.STORE, (lop, ), MemBuffer(i, out.dtype, output_view)))'</span>
<span class='rem'>`return tuple(ast), inputs, var_vals,` + metadata stuff, ignored for now. `var_vals` is `&#123;}` because nothing symbolic in this case.</span>
<span class='rem'></span>
<br>
<span class='rem'>prescheduled:List[Tuple[]] &#123;</span>
<span class='add'>prescheduled:Dict[</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;group[0]:LazyBuffer (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;"group[0]":LazyBuffer : Tuple[</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group: List[LazyBuffer],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"group": List[LazyBuffer]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;abstract syntax tree (ast): Tuple[LazyOp],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs: List[LazyBuffer]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"inputs": List[LazyBuffer],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;variable values: Dict[Variable, int],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"var_vals": Dict[],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata: List[?]</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata": List[]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;]</span>
<span class='add'>]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;BinaryOps.ADD: 1>, None)>: (</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(MetaOps.KERNEL, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.STORE, arg=MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BinaryOps.ADD, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.LOAD, arg=MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.CONST, arg=ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[__add__]</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: (</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>: (</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CLANG (3,) int (&lt;MetaOps.COPY: 3>, None)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(MetaOps.COPY, arg=12, src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[__add__ - __main__:3::&lt;module>]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;MetaOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<br>
<span class='add'>... skipping some ...</span>
<span class='rem'>back in `_graph_schedule`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule_targets = &#123;out:ps for ps in prescheduled.values() for out in ps.outputs}</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;graph: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;in_degree: DefaultDict[LazyBuffer, int] = defaultdict(int)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for key, lsi in prescheduled.items():</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if key not in in_degree: in_degree[key] = 0</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs after all parents are realized</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled_parents = set(schedule_targets[x].outputs[0] for x in lsi.inputs if x in schedule_targets)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for x in scheduled_parents:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[x].append(key)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[key] += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# realize outputs before a parent is assigned to</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parents_assigns = set(schedule_targets[assign_targets[x]].outputs[0] for x in lsi.inputs if x in assign_targets)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for assign in parents_assigns:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;graph[key].append(assign)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[assign] += 1</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return graph, in_degree, prescheduled</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`lsi` = LazyScheduleItem?</span>
<span class='rem'>`schedule_targets` makes an entry for every item in every group and assigns it the tuple in `prescheduled` that it is part of.</span>
<span class='rem'></span>
<span class='rem'>`scheduled_parents = set(schedule_targets[x][0][0] for x in lsi[2] if x in schedule_targets)`</span>
<span class='rem'>`lsi[2]` is inputs, so if an input is one of the entries in a lazybuffer group, add the tuple with its info.</span>
<span class='rem'>this returns an empty set for the third group, because its input (the `NPY` lazybuffer) is not in any group (= not in `output_groups` because it is already realized)</span>
<span class='rem'></span>
<span class='rem'>the input group's `group[0]` as a key in `graph` and append the current prescheduled `key`</span>
<span class='rem'>some detailed explanation: The `ADD` lb is the first key in `graph` because it is the input of the first group (where the key is the `COPY` lb) in `prescheduled` that is also part of group itself. The value it gets assigned is the first item in the group that it was an input of, so, the `COPY` lb.</span>
<span class='rem'>this way, graph "points" from the inputs to the groups that depend on them.</span>
<span class='rem'>every time an input of a group is added to graph this way, the groups key in the `in_degree` dictionary increases by 1.</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>graph = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: [&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: [&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>]</span>
<span class='rem'>}</span>
<span class='rem'></span>
<span class='rem'>in_degree = &#123;</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CLANG (3,) int (&lt;LoadOps.COPY: 3>, None)>: 1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;BinaryOps.ADD: 1>, None)>: 1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>: 0</span>
<span class='rem'>}</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>back in `create_schedule_with_vars`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;queue = deque(si for key, si in prescheduled.items() if in_degree[key] == 0)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule: List[ScheduleItem] = []</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals: Dict[Variable, int] = &#123;}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;kernel_number = GlobalCounters.kernel_count</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;while queue:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps = queue.popleft()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for buf in ps.outputs: seen.add(buf)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if GRAPH:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_number += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: realized_lazybuffer(out, kernel_number)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;var_vals = merge_dicts([var_vals, ps.var_vals])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for out in ps.outputs: del out.srcs&nbsp;&nbsp;&nbsp;&nbsp;# can only schedule once</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;schedule.append(si:=ScheduleItem(ps.ast, tuple(x.buffer for x in (ps.outputs+ps.inputs) if x.size != 0)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if logops and si.ast[0].op not in LoadOps and not any(i.device.startswith("DISK:") for i in si.inputs): logops.write(str(si.ast)+"\n")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for x in graph[ps.outputs[0]]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in_degree[x] -= 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if in_degree[x] == 0: queue.append(prescheduled[x])</span>
<span class='rem'></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if SAVE_SCHEDULE:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def _save():</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"saving &#123;len(SCHEDULES)} schedule graphs to", fp:=getenv("SAVE_SCHEDULE_PATH", "schedule.pkl"))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with open(fp, "wb") as f: pickle.dump(SCHEDULES, f)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if len(SCHEDULES) == 0: atexit.register(_save)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;SCHEDULES.extend((ps.ast for ps in prescheduled.values()) if getenv("CAPTURE_AST") else [(graph, prescheduled)])</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# confirm everything was scheduled correctly</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if not all(degree == 0 for degree in in_degree.values()) or len(prescheduled) != len(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"cycle detected in graph, prescheduled &#123;len(prescheduled)} but only scheduled &#123;len(schedule)}")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 1 and len(schedule) >= 10: print(f"scheduled &#123;len(schedule)} kernels")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return schedule, var_vals</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>queue = deque([</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp; (</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB CUDA (3,) int (&lt;LoadOps.COPY: 3>, None)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LazyOp(op=LoadOps.COPY, src=(), arg=12),),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[&lt;LB NPY (3,) int (&lt;LoadOps.EMPTY: 1>, &lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#123;},</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>])</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>adds any buffers of the group to `seen`.</span>
<span class='rem'>deletes `srcs` of lazybuffers in the group</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ScheduleItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ast: Tuple[LazyOp, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: Tuple[Buffer, ...]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>schedule.append(si:=ScheduleItem(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps[1],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;tuple(x.buffer for x in ps[0]+ps[2] if x.size != 0),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ps[4]</span>
<span class='rem'>))</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>then finds the lb it just made a `ScheduleItem` from in `graph`, which returns the `group[0]` item of the groups that depend on the just processed one.</span>
<span class='rem'>Add the group tuple from `prescheduled` to `queue`.</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=LazyOp(MetaOps.COPY, arg=12, src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=LazyOp(MetaOps.KERNEL, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.STORE, arg=MemBuffer(idx=0, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BinaryOps.ADD, arg=None, src=(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.LOAD, arg=MemBuffer(idx=1, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))), src=()),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(BufferOps.CONST, arg=ConstBuffer(val=2, dtype=dtypes.int, st=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))), src=()),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.STORE,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BinaryOps.ADD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.LOAD,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=1,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=BufferOps.CONST,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=ConstBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val=2,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(0,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=MemBuffer(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;idx=0, </span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dtype=dtypes.int,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st=ShapeTracker(views=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape=(3,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=(1,),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;offset=0,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask=None,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contiguous=True</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__ - __main__:3::&lt;module>]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;ScheduleItem(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ast=(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LazyOp(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op=LoadOps.COPY,</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src=(),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arg=12</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0></span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0></span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata=[__add__]</span>
<br>
<span class='rem'>with `GRAPH=1`, tinygrad produces output that reflects this schedule:</span>
<br>
<span class='add'>with `GRAPH=1`, tinygrad produces an svg that reflects this schedule:</span>
<br>
<span class='add'>`_internal_memory_planner` does nothing here</span>
<span class='rem'>back in `tensor.py` -> `schedule_with_vars`</span>
<span class='rem'>```python</span>
<span class='rem'>return memory_planner(schedule), var_vals</span>
<span class='rem'>```</span>
</div>
<span class='add'>lower schedule</span>
<div class='indent'>
<span class='rem'>-> `schedule.py`</span>
<span class='rem'>```python</span>
<span class='rem'>def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;# Exclude buffers involved in load ops (e.g transfers) to preserve parallelism in graphs.</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assigned = _internal_memory_planner(</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[si.bufs for si in schedule],</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;noopt_buffers=&#123;b for si in schedule if si.ast.op is not MetaOps.SINK for b in si.bufs}</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]</span>
<span class='rem'>```</span>
<br>
<span class='add'>in `lower_schedule_item`, trying to get "transfer" attribute from the allocator, ops_clang are imported</span>
<span class='rem'>`_internal_memory_planner` is optional (`NO_MEMORY_PLANNER=1` skips it and it still works).</span>
<span class='rem'>actually, it skips all buffers where `buf.lb_refcount > 0` anyway , which applies to all of the current buffers since they all come from the `LazyBuffer` constructor, where they automatically get a reference. `assigned` will be `&#123;}`.</span>
<br>
<span class='add'>first ExecItem: </span>
<span class='rem'>`memory_planner` returns the `schedule` exactly as it was.</span>
<span class='rem'></span>
<span class='rem'>then back in `Tensor.realize`</span>
<br>
<span class='rem'>def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)</span>
<span class='rem'>```</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>Run</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;for ei in lower_schedule(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(capturing) and CAPTURING: capturing[0].add(ei)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ei.run(var_vals, do_update_stats=do_update_stats)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;while len(schedule):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;si = schedule.pop(0)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try: yield lower_schedule_item(si)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"error lowering &#123;si.ast.op}")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("tensor operations:")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pprint.pprint(si.metadata, indent=2)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise e</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>def lower_schedule_item(si:ScheduleItem) -> ExecItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;assert len(set(x.device for x in si.bufs)) == 1 or si.ast.op is MetaOps.COPY or getenv("USE_COPY_KERNEL")</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.SINK:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runner = get_runner(si.outputs[0].device, si.ast)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(runner, [si.bufs[x[0]] for x in runner.p.globals], si.metadata)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;out = si.outputs[0]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.COPY:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferCopy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferXfer</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.CUSTOM: return ExecItem(CustomOp(si.ast.arg), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.EMPTY: return ExecItem(EmptyOp(out), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.VIEW: return ExecItem(ViewOp(out), list(si.bufs))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"don't know how to lower &#123;si.ast}")</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>TODO: `LoadOps` was recently renamed to `MetaOps` and `MetaOps.SINK` was added.</span>
<span class='rem'>on the first `ScheduleItem` which copies from `NPY` to `CUDA`</span>
<span class='rem'>determines `kernel_type=BufferCopy` which is a class.</span>
<span class='rem'>`return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))` initiates `BufferCopy`</span>
<span class='rem'></span>
<span class='rem'>```python</span>
<span class='rem'>class BufferCopy(Runner):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, total_sz, dest_device, src_device):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if total_sz >= 1e6: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz/1e6:7.2f}M, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz:8d}, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def copy(self, dest, src):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.allocator.copy_from_disk(dest._buf, src._buf, src.nbytes)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fast(ish) path, uses readinto in diskbuffers</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src.allocator.copyout(dest.allocator.as_buffer(dest._buf), src._buf)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.copyin(src.as_buffer(allow_zero_copy=True))&nbsp;&nbsp;&nbsp;&nbsp;# may allocate a CPU buffer depending on allow_zero_copy</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest, src = rawbufs[0:2]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, &#123;dest.size} != &#123;src.size}, &#123;dest.dtype} != &#123;src.dtype}"</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = time.perf_counter()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.copy(dest, src)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if wait:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Device[dest.device].synchronize()</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return time.perf_counter() - st</span>
<span class='rem'></span>
<span class='rem'>class Runner:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate = True, display_name, dname, op_estimate, mem_estimate</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;@property</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def device(self): return Device[self.dname]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def exec(self, rawbufs:List[Buffer], var_vals:Optional[Dict[Variable, int]]=None) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self(rawbufs, &#123;} if var_vals is None else var_vals)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise NotImplementedError("override this")</span>
<span class='rem'></span>
<span class='rem'>@dataclass(frozen=True)</span>
<span class='rem'>class ExecItem:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prg: Runner</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: List[Optional[Buffer]]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if do_update_stats:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.kernel_count += 1</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_ops += (op_estimate:=sym_infer(self.prg.op_estimate, var_vals))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_mem += (mem_estimate:=sym_infer(self.prg.mem_estimate, var_vals))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if et is not None: GlobalCounters.time_sum_s += et</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ptm = (colored(f"&#123;et*1e3:9.2f}ms", "yellow") if et > 0.01 else f"&#123;et*1e6:9.2f}us") if et is not None else ""</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"&#123;colored(f'*** &#123;self.prg.dname[:7]:7s} &#123;GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} &#123;self.prg.display_name+' '*(38-ansilen(self.prg.display_name))} arg &#123;len(self.bufs):3d} mem &#123;GlobalCounters.mem_used/1e9:5.2f} GB " +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(str() if et is None else f"tm &#123;ptm}/&#123;GlobalCounters.time_sum_s*1e3:9.2f}ms (&#123;op_estimate/((et or 1e-20)*1e9):8.2f} GFLOPS, &#123;mem_estimate/((et or 1e-20)*1e9):7.2f} GB/s)" +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f" &#123;[repr(m) if DEBUG >= 3 else str(m) for m in self.metadata] if self.metadata else ''}"))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prg.first_run = False</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return et</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>`name='copy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'`</span>
<span class='rem'>`colored` from `helpers.py` for ANSI color coding the string</span>
<span class='rem'>after `super(init)` and creating an instance of `ExecItem`, the instance looks like this:</span>
<span class='rem'>```python</span>
<span class='rem'>ExecItem(</span>
<br>
<span class='add'>ExecItem &#123;</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;prg=BufferCopy(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=tinygrad.engine.realize.BufferCopy &#123;</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;device = &lt;tinygrad.runtime.ops_clang.ClangDevice object at 0x7147ec6d3be0></span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp; CLANG &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = "CLANG",</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lds_estimate = 0,</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;},</span>
<br>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>,</span>
<br>
<span class='rem'>)</span>
<span class='add'>}</span>
<br>
<span class='add'>`_MallocAllocator` always allocates with `ctypes.c_uint8 * size`</span>
<span class='rem'>after constructing `ExecItem` it is yielded to `run_schedule`</span>
<span class='rem'>-> `ExecItem.run(var_vals=&#123;}, do_update_stats=True)</span>
<span class='rem'>allocates the buffers</span>
<span class='rem'>first the cuda buffer, which through some ugly back and forth calls `CUDAAllocator._alloc`</span>
<span class='rem'>```python</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;def _alloc(self, size, options:BufferOptions):</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;check(cuda.cuCtxSetCurrent(self.device.context))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if options.host: return init_c_var(ctypes.c_void_p(), lambda x: check(cuda.cuMemHostAlloc(ctypes.byref(x), size, 0x01)))</span>
<span class='rem'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return init_c_var(cuda.CUdeviceptr(), lambda x: check(cuda.cuMemAlloc_v2(ctypes.byref(x), size)))</span>
<span class='rem'>```</span>
<br>
<span class='rem'>`init_c_var` returns the variable after calling the supplied function with the variable.</span>
<span class='rem'>using `cuda` library, which escaped me for now.</span>
<span class='add'>if DEBUG >= 2, it will print the kernel (prg) name and kernel number</span>
<span class='add'>- in magenta if jit</span>
<span class='add'>- in green if the kernel is run the first time</span>
<br>
<span class='add'>"methods" (MetaOps.KERNEL) are cached in the method_cache if they repeat in the schedule</span>
<span class='rem'>`NPY` buffer already has `buf._buf`, which is a `numpy.ndarray` with `[1,2,3]` in it.</span>
<span class='rem'></span>
<span class='rem'>`et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)`</span>
<span class='rem'>-> `BufferCopy(bufs, &#123;}, False)</span>
<span class='rem'>-> `dest.copyin(src.as_buffer(allow_zero_copy=True))` where `src` is `bufs[1]` and `dest` is `bufs[0]`</span>
<span class='rem'></span>
<span class='rem'>`src.as_buffer` -> `return self.copyout(memoryview(bytearray(self.nbytes)))`</span>
<span class='rem'>eventually calls `NPYAllocator.copyout(mv:memoryview, self._buf:np.ndarray)`</span>
<span class='rem'>which mostly does numpy stuff to ensure "C-contiguous array", returns a new memoryview to the new array.</span>
<span class='rem'></span>
<span class='rem'>`dest.copyin` produces `host_mem` on the device through cuda library and more weird cuda things.</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
<br>
<span class='rem'>watch out for garbo below</span>
<span class='rem'></span>
</div>
</div>
<span class='rem'>creating tensors through methods</span>
<div class='indent'>
<span class='rem'></span>
<span class='rem'>- Tensor.empty - no new ops</span>
<span class='rem'>- Tensor.zeros - `full(shape, 0, ...)`</span>
<span class='rem'>- Tensor.ones - `full(shape, 1, ...)`</span>
<span class='rem'>- `full(shape, fill_value)`:</span>
<span class='rem'>```python</span>
<span class='rem'>Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)</span>
<span class='rem'>```</span>
<span class='rem'></span>
<span class='rem'>- Tensor.arange - `full(shape, step, dtype, **kwargs)._cumsum() + (start - step)` -> `.cast(dtype)`</span>
<span class='rem'>- Tensor.eye - `ones().pad().flatten().shrink().reshape()`</span>
<span class='rem'>- Tensor.full_like - `full`</span>
<span class='rem'>- Tensor.zeros_like `full_like`</span>
<span class='rem'>- Tensor.ones_like `full_like`</span>
<span class='rem'></span>
<span class='rem'>all Tensor constructors that aren't random build on the `Tensor.full(shape, fill_value)` function, which first *reshapes* the Tensor with 1 element (fill_value) to the target number of dimensions.</span>
<span class='rem'>`Tensor.reshape` calls `F.Reshape.apply(self, new_shape)` from `function.py`, which inherits from `class Function` in `tensor.py`.</span>
<span class='rem'></span>
<span class='rem'>all `Function` "children", in their `apply`function, create a new Tensor and populate it with new `lazydata`, `requires_grad`, `grad=None` and `_ctx` if `requires_grad` is True. `_ctx` contains the function that was called, which also contains the parent Tensors.</span>
<span class='rem'></span>
<span class='rem'>the `forward` method for `F.Reshape()` is called on the `lazydata`.</span>
<span class='rem'>`lazydata.reshape` turns into `self._view(st.reshape())` (st = ShapeTracker) in `lazy.py`.</span>
<span class='rem'>`ShapeTracker.reshape()` returns a new `ShapeTracker` with (by default) its latest `views` replaced by a new one with the new shape. if `MERGE_VIEWS=0`, the new view is appended to `views` instead.</span>
<span class='rem'>In the current case, the previous View with shape `(1,)` is directly replaced by the new one `(1,)*len(new_shape)`.</span>
<span class='rem'>finally, the tensor gets a new `LazyBuffer` from&nbsp;&nbsp;&nbsp;&nbsp;`create_lazybuffer(self.device, new_st, self.dtype, base=self.base)`</span>
<span class='rem'></span>
<span class='rem'>after the reshape, the dimension use `Tensor.expand(new_shape)` to get the now correct number of dimensions to the final shape.</span>
<span class='rem'>```python</span>
<span class='rem'>self._broadcast_to(tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_pad_left(self.shape, argfix(shape, *args))))))</span>
<span class='rem'>```</span>
<span class='rem'>`argfix` ensures the function works even if the shape was not input as a tuple but through multiple arguments like `reshape(2,2,2)`.</span>
<span class='rem'>`_pad_left` gets inputs to the same number of dimensions.</span>
<span class='rem'>`*` unpacks the tuple with both shapes that `_pad_left` returns</span>
<span class='rem'></span>
<span class='rem'>`Tensor._broadcast_to(self, shape)` runs `_pad_left` again</span>
<span class='rem'>runs `self.reshape` again to the "padded" shape</span>
<span class='rem'>then `F.Expand.apply()` -> `lazybuffer.expand()` -> `shapetracker.expand()` -> `View.expand()` which producees&nbsp;&nbsp;&nbsp;&nbsp;a new `View` with the new shape and everything else being equal. returns a new `ShapeTracker`, returns a new `LazyBuffer`, returns a new `Tensor`</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'>Tensor.arange offers new stuff, calling `Tensor._cumsum()`, using Tensor-Int addition and casting the Tensor.</span>
<span class='rem'>from `Tensor._cumsum()`:</span>
<span class='rem'>```python</span>
<span class='rem'>self.transpose(axis,-1).pad2d((pl_sz,-int(_first_zero)))._pool((self.shape[axis],)).sum(-1).transpose(axis,-1)</span>
<span class='rem'>```</span>
<span class='rem'>where `axis` is 0 and `pl_sz` will in this case be `self.shape[0] - 1`</span>
<span class='rem'></span>
<span class='rem'>`Tensor.transpose(0, -1)`, which translates to `Tensor.permute(order)` where in the order dim 0 and the last dim were swapped. `permute` resolves orders with negative dim indices, error checks and runs `F.Permute.apply(self, order=resolve_order)` -> `lazybuffer.permute(order)` -> `ShapeTracker.permute(order)` -> `View.permute(axis=order)` -> `View.create(permuted_shape, permuted_strides, permuted_mask(if applicable),...)`</span>
<span class='rem'>returns a new `View`in a new `ShapeTracker` in a new `lazybuffer` in a new `Tensor`</span>
<span class='rem'>this transpose changes nothing because the input was a 1D Tensor.</span>
<span class='rem'></span>
<span class='rem'>`Tensor.pad2d(self.shape[0] - 1, 0)` adds `self.shape[0] - 1` 0s to the left on the lowest dimension. Using `pad2d()` seems crazy here, it goes through `Tensor._slice()`, which eventually calls `Tensor.pad((self.shape[0] - 1, 0))` which is even crazier, which calls `F.Pad.apply(...)` which goes on the tour again.</span>
<span class='rem'>`LazyBuffer.pad()` -> `ShapeTracker.pad()` -> `View.pad()`</span>
<span class='rem'>where `(self.shape[0] - 1, 0)` turns into&nbsp;&nbsp;&nbsp;&nbsp;`(-self.shape[0] - 1, self.shape)`, which was already calculated in `Tensor.pad2d` for some reason.</span>
<span class='rem'>A mask is created: `((self.shape[0] - 1, self.shape[0] + self.shape[0] - 1))`</span>
<span class='rem'>calling a trustworthy `View.__unsafe_resize(evernew_arg, new_mask)` where a new `View` is created with the extended `shape` (`self.shape[0] + self.shape[0] - 1`), `offset` of `-self.shape[0] - 1` and the `mask` as it was created. `contiguous` turns `False` whatever that means.</span>
<span class='rem'></span>
<span class='rem'>To see how mask, offset and maybe contiguous are interpreted, a detour to `Tensor.__getitem__()` follows. Or not, because `__getitem__` only returns more "metadata" and does not resolve it. So the detour extends to understanding how the Tensors are realized starting from `Tensor.tolist()`</span>
<span class='rem'>To return to later: rest of `Tensor.arange`, other Tensor construction methods and random construction methods:</span>
<span class='rem'>- Tensor.manual_seed</span>
<span class='rem'>- Tensor.rand</span>
<span class='rem'>- Tensor.randn</span>
<span class='rem'>- Tensor.randint</span>
<span class='rem'>- Tensor.normal</span>
<span class='rem'>- Tensor.uniform</span>
<span class='rem'>- Tensor.scaled_uniform</span>
<span class='rem'>- Tensor.glorot_uniform</span>
<span class='rem'>- Tensor.kaiming_uniform</span>
<span class='rem'>- Tensor.kaiming_normal</span>
<span class='rem'></span>
<span class='rem'></span>
<span class='rem'></span>
</div>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='rem'>Some environment variables are stored in `ContexVar._cache` and as `ContextVar` instances and can be imported from `tinygrad.helpers` but others are determined dynamically through `getenv` which is also imported from `tinygrad.helpers` and used like `getenv("LAZYCACHE", 1)`. Not obvious why this added complexity.</span>
<span class='rem'></span>
<span class='rem'>`tensor.py` too big, methods more around imitating style than being nicely categorized? Remove stuff like `Tensor.ones` or duplication of `Tensor.transpose` and `Tensor.T`</span>
<span class='rem'></span>
<br>
<span class='rem'>`Tensor(1).lazydata.contiguous_child` is a tuple of weakref to some lazybuffer and its own ShapeTracker ??</span>
<br>
<span class='add'>`Tensor(1).lazydata.contiguous_child` is a tuple of weakref to some lazybuffer and its own ShapeTracker?</span>
<br>
<span class='add'>context vars don't add themselves to actual context</span>
<span class='add'>some context vars are acceseed via import from helpers, some through getenv</span>
<span class='rem'>trying the AMD device takes a lot of lines, importing from `renderer/cstyle.py`. can be solved by switching lines in import</span>
<span class='rem'></span>
<span class='rem'>can create a Tensor on a device that does not actually work and will only cause an error when realized (not when realized even, but tolist does not work. where do they fail, how much work is wasted on it?</span>
<span class='rem'></span>
<span class='rem'>if `CUDA`, `ptx_matcher:PatternMatcher` might replace the other pattern matcher that was laboriously created when importing tensor?</span>
<span class='rem'></span>
<span class='rem'>how good is tinygrad introspection? feel need for an inliner to be rooted in base reality.</span>
<span class='rem'></span>
<span class='rem'>context vars set in helpers.py return incorrect value through getenv?</span>
<span class='rem'>try `from tinygrad.helpers import CAPTURING; bool(CAPTURING)`</span>
<br>
<span class='add'>try `from tinygrad.helpers import JIT; bool(JIT)` -> True</span>
<br>
<span class='rem'>and `from tinygrad.helpers import getenv; getenv("CAPTURING")`</span>
<br>
<span class='add'>and `from tinygrad.helpers import getenv; bool(getenv("JIT"))` -> False</span>
<span class='add'></span>
</div>
<span class='add'>Research</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>https://towardsdatascience.com/matrix-multiplication-on-the-gpu-e920e50207a8</span>
</div>
</div>
</div>
</div>
</div>
<span class='date' id='t2024-07-16-15:35'>2024 07 16 15:35</span><div class='indent'>
<span>Inbox.md</span>
<div class='indent'>
<span class='hdg'>Inbox</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>2024-07-14 20:23</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>there are bridges to be built. between the ideas, nature, work, the spirits. Curiosity leads to testing new ways to link them.</span>
<span class='add'>building a product means actualizing the spirits, making the product beautiful requires facing and approaching beauty myself. The nerd-unattractiveness comes from an incomplete person if at all, is not inherent in nerdbeing. requires negotiating between the spirits and curageously producing something complete. at least the most complete I can manage.</span>
<span class='add'>the product, say tinygrad, in greatness, is not a mere tool. it developed the elegance to reflect the truth in itself. to shine with the greens of growth, potential and mysteriousness, the mischivousness of a great troll, the danger and exhiliaration of its varied use, power and darkness. It reflects so stongly, it might just show the way. It should not miss but contain challenging sexiness, doors to transformation, destructive determination, exposition, spontaneousness.</span>
<span class='add'>The tool is dead if I talk to it and its reflection does not answer.</span>
<span class='add'>These properties do not lead to a singular product, they are the consequence of a refined product that reflects in truth, which can be the future of many products, though not all. The stupidity of some might be so near infinite, their abolishion might be the best thing to happen to them. Like projects clinging to past technology, unwilling to die gracefully.</span>
<span class='add'>The awfully draining, painful, torturous creation of something beautiful looks deep into the creator. Any of my tendencies to overplan, to clean obsessively, to autodestruct and turn evil in despair will become concentrated and obvious in the naked product, subject to the open world, inevitably failing due to its inadequacy and stupidity. I should not fear destruction, for I can try to fail gracefully.</span>
<span class='add'></span>
<span class='add'>The optimized organisms in nature reflect its truths. Nature does not leave alone, it knows me. The production of highly optimized, open systems that explore more of it, the continuation and expression of the spirits, is what they ask of me.</span>
<span class='add'></span>
<span class='add'>These are but empty words if they don't become actualized in a product.</span>
<span class='add'></span>
<span class='add'>There isn't anything but the present. The symbols of the past and future are superficial. it does not matter if I become terminally ill, am tortured, amount material wealth, receive social approval. there is only a naked person, the spirits and opportunity.</span>
<span class='add'></span>
<span class='add'>tinygrad should not be exclusively about implementing the latest techniques to become an acceptable deep learning framework.</span>
<span class='add'>Elegance through open selection, trial of characters through risk, competition. exploration of the depths through implementing increasingly complete capacity. If I am not scared of tinygrad, what is even the purpose of dealing with it?</span>
<span class='add'>Integration into silicon is the next frontier.</span>
<span class='add'>Uncover the poetry inherent in computing?</span>
<span class='add'></span>
<span class='add'>I don't know how to translate from these words to action, there are bridges to be built.</span>
<span class='add'></span>
<span class='add'>much that I read in tinygrad is ugly, does not present its role openly, merely produces more objects and variables. It isn't obvious why `LazyOp` even exists, why translating `LazyBuffer` to `ScheduleItem` requires hundreds of lines of code. Why `metadata` needs to creep in everywhere, why `Tensor` has so many weak methods that could be boiled down.</span>
<span class='add'>Could try and go all the way down the `realize` to at least see what the outcome is, then understand the structure above it.</span>
<span class='add'>Find archetypes, describe them. Maybe explain tinygrad, produce a vision in the meanwhile.</span>
</div>
</div>
</div>
<span>tinygrad dev exploration.md</span>
<div class='indent'>
<span class='hdg'>tinygrad dev exploration</span>
<div class='indent'>
<span class='hdg'>Less refined</span>
<div class='indent'>
<span class='hdg'>encountered python</span>
<div class='indent'>
<span class='rem'></span>
<span class='add'>`dict.get(self, key, default=None, /)` Return the value for key if key is in the dictionary, else default.</span>
<span class='add'>format specifiers. `int:8` means the int takes up 8 spaces when printing. same as `int:8d` d for digit. can do alignment with `int:>8d` for right alignment.</span>
</div>
</div>
</div>
<span class='hdg'>LazyBuffer._copy:</span>
<div class='indent'>
<span class='hdg'>`Tensor` also has some classvariables, ignored here, can be seen in [Importing Tensor](#Importing%20Tensor) at `tensor.py`.</span>
<div class='indent'>
<span class='hdg'>Realizing a Tensor</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>Schedule</span>
<div class='indent'>
<span class='rem'>back in `schedule_with_vars`</span>
<span class='add'>back in `tensor.py` -> `schedule_with_vars`</span>
<br>
<span class='add'>-> `schedule.py`</span>
<span class='add'>```python</span>
<span class='add'>def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;# Exclude buffers involved in load ops (e.g transfers) to preserve parallelism in graphs.</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assigned = _internal_memory_planner(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[si.bufs for si in schedule],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;noopt_buffers=&#123;b for si in schedule if si.ast.op is not MetaOps.SINK for b in si.bufs}</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]</span>
<span class='add'>```</span>
<br>
<span class='add'>`_internal_memory_planner` is optional (`NO_MEMORY_PLANNER=1` skips it and it still works).</span>
<span class='add'>actually, it skips all buffers where `buf.lb_refcount > 0` anyway , which applies to all of the current buffers since they all come from the `LazyBuffer` constructor, where they automatically get a reference. `assigned` will be `&#123;}`.</span>
<span class='add'></span>
<span class='add'>`memory_planner` returns the `schedule` exactly as it was.</span>
<span class='add'></span>
</div>
<span class='add'>Run</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;for ei in lower_schedule(schedule):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if len(capturing) and CAPTURING: capturing[0].add(ei)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ei.run(var_vals, do_update_stats=do_update_stats)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;while len(schedule):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;si = schedule.pop(0)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try: yield lower_schedule_item(si)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"error lowering &#123;si.ast.op}")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("tensor operations:")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pprint.pprint(si.metadata, indent=2)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise e</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>def lower_schedule_item(si:ScheduleItem) -> ExecItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;assert len(set(x.device for x in si.bufs)) == 1 or si.ast.op is MetaOps.COPY or getenv("USE_COPY_KERNEL")</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.SINK:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runner = get_runner(si.outputs[0].device, si.ast)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(runner, [si.bufs[x[0]] for x in runner.p.globals], si.metadata)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;out = si.outputs[0]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.COPY:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferCopy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_type = BufferXfer</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.CUSTOM: return ExecItem(CustomOp(si.ast.arg), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.EMPTY: return ExecItem(EmptyOp(out), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;if si.ast.op is MetaOps.VIEW: return ExecItem(ViewOp(out), list(si.bufs))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(f"don't know how to lower &#123;si.ast}")</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>TODO: `LoadOps` was recently renamed to `MetaOps` and `MetaOps.SINK` was added.</span>
<span class='add'>on the first `ScheduleItem` which copies from `NPY` to `CUDA`</span>
<span class='add'>determines `kernel_type=BufferCopy` which is a class.</span>
<span class='add'>`return ExecItem(kernel_type(si.ast.arg, out.device, si.inputs[0].device), list(si.bufs))` initiates `BufferCopy`</span>
<span class='add'></span>
<span class='add'>```python</span>
<span class='add'>class BufferCopy(Runner):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, total_sz, dest_device, src_device):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if total_sz >= 1e6: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz/1e6:7.2f}M, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else: name = f"&#123;type(self).__name__[6:].lower()} &#123;total_sz:8d}, &#123;dest_device[:7]:>7s} &lt;- &#123;src_device[:7]:7s}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def copy(self, dest, src):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.allocator.copy_from_disk(dest._buf, src._buf, src.nbytes)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# fast(ish) path, uses readinto in diskbuffers</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;src.allocator.copyout(dest.allocator.as_buffer(dest._buf), src._buf)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest.copyin(src.as_buffer(allow_zero_copy=True))&nbsp;&nbsp;&nbsp;&nbsp;# may allocate a CPU buffer depending on allow_zero_copy</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dest, src = rawbufs[0:2]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, &#123;dest.size} != &#123;src.size}, &#123;dest.dtype} != &#123;src.dtype}"</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st = time.perf_counter()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.copy(dest, src)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if wait:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Device[dest.device].synchronize()</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return time.perf_counter() - st</span>
<span class='add'></span>
<span class='add'>class Runner:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate = True, display_name, dname, op_estimate, mem_estimate</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;@property</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def device(self): return Device[self.dname]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def exec(self, rawbufs:List[Buffer], var_vals:Optional[Dict[Variable, int]]=None) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self(rawbufs, &#123;} if var_vals is None else var_vals)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise NotImplementedError("override this")</span>
<span class='add'></span>
<span class='add'>@dataclass(frozen=True)</span>
<span class='add'>class ExecItem:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg: Runner</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs: List[Optional[Buffer]]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata: Optional[List[Metadata]] = None</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if do_update_stats:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.kernel_count += 1</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_ops += (op_estimate:=sym_infer(self.prg.op_estimate, var_vals))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GlobalCounters.global_mem += (mem_estimate:=sym_infer(self.prg.mem_estimate, var_vals))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if et is not None: GlobalCounters.time_sum_s += et</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if DEBUG >= 2:</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ptm = (colored(f"&#123;et*1e3:9.2f}ms", "yellow") if et > 0.01 else f"&#123;et*1e6:9.2f}us") if et is not None else ""</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"&#123;colored(f'*** &#123;self.prg.dname[:7]:7s} &#123;GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} &#123;self.prg.display_name+' '*(38-ansilen(self.prg.display_name))} arg &#123;len(self.bufs):3d} mem &#123;GlobalCounters.mem_used/1e9:5.2f} GB " +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(str() if et is None else f"tm &#123;ptm}/&#123;GlobalCounters.time_sum_s*1e3:9.2f}ms (&#123;op_estimate/((et or 1e-20)*1e9):8.2f} GFLOPS, &#123;mem_estimate/((et or 1e-20)*1e9):7.2f} GB/s)" +&nbsp;&nbsp;&nbsp;&nbsp;# noqa: E501</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f" &#123;[repr(m) if DEBUG >= 3 else str(m) for m in self.metadata] if self.metadata else ''}"))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prg.first_run = False</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return et</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`name='copy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'`</span>
<span class='add'>`colored` from `helpers.py` for ANSI color coding the string</span>
<span class='add'>after `super(init)` and creating an instance of `ExecItem`, the instance looks like this:</span>
<span class='add'>```python</span>
<span class='add'>ExecItem(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;prg=BufferCopy(</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display_name = '\x1b[33mcopy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CUDA &lt;- NPY&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\x1b[0m',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dname = 'CUDA',</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first_run = True,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mem_estimate = 12,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;op_estimate = 0</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;),</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;bufs=[</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:False device:CUDA size:3 dtype:dtypes.int offset:0>,</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;buf real:True device:NPY size:3 dtype:dtypes.int offset:0></span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;],</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;metadata=None</span>
<span class='add'>)</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>after constructing `ExecItem` it is yielded to `run_schedule`</span>
<span class='add'>-> `ExecItem.run(var_vals=&#123;}, do_update_stats=True)</span>
<span class='add'>allocates the buffers</span>
<span class='add'>first the cuda buffer, which through some ugly back and forth calls `CUDAAllocator._alloc`</span>
<span class='add'>```python</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;def _alloc(self, size, options:BufferOptions):</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;check(cuda.cuCtxSetCurrent(self.device.context))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if options.host: return init_c_var(ctypes.c_void_p(), lambda x: check(cuda.cuMemHostAlloc(ctypes.byref(x), size, 0x01)))</span>
<span class='add'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return init_c_var(cuda.CUdeviceptr(), lambda x: check(cuda.cuMemAlloc_v2(ctypes.byref(x), size)))</span>
<span class='add'>```</span>
<span class='add'></span>
<span class='add'>`init_c_var` returns the variable after calling the supplied function with the variable.</span>
<span class='add'>using `cuda` library, which escaped me for now.</span>
<span class='add'></span>
<span class='add'>`NPY` buffer already has `buf._buf`, which is a `numpy.ndarray` with `[1,2,3]` in it.</span>
<span class='add'></span>
<span class='add'>`et = self.prg(bufs, var_vals if var_vals is not None else &#123;}, wait=wait or DEBUG >= 2)`</span>
<span class='add'>-> `BufferCopy(bufs, &#123;}, False)</span>
<span class='add'>-> `dest.copyin(src.as_buffer(allow_zero_copy=True))` where `src` is `bufs[1]` and `dest` is `bufs[0]`</span>
<span class='add'></span>
<span class='add'>`src.as_buffer` -> `return self.copyout(memoryview(bytearray(self.nbytes)))`</span>
<span class='add'>eventually calls `NPYAllocator.copyout(mv:memoryview, self._buf:np.ndarray)`</span>
<span class='add'>which mostly does numpy stuff to ensure "C-contiguous array", returns a new memoryview to the new array.</span>
<span class='add'></span>
<span class='add'>`dest.copyin` produces `host_mem` on the device through cuda library and more weird cuda things.</span>
<span class='add'></span>
<span class='add'></span>
</div>
</div>
<span class='hdg'>Detected room for improvement / questions</span>
<div class='indent'>
<span class='add'></span>
<span class='add'>context vars set in helpers.py return incorrect value through getenv?</span>
<span class='add'>try `from tinygrad.helpers import CAPTURING; bool(CAPTURING)`</span>
<span class='add'>and `from tinygrad.helpers import getenv; getenv("CAPTURING")`</span>
</div>
</div>
</div>
</div>
</div></article></main><script>MathJax = { tex: {inlineMath: [['$', '$']],displayMath: [['$$', '$$']]}};</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script></body></html>