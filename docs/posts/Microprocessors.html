<!DOCTYPE html><html lang=en><head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Microprocessors</title>
<link rel="stylesheet" href="../main.css">
<link rel="shortcut icon" href="../favicon.ico"></head><body><main><nav><a href='../index.html'>Entrance</a></nav><article><p class="post-date">Created <time datetime="2024-09-07T08:49:30+02:00">2024 09 07</time>, last changed <time datetime="2024-11-09T14:46:09.691990+00:00">2024 11 09</time></p>
<p>Following my working definition of beauty (truthful, open reflection of underlying character / properties / identity), I like computers as a potential beauty-machine. Build a beautiful beauty machine. Then, see it wander off into the New and Unpredicted. Fuck comfort. I am already dead when I recognize that the elusive self is the destination.<br />
Build the beautiful machine, extend into it, let it extend into me.<br />
There is a tech stack to map and integrate such that the result can be reflective of the universe.<br />
I hate technicalities and nomenclature. Ugliness. The aim is to extract the principles, the self in the universe and the universe in the self.<br />
To the unknown.</p>
<h1 id="Microprocessors">Microprocessors</h1>
<div class="toc">
<ul>
<li><a href="#Microprocessors">Microprocessors</a><ul>
<li><a href="#Direction">Direction</a></li>
<li><a href="#More%20refined">More refined</a><ul>
<li><a href="#Programming%20Massively%20Parallel%20Processors">Programming Massively Parallel Processors</a><ul>
<li><a href="#1.%20Introduction">1. Introduction</a><ul>
<li><a href="#Why%20massively%20parallel%20processors%3F">Why massively parallel processors?</a></li>
<li><a href="#How%20will%20reading%20this%20help%3F">How will reading this help?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#Less%20refined">Less refined</a><ul>
<li><a href="#2.%20Heterogenous%20data%20parallel%20computing">2. Heterogenous data parallel computing</a></li>
<li><a href="#The%20Elements%20of%20Computing%20Systems%20%20Building%20a%20Modern%20Computer%20from%20First%20Principles">The Elements of Computing Systems: Building a Modern Computer from First Principles</a><ul>
<li><a href="#Hardware">Hardware</a></li>
<li><a href="#1.%20Boolean%20Logic">1. Boolean Logic</a></li>
</ul>
</li>
<li><a href="#The%20Art%20of%20Electronics">The Art of Electronics</a></li>
<li><a href="#Research">Research</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="Direction">Direction</h2>
<h2 id="More%20refined">More refined</h2>
<h3 id="Programming%20Massively%20Parallel%20Processors">Programming Massively Parallel Processors</h3>
<p>(4th edition, Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj)</p>
<h4 id="1.%20Introduction">1. Introduction</h4>
<h5 id="Why%20massively%20parallel%20processors%3F">Why massively parallel processors?</h5>
<p>Because depending on the program</p>
<ul>
<li>massively parallel, throughput-orientied processors, commonly referred to as Graphics Processing Units (GPUs, graphics was their first leading application),</li>
<li>latency-oriented, less parallel, general-purpose processors, traditionally referred to as Central Processing Units (CPUs, every computer has one),</li>
<li>or a combination of both</li>
</ul>
<p>could be fastest or most efficient.</p>
<p><img alt="" src="attachments/CPUvsGPU.png" /><br />
<em>Cache = fast on-chip memory<br />
DRAM = "slow" off-chip Dynamic Random Access Memory<br />
Many CPUs also have a GPU on the same chip, which is less powerful than contemporanous, discrete ones.</em></p>
<p>These approaches are distinct, because optimizing for low latency means</p>
<ul>
<li>
<p>sacrificing expensive chip area for</p>
<ul>
<li>large caches for faster data access</li>
<li>control units for better utilization<br />
with diminishing returns</li>
</ul>
</li>
<li>
<p>increasing clock rate -&gt; higher voltage -&gt; exponentially higher power consumption</p>
</li>
</ul>
<p>Throughput-oriented processors use the chip area for more processing units at lower clockrates and implement parallel memory access. This leads to much higher throughput for similar cost and power consumption.</p>
<p>Low latency is best for sequential programs, were each step depends on the previous one.<br />
Many tasks in simulation, graphics processing and machine learning inherently offer potential for parallelization and so, can benefit from throughput-oriented processors.</p>
<p>Graphics processing because each pixel can often be treated independently.<br />
machine learning because it involves many matrix multiplications that can be done in parallel.<br />
simulation because many particles have to account for the same forces and so, the same calculation</p>
<h5 id="How%20will%20reading%20this%20help%3F">How will reading this help?</h5>
<p>It's a guide on using GPUs effectively, which requires careful, application specific management of their many processing units and small caches and cooperation with the CPU.<br />
Various, reportedly similar, programming models exist to accomplish this and here, Nvidias Compute Unified Device Architecture (CUDA) will be used. It works on Nvidia GPUs only, is the best performing and most widely used.</p>
<h2 id="Less%20refined">Less refined</h2>
<p>GPUs bottom up<br />
I know why they are built.<br />
actually I don't because I don't know how CPUs use their Caches and Control Units and higher clock rates to improve performance.<br />
leaving aside exact implementation, how are GPUs organized so I can benefit from them optimally?<br />
how precisely do I program them?</p>
<p>organized around streaming multiprocessors and memory hierarchy<br />
so they can cooperate: share memory for speed and synchronize (for?)</p>
<ul>
<li>grid -&gt; (blockCluster -&gt;) block -&gt; thread.</li>
<li>warps are more hardware related and translate threads to the cores?</li>
</ul>
<h3 id="2.%20Heterogenous%20data%20parallel%20computing">2. Heterogenous data parallel computing</h3>
<p>2.1 Data parallelism</p>
<ul>
<li>data parellelism where I can treat individual data points independetly to a varying degree. example: converting image to grayscale. data parellelism is main source of parallelism because the intereting applications use large amounts of data (simulation, image recognition (? matrix multiplication))</li>
<li>task parallelism (to be explained more later) means splitting up a task into multiple independent ones. data parallelism is a simpler special case of task parallelism</li>
<li>code is being reorganized to be executed around the new data structure</li>
</ul>
<p>2.2 CUDA C program structure</p>
<ul>
<li>framework vs programming model<ul>
<li>framwork provides specific functino, programming model more like a way to think about program and data structure (warps, block, threads)</li>
</ul>
</li>
<li>kernel (seed) vs function. its a function that is "launched" onto the GPU and executed for each thread</li>
<li>cuda c extends normal c (standard / ANSI C) with keywords, some new syntax and functions to run stuff on the GPU. all normal c code runs on CPU</li>
<li>host and device code somehow cooperate. the host launches (kernel) a grid of threads</li>
<li>cuda threads are sequential programs with a program counter and its variables. (each thread runs the same program but effectively has an id as defined by predefined variables that differ for each thread)</li>
<li>Generating and scheduling threads on GPU is very fast. Not on CPU. kernels tend to be simple and can just be copied into the threads. context switching is also extremely fast on GPU for latency hiding.</li>
<li>On CPU what if I open more threads than are available on the CPU? they are switched into the physical threads</li>
</ul>
<p>2.3 vector addition kernel CPU</p>
<ul>
<li>conventional c host code</li>
<li>declaring pointers is with <code>float *P;</code>, accessing address of a variable with <code>int *addr = &amp;V</code> and getting the item at the pointer with <code>float V = *P</code></li>
<li>subsequent statements in the main function can use the output <code>C</code></li>
<li>getting ~20 MFLOPS on CPU</li>
<li>the following code for on-device computation will practically outsource this part, but its inefficient because it will be moving a lot of dta around which is slow.</li>
<li>it will allocate memory on device and move the arrays there, then run the kernel, then free device vectors</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;time.h&gt;</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// initialize vectors</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">1.2</span><span class="p">,</span><span class="w"> </span><span class="mf">3.1</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span><span class="w"> </span><span class="mf">1.6</span><span class="p">,</span><span class="w"> </span><span class="mf">2.5</span><span class="p">};</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">3.0</span><span class="p">,</span><span class="w"> </span><span class="mf">2.7</span><span class="p">,</span><span class="w"> </span><span class="mf">0.3</span><span class="p">,</span><span class="w"> </span><span class="mf">1.3</span><span class="p">,</span><span class="w"> </span><span class="mf">2.2</span><span class="p">};</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// kernel</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">timespec</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">;</span>
<span class="w">    </span><span class="n">clock_gettime</span><span class="p">(</span><span class="n">CLOCK_MONOTONIC_RAW</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">start</span><span class="p">);</span>
<span class="w">    </span><span class="n">vecAdd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="w">    </span><span class="n">clock_gettime</span><span class="p">(</span><span class="n">CLOCK_MONOTONIC_RAW</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">end</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// results</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%.2f + %.2f = %.2f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Wall clock time: %.9fs</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">((</span><span class="n">end</span><span class="p">.</span><span class="n">tv_sec</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">.</span><span class="n">tv_sec</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">end</span><span class="p">.</span><span class="n">tv_nsec</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">.</span><span class="n">tv_nsec</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1000000000.0</span><span class="p">));</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>the code will compiled into binary so the cpu can execute it</p>
<p>2.4 Device global memory and data transfer</p>
<ul>
<li>cudaMalloc and cudaFree</li>
<li>cudaMalloc assigns to the argument pointer (<code>void **</code>) and returns possible errors. so there is need for the cudaCheck function.</li>
<li>A_h and A_d for host and device</li>
<li>the vecAdd function that allocates, copies and copies back and frees is called <em>stub</em> for calling a kernel.</li>
<li>error checking macro</li>
</ul>
<p>2.5 Kernel functions and threading</p>
<ul>
<li>the kernel functions will be written in SPMD style (single program multiple data).</li>
<li>grid -&gt; block -&gt; thread</li>
<li>1 block = 1024 threads max</li>
<li>threadIdx and blockidx for thread id</li>
<li>what is ANSI C? CUDA C is an extension of ANSI C</li>
<li>globa, host and device keywords for kernel functions.</li>
<li>global kernel function = new grid. </li>
<li>grid of threads = loop (loop parallelism)</li>
<li>boundary checking</li>
</ul>
<p>2.6 Calling kernel functions</p>
<ul>
<li>execution configuration parameters</li>
<li>cannot make assumptions about execution order</li>
<li>some gpus will work through it in smaller pieces than others</li>
<li>language needs a compiler. NVCC produces host code (gcc?) and device code (PTX -&gt; binary)</li>
<li>what is the purpose of just in time compilation?</li>
</ul>
<p>2.7 Compilation</p>
<ul>
<li>needs a different compiler</li>
<li>to virtual binary files (PTX)</li>
<li>runtime component of nvcc translates to "real object files" to be executed on GPU. but in the illustration its called "device just-in-time compiler"</li>
</ul>
<h3 id="The%20Elements%20of%20Computing%20Systems%20%20Building%20a%20Modern%20Computer%20from%20First%20Principles">The Elements of Computing Systems: Building a Modern Computer from First Principles</h3>
<p>(second edition - Noam Nisan, Shimon Schocken)</p>
<blockquote>
<p>What I hear, I forget; What I see, I remember; What I do, I understand.<br />
—Confucius (551–479 B.C.)</p>
</blockquote>
<h4 id="Hardware">Hardware</h4>
<ul>
<li>Church-Turing conjecture that all computers are essentially the same. It does not matter which computer is implemented here.</li>
<li>good modular design -&gt; module are truly independent and can be treated as black boxes by users.</li>
<li>NAND Gates can implement any computer</li>
<li>general road is bottom up but each chapter is top down, goal -&gt; implementation</li>
</ul>
<h4 id="1.%20Boolean%20Logic">1. Boolean Logic</h4>
<p>A truth table shows all possible input combinations and their desired output.<br />
Any truth table can be implemented. Using a subset of simple logic gates. One is {And, Or, Not}, but Nand or Nor can do it too.</p>
<p>various functions (defined in truth tables or other forms) exist. Possible boolean functions for n binary inputs is ${2}^{2^{n}}$. and some have names, like here with two inputs:</p>
<p><img alt="" src="attachments/boolean_functions.png" /></p>
<p>Testing complex chip implementation completely is infeasible, so they test on a subset.</p>
<h3 id="The%20Art%20of%20Electronics">The Art of Electronics</h3>
<p>(Paul Horowitz, Winfield Hill, 2015)</p>
<p>(there are many ways to implement logic gates but electronics seems to be the dominant one, so on to electronics!)</p>
<p>In some substrates, there are electrons loosely bound to their atom cores. They are presumably far away from it.<br />
If there is an electron surplus on one side and an electron deficit on the other, they will leave their atom and flow in the direction of the deficit.<br />
More loose electrons = better conductor.<br />
Resistors, sometimes made from both conductors and insulators impede flow by forcing barriers into the path. Energy is converted to heat.</p>
<p>In a silicon crystal, there are no free electrons.<br />
It is possible to replace some silicon atoms with others that will either have free floating electrons or too few electrons to connect in the grid and will thus produce an electron hole. Both materials thus become conductors. They remain electrically neutral.<br />
If placed next to each other, some free electrons flow to over to fill the electron holes. But now there is an electron imbalance that wants them to flow back. This prevents all free electrons filling the holes in the other material.<br />
If more electrons are added to the side of free floating electrons and a deficit connected to the other side. The side which lack electrons will lack even more and the side that has them will have even more and quickly the pressure is large enough for electrons to jump float through the non-conductive gap.<br />
In the other direction, electrons will first fill the electron holes and on the other side they will be sucked out, effectively eliminating charge carries and widening the non-conductive gap (depletion zone). Only if a large voltage is applied, do electrons flow again.<br />
(Diode)<br />
Transistors can be built from an n-p-n arrangement (or p-n-p) where each n-p or p-n interface has a depletion zone and so, is a diode. No electrions flow unless a large voltage is applied. (source, drain).<br />
Between source and drain is a capacitor (gate), that if activated sucks electrons in (or out) of the depletion zone and that creates a small channel where charge can flow freely.</p>
<p>=&gt; switch, that can switch based on a signal (don't have to use manually).<br />
These switches are off by default. I can make them on by default if I let electrons flow through a lower-resistance path if the switch is on (bypassing the output, making it 0. If the switch is off, all electrons flow to the output, making it 1).</p>
<p>Transistors exibit if-then behaviour, controllable purely by their connections to each other.<br />
Various gates can be built from them, which is another language to simplify logic design (OR, NAND, NOT, AND, XOR,...). And more complex machines like adders, multipliers, data storage. They are all transistors.<br />
When trying to save or reuse a calculation result somewhere else, a particular problem comes up. The output of an adder for example is not immediately stable. For example, 19+21 will first calculate 30 (1+2 and 9+1) and will then carry over the one from 9+1 to produce 40, the correct answer. To deal with this, computers have clocks that preiodically send out 0s and 1s. Storage will only accept input if the clock is on a 1 cycle. The clock is timed such that any calculation can stabilize during the 0 cycle. They also help synchronized processes.<br />
Each gate and each larger part has a truth table, which defines which outputs are expected under which inputs. Any truth table can be realized with transistors and their connections.<br />
Many ways exist to implement switches. Field effect transistors (FETs), as presented here are nice because they have fast switching speeds and are very small, so they can be packed densly, produce little resistance for electricity and can switch fast because the capacitors needs to pull / push only a small distance to create a large channel.</p>
<p>Building chips is rather difficult and expensive, so processors are most often built to be general-purpose processors that can be programmed for any possible behaviour. The processor optimization and their actual usecase better align well.</p>
<p>Naively, to make an intelligent system, the challenge is to find a truth table of a system that can accurately reproduce a sequence of events and that can then build on that.<br />
In this view, intelligence is prioritizing the information that is most useful to predict future states. So the most accurate possible and stable information can be generated from a truth table.</p>
<p>A programmable processor is essentially a truth table, that when fed with certain instructions (parallel wires, either Low (0) or High (1)) will produce the expected output.<br />
Another language is introduced to describe these instruction in a human-readable way. eg. C=A+B, where storage C will contain the content of A and the content of B added up after this instruction.<br />
More abstract languages help to define for-loops and if-statements and even more abstract languages will have libraries that help me use the network interface, the monitor, the mouse, all very easily, grouping together many smaller commands in single larger command.</p>
<h3 id="Research">Research</h3>
<p>GPU PCBs are huge but mostly data storage and delivery, power transformation and delivery and other I/O in support of the core. The Voltage Regulator Modules (VRMs) emit notable heat.<br />
Non Founders Edition cards offer more powerful cooling and sometimes electrical robustness and smallness.</p>
<p>Trying to verify ALU percentage on chip area, but ALUs are too small to differentiate easily?<br />
<a href="https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake">Intel Raptor Lake microarchitecture</a><br />
<a href="https://hothardware.com/news/intel-raptor-lake-huge-cache-upgrade-for-gaming">Intel Alder lake-S good annotation</a><br />
<img alt="" src="attachments/H100-chip.jpg" /><br />
<em>H100 die. "squares" are streaming multiprocessors (144). Darker areas between are mostly L3 Cache.</em><br />
<a href="https://resources.nvidia.com/en-us-tensor-core">H100 Tensor Core GPU Architecture</a></p>
<p><a href="https://blog.ovhcloud.com/understanding-the-anatomy-of-gpus-using-pokemon/">Understanding the anatomy of GPUs using Pokémon</a><br />
<a href="https://www.reddit.com/r/GraphicsProgramming/comments/1871frx/books_for_gpu_arch/">Reddit Books for GPU arch</a></p>
<p>Learning the Art of Electronics<br />
The Art of Electronics<br />
https://www.tinkercad.com/circuits</p>
<p>Different implementations. See how (if) they differ fundamentally (all switch based implementations seem similar)</p>
<p></p>
<p></p>
</article></main><script>MathJax = { tex: {inlineMath: [['$', '$']],displayMath: [['$$', '$$']]}};</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script></body></html>